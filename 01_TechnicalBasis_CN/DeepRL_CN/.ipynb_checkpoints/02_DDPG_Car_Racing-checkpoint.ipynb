{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318f5502-4e1d-4a5d-8f03-5b7d41fc5454",
   "metadata": {},
   "source": [
    "DDPG（Deep Deterministic Policy Gradient）是一种结合了策略梯度方法和Q学习的算法，特别适用于连续动作空间的问题。\n",
    "\n",
    "使用DDPG方法处理具有连续动作空间的Box2D Car Racing环境是一个更合适的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69507d68-70d0-4db9-a78d-411920073277",
   "metadata": {},
   "source": [
    "## 导入必要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40139410-0c1b-4731-bb4a-764b66674649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595c9af-b5c8-4ee0-a68e-04445c2ddb27",
   "metadata": {},
   "source": [
    "## 初始化环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5096a2-ffd1-4ecb-8380-3cb82b14a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境初始化和参数设定\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CarRacing-v2')\n",
    "\n",
    "# 对于CarRacing-v0，状态是一幅图像，我们使用图像的维度来定义状态空间的大小\n",
    "# 这里我们简化处理，直接将状态维度设置为图像的像素数量\n",
    "# 注意，这种处理方式仅为示例，实际应用中需要根据网络输入调整\n",
    "state_dim = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "\n",
    "action_dim = env.action_space.shape[0]  # CarRacing的动作空间是连续的，具有3个动作（加速、转向、刹车）\n",
    "max_action = float(env.action_space.high[0])  # 假设所有动作的最大值相同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f65f1-55c5-4271-983f-5ec174700712",
   "metadata": {},
   "source": [
    "## 定义Actor和Critic模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc0b07-8e35-4350-884b-cd7fcd14ad42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbd13614-3b5a-48e2-9f13-8959d3a075aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCNN(nn.Module):\n",
    "    def __init__(self, action_dim, max_action):\n",
    "        super(ActorCNN, self).__init__()\n",
    "        # 定义卷积层\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),  # 注意：这里的尺寸取决于卷积层的输出\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # 将卷积层的输出展平\n",
    "        x = self.fc_layers(x)\n",
    "        return self.max_action * torch.tanh(x)\n",
    "\n",
    "\n",
    "class CriticCNN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(CriticCNN, self).__init__()\n",
    "        # 定义针对状态的卷积层\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 定义全连接层，同时处理状态和动作\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7 + action_dim, 512),  # 注意：尺寸取决于卷积层输出和动作维度\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # 将卷积层输出展平\n",
    "        x = torch.cat([x, u], 1)  # 将状态和动作融合\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0db234-abcb-4f4b-8e75-f653a9272ef3",
   "metadata": {},
   "source": [
    "## DDPG算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e4877c-448a-40d7-9954-028226298e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经验回放\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0c1d812-8c8d-42f1-8e8e-38c32c6ea531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG算法类将包含初始化、选择动作、存储转换、从经验回放中采样、更新Critic和Actor模型等方法。\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = ActorCNN(action_dim, max_action)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "\n",
    "        self.critic = CriticCNN(action_dim)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(1000000)\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.batch_size = 100\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # 预处理状态数据，例如，调整通道顺序和添加批量维度\n",
    "        state = np.transpose(state, (2, 0, 1))\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.FloatTensor(action)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        done = torch.FloatTensor(done).unsqueeze(1)\n",
    "\n",
    "        # Critic loss\n",
    "        target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_Q = reward + ((1 - done) * self.gamma * target_Q).detach()\n",
    "        current_Q = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "        # Critic update\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        # Actor update\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab7d2c4d-9300-49be-8f4b-9eda4085dc4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 14\u001b[0m     action \u001b[38;5;241m=\u001b[39m ddpg\u001b[38;5;241m.\u001b[39mselect_action(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     16\u001b[0m     ddpg\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd(state, action, reward, next_state, done)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# 初始化和训练模型\n",
    "\n",
    "# Assuming the environment is correctly initialized\n",
    "\n",
    "ddpg = DDPG(state_dim, action_dim, max_action)\n",
    "episodes = 100  # Define the number of episodes for training\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = ddpg.select_action(np.array(state))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        ddpg.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        ddpg.train()\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode} Reward: {episode_reward}\")\n",
    "    if episode % 10 == 0:\n",
    "        ddpg.save(f\"ddpg_{episode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8272aeb-0b71-49b6-a925-8e20fae621ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918362f-0573-46ac-ac35-66a06305a1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f066e7-23f8-4aea-aa9a-4a11d5634716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
