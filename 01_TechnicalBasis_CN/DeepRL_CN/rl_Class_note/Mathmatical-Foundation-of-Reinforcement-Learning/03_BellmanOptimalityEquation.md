# 第三课

## 01. 最优策略

最优策略（Optimal Policy）在强化学习中指的是一种策略，它对于环境中的每个状态，都能产生最大的期望回报，或者说，它能最大化长期累积奖励。在强化学习的框架下，代理（Agent）的目标就是通过学习找到这样的最优策略。

### 定义

- **策略（Policy）**：策略是从状态到动作的映射，定义了在给定状态下代理应采取的动作。策略可以是确定性的，也可以是随机性的。
  - **确定性策略**：在每个状态下都有一个明确的动作被选择。
  - **随机性策略**：在每个状态下选择每个动作有一定的概率。

- **最优策略（Optimal Policy）**：如果对于所有可能的状态，策略\(\pi^*\)都能获得最大的期望回报，则该策略\(\pi^*\)被认为是最优的。形式上，如果对于所有状态\(s\)和所有可能的策略\(\pi\)，都有\(V^{\pi^*}(s) \geq V^\pi(s)\)，则策略\(\pi^*\)是最优的，其中\(V^\pi(s)\)表示在策略\(\pi\)下状态\(s\)的价值。

### 特性

- 在给定的马尔可夫决策过程（MDP）中，可能存在多个最优策略，但所有最优策略都会共享相同的最优状态价值函数\(V^*(s)\)和最优动作价值函数\(Q^*(s, a)\)。
- 最优策略不一定是唯一的，但最优状态价值函数和最优动作价值函数是唯一的。

### 计算最优策略

寻找最优策略的过程涉及到对环境的深入理解，包括状态转移概率和奖励函数。强化学习算法如动态规划（例如值迭代和策略迭代）、蒙特卡罗方法、时序差分学习（例如Q学习和Sarsa）等，都是为了估计状态价值函数或动作价值函数，并最终找到最优策略。

- **动态规划**：当MDP的模型（状态转移概率和奖励函数）已知时，可以使用动态规划方法。
- **无模型方法**：当MDP的模型未知时，可以使用蒙特卡罗方法或时序差分学习方法来估计价值函数，并通过这些估计来寻找最优策略。

总之，最优策略是代理在强化学习任务中追求的目标，它定义了在所有可能状态下如何选择动作以最大化长期奖励。通过学习和适应环境，代理可以逐步改进其策略，以逼近或达到最优策略。

## 02. 贝尔曼最优性方程

贝尔曼最优性方程是强化学习中的一个核心概念，用于寻找最优策略。这些方程提供了一种递归的方法来计算最优策略下的状态价值函数和动作价值函数。

### 贝尔曼最优性方程：状态价值函数

对于状态价值函数\(V^*(s)\)，贝尔曼最优性方程表达了在最优策略下，状态\(s\)的价值等于采取所有可能动作中最好一个动作所得到的期望回报：

\[V^*(s) = \max_a \sum_{s', r} P(s', r | s, a) [r + \gamma V^*(s')]\]

其中，\(V^*(s)\)是状态\(s\)在最优策略下的价值，\(\max_a\)表示对所有可能的动作进行最大化，\(P(s', r | s, a)\)是在状态\(s\)采取动作\(a\)转移到状态\(s'\)并获得即时奖励\(r\)的概率，\(\gamma\)是折扣因子。

### 贝尔曼最优性方程：动作价值函数

对于动作价值函数\(Q^*(s, a)\)，贝尔曼最优性方程描述了在最优策略下，从状态\(s\)开始采取动作\(a\)的期望回报：

\[Q^*(s, a) = \sum_{s', r} P(s', r | s, a) [r + \gamma \max_{a'} Q^*(s', a')]\]

这里，\(Q^*(s, a)\)是在状态\(s\)采取动作\(a\)下的最优期望回报，\(\max_{a'}\)表示对下一状态\(s'\)的所有可能动作\(a'\)的动作价值进行最大化。

### 解释

贝尔曼最优性方程的核心思想是，最优策略下的价值函数可以通过查看所有可能的未来状态（以及这些状态下的最佳动作）来确定。这些方程不仅帮助我们定义了最优策略的性质，还为寻找最优策略提供了一种计算方法。

- **状态价值函数**的方程说明了在最优策略下，选择导致最高期望回报的动作是最佳选择。
- **动作价值函数**的方程则直接给出了在当前状态下采取任何可能动作的最优期望回报，考虑到了所有可能的未来轨迹。

### 应用

贝尔曼最优性方程是动态规划方法的基础，如值迭代和策略迭代算法，这些算法用于计算MDP的最优策略。通过迭代更新价值函数或动作价值函数，这些方法能够逐渐逼近最优解。

### 逐元素形式

贝尔曼最优性方程的逐元素（elementwise）形式直接描述了在马尔可夫决策过程（MDP）中每个状态或状态-动作对的最优价值。这些方程是找到最优策略的关键，因为它们提供了一种计算最优状态价值函数和最优动作价值函数的方法。

### 贝尔曼最优性方程：状态价值函数

对于每个状态\(s\)，最优状态价值函数\(V^*(s)\)满足的贝尔曼最优性方程为：

\[V^*(s) = \max_a \sum_{s', r} P(s', r | s, a) [r + \gamma V^*(s')]\]

这里，\(V^*(s)\)是在状态\(s\)下可以达到的最大期望回报，\(\max_a\)表示对所有可能的动作\(a\)进行最大化，以找到能够产生最大期望回报的动作。\(P(s', r | s, a)\)是在状态\(s\)采取动作\(a\)后转移到状态\(s'\)并得到即时奖励\(r\)的概率，\(\gamma\)是折扣因子。

### 贝尔曼最优性方程：动作价值函数

对于每个状态-动作对\((s, a)\)，最优动作价值函数\(Q^*(s, a)\)满足的贝尔曼最优性方程为：

\[Q^*(s, a) = \sum_{s', r} P(s', r | s, a) [r + \gamma \max_{a'} Q^*(s', a')]\]

这里，\(Q^*(s, a)\)是在状态\(s\)采取动作\(a\)后可以得到的最大期望回报，\(\max_{a'}\)表示在下一状态\(s'\)选择任何可能动作\(a'\)的最大动作价值。这个方程考虑了即时奖励和未来最优状态的折扣回报的总和。

### 解释

- **状态价值函数的贝尔曼最优性方程**提供了一种方法，通过寻找给定状态下所有可能动作的最大期望回报来计算每个状态的最优价值。
- **动作价值函数的贝尔曼最优性方程**则直接关注在给定状态下采取特定动作的最优期望回报，进而通过考虑所有可能的后继状态和在这些状态下采取的最优动作来确定。

这些方程的逐元素形式使得它们在实际计算中非常有用，特别是在实现强化学习算法，如值迭代和策略迭代时。通过迭代应用这些方程，可以逐步逼近每个状态或状态-动作对的最优价值，从而找到最优策略。

### 矩阵-向量形式

贝尔曼最优性方程的矩阵-向量形式为解决马尔可夫决策过程（MDP）中的最优化问题提供了一种高效的计算框架。虽然贝尔曼最优性方程通常以逐元素的形式表达，用于描述状态价值函数或动作价值函数的最优值，将其转换为矩阵-向量形式可以利用线性代数的方法来简化和加速计算。

### 贝尔曼最优性方程的矩阵-向量形式

虽然直接的矩阵-向量形式更常见于贝尔曼期望方程，贝尔曼最优性方程由于其涉及最大化操作，不易直接表示为简洁的矩阵-向量乘积形式。不过，我们可以讨论一个与贝尔曼最优性方程相似的矩阵形式概念，用于理解如何在计算上处理最优化问题。

对于动作价值函数\(Q^*(s, a)\)，如果我们考虑一个有限状态和动作空间，其中每个状态\(s\)和每个动作\(a\)都对应于\(Q\)矩阵中的一个元素，那么更新\(Q^*(s, a)\)的过程可以通过迭代过程进行，每一步都根据其他状态-动作对的当前估计来更新\(Q\)的值。

### 迭代更新过程的矩阵视角

虽然贝尔曼最优性方程的直接矩阵-向量表示受限于最大化操作，我们可以将迭代更新\(Q^*(s, a)\)的过程视为在每一步中使用当前\(Q\)矩阵的值来计算新的\(Q\)矩阵的值。这个过程涉及到：

1. **计算即时奖励和折扣后的未来价值**：对于每个状态-动作对\((s, a)\)，计算即时奖励加上转移到所有可能后继状态的最优动作价值的期望值。
2. **应用最大化操作**：对于每个状态\(s\)，找到所有可能动作\(a\)中使\(Q(s, a)\)最大的\(a\)的值，这一步通常在计算过程中隐含地进行，而不是通过直接的矩阵操作。

### 数学表示

在具体实现中，尽管最大化操作阻碍了将整个过程简洁地表达为单一的矩阵-向量方程，我们可以将每一次迭代的更新过程视为：

\[Q_{\text{new}}(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \max_{a'} Q_{\text{old}}(s', a')]\]

这里，\(P(s'|s, a)\)是从状态\(s\)通过动作\(a\)转移到状态\(s'\)的概率，\(R(s, a, s')\)是相应的即时奖励，\(\gamma\)是折扣因子，\(\max_{a'} Q_{\text{old}}(s', a')\)是考虑所有可能动作在状态\(s'\)下的最大动作价值。

### 结论

尽管贝尔曼最优性方程的矩阵-向量形式不像贝尔曼期望方程那样直接，通过将迭代更新过程视为基于当前估计进行的计算，我们能够利用矩阵操作来有效地进行这些计算，尤其是在使用计算机算法处理大规模MDP问题时。这种方法的关键在于高效地处理和存储状态转移概率、即时奖励，以及在每次迭代中应用最大化操作来更新动作价值估计。

## 03. 贝尔曼最优性方程值迭代

解决贝尔曼最优性方程通常涉及到迭代算法，比如值迭代（Value Iteration）或策略迭代（Policy Iteration），这些算法能够逐步逼近最优策略。由于贝尔曼最优性方程是非线性的（因为包含最大化操作），它不能通过简单的代数方法直接解决。下面，我将通过一个简化的例子来说明如何使用值迭代算法来解决贝尔曼最优性方程。

### 值迭代算法

值迭代算法是通过反复更新状态价值函数的估计来逼近最优解的。对于每个状态\(s\)，算法都会计算执行所有可能动作后的期望回报，并选择最大的那个期望回报作为该状态的价值。这个过程重复进行，直到价值函数收敛。

### 例子：简单的迷宫游戏

假设有一个迷宫游戏，其中有三个状态：\(S = \{s_1, s_2, s_3\}\)，其中\(s_3\)是目标状态，到达即获得奖励+10，游戏结束。从\(s_1\)和\(s_2\)可以通过执行动作\(a_1\)和\(a_2\)转移到其他状态，但每次移动会获得-1的奖励。状态转移图如下所示：

- \(s_1\)执行\(a_1\)会以一定的概率转移到\(s_2\)，执行\(a_2\)会回到自身。
- \(s_2\)执行\(a_1\)会以一定的概率转移到\(s_3\)，执行\(a_2\)会回到\(s_1\)。

折扣因子\(\gamma = 0.9\)。

### 初始设置

- 初始状态价值函数\(V(s) = 0\)，对所有\(s \in S\)。

### 迭代更新

- 对于每个状态\(s\)，更新其价值为所有可能动作的最大期望回报：

\[V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') \right)\]

其中，\(R(s, a)\)是执行动作\(a\)在状态\(s\)的即时奖励，\(P(s'|s, a)\)是从状态\(s\)通过动作\(a\)转移到状态\(s'\)的概率。

- 假设从\(s_1\)到\(s_2\)和从\(s_2\)到\(s_3\)的转移概率都是1，执行动作的即时奖励是-1（除了到达\(s_3\)外，获得+10）。

### 迭代步骤

1. **迭代1**：计算\(V(s_1)\)和\(V(s_2)\)的初始估计。

- \(V(s_1) = \max(-1 + 0.9 \times 0, -1 + 0.9 \times 0) = -1\)
- \(V(s_2) = \max(-1 + 0.9 \times 0, -1 + 0.9 \times 10) = 8\)

2. **迭代2**：使用\(V(s_2)\)的新值更新\(V(s_1)\)。

- \(V(s_1) = \max(-1 + 0.9 \times 8, -1 + 0.9 \times 0) = 6.2\)

这个过程会继续进行，直到\(V(s)\)的值收敛。

### 结论

通过值迭代算法，我们逐步更新每个状态的价值，直到达到稳定状态，此时的状态价值函数就接近最优解。最后，我们可以通过比较每个状态下不同动作的期望回报来确定最优策略。

### 完整例子

让我们通过一个详细的例子来解释值迭代算法。假设我们有一个非常简单的环境，代理可以在三个状态\(S = \{1, 2, 3\}\)之间移动，目标是找到从状态1到状态3的最优路径。状态3是终点状态，并给予代理+10的奖励。代理从状态1开始，每次移动到下一个状态都会收到-1的奖励。代理的动作集合为\(A = \{\text{前进}, \text{停留}\}\)。状态转移概率为1（即，选择动作后状态转移是确定的）。折扣因子\(\gamma = 0.9\)。

### 状态转移和奖励

- 从状态1执行动作"前进"会转移到状态2，奖励为-1。
- 从状态2执行动作"前进"会转移到状态3，奖励为-1。
- 从状态3执行任何动作都会停留在状态3，奖励为+10（终点状态）。

### 初始设置

- 初始状态价值函数\(V(s) = 0\)，对所有\(s \in S\)。

### 值迭代过程

#### 迭代1

- 对于状态1：
  \[V(1) = \max\{-1 + 0.9 \times V(2), 0\} = \max\{-1 + 0, 0\} = -1\]
- 对于状态2：
  \[V(2) = \max\{-1 + 0.9 \times V(3), 0\} = \max\{-1 + 0, 0\} = -1\]
- 对于状态3（终点）：
  \[V(3) = \max\{10 + 0.9 \times V(3), 0\}\]

  由于状态3是终点，我们可以直接计算或者理解为它获得的是即时奖励+10，因此\(V(3) = 10\)。

#### 迭代2

- 更新状态1和状态2的价值，现在我们已经知道\(V(3) = 10\)：
  - 对于状态1：
    \[V(1) = \max\{-1 + 0.9 \times V(2)\} = \max\{-1 + 0.9 \times (-1)\} = -1.9\]
  - 对于状态2：
    \[V(2) = \max\{-1 + 0.9 \times V(3)\} = \max\{-1 + 0.9 \times 10\} = 8\]

#### 迭代3

- 重新计算状态1的价值，使用状态2的新价值：
  \[V(1) = \max\{-1 + 0.9 \times V(2)\} = -1 + 0.9 \times 8 = 6.2\]

### 结论

继续这个过程，每次迭代都会更新状态的价值，直到收敛到稳定的值。最终，我们会得到每个状态在最优策略下的价值，以及如何从状态1移动到状态3以获得最大奖励的指导。

这个简单的例子说明了值迭代算法如何通过逐步逼近来计算状态价值，并通过这些价值来推导出最优策略。在实际应用中，值迭代算法可以应用于更复杂的环境和决策问题，是强化学习中一种非常强大的工具。

## 04. 贝尔曼最优性方程策略迭代

策略迭代是解决强化学习中的最优策略问题的一种方法，它直接利用贝尔曼最优性方程来逐步改进策略直到找到最优策略。策略迭代通常包括两个主要步骤：策略评估（Policy Evaluation）和策略改进（Policy Improvement），这两个步骤交替进行直到策略收敛。

### 策略评估

在策略评估步骤中，对于给定的策略\(\pi\)，我们计算出该策略下的状态价值函数\(V^\pi(s)\)。具体来说，就是解决以下方程，找到每个状态\(s\)的价值，使得：

\[V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} P(s',r|s,a) [r + \gamma V^\pi(s')]\]

这个方程表示在策略\(\pi\)下，状态\(s\)的价值等于在该状态下根据策略选择每个动作的概率，乘以执行该动作后可能转移到的每个状态\(s'\)的转移概率，再乘以即时奖励加上转移到状态\(s'\)的价值的总和。

### 策略改进

在策略改进步骤中，我们使用当前的状态价值函数来更新策略。对于每个状态\(s\)，我们寻找能够最大化\(Q^\pi(s, a)\)的动作\(a\)，然后更新策略以在状态\(s\)时选择这个动作。即，对于每个状态\(s\)，我们选择使得下式最大的动作\(a\)作为新策略：

\[Q^\pi(s, a) = \sum_{s', r} P(s',r|s,a) [r + \gamma V^\pi(s')]\]

如果这个过程中策略没有发生变化，那么我们找到了最优策略\(\pi^*\)，并且当前的状态价值函数就是最优状态价值函数\(V^*\)。

### 策略迭代算法

1. **初始化**：随机初始化策略\(\pi\)和状态价值函数\(V(s)\)。
2. **策略评估**：对于当前策略\(\pi\)，计算每个状态的价值\(V^\pi(s)\)直到收敛。
3. **策略改进**：利用当前的状态价值函数\(V^\pi(s)\)来更新策略\(\pi\)。如果在任何状态下策略都没有变化，那么停止迭代；否则，返回步骤2。

### 示例

假设有三个状态的简化MDP问题，和上述例子相同。我们通过策略迭代来找到从状态1到状态3的最优策略。通过迭代评估当前策略下的状态价值，并基于这些价值改进策略，我们最终能够得到最优策略和最优状态价值函数。

策略迭代算法的优点在于它可以保证收敛到最优策略。在每次迭代中，策略评估确保了我们准确地估计了给定策略下的状态价值，而策略改进则确保我们朝着提高这些价值的方向更新策略，直到找到最优策略。

### 完整例子

让我们通过一个完整的例子来演示策略迭代算法，解决一个简化的马尔可夫决策过程（MDP）。假设有一个小型的环境，只有三个状态\(S = \{s_1, s_2, s_3\}\)，其中\(s_3\)是终点状态，并且状态转移和奖励如下所定义：

- 从\(s_1\)可以选择动作\(a_1\)转移到\(s_2\)，获得-1的奖励，或选择动作\(a_2\)停留在\(s_1\)，获得-2的奖励。
- 从\(s_2\)可以选择动作\(a_1\)转移到\(s_3\)，获得+10的奖励，或选择动作\(a_2\)回到\(s_1\)，获得-2的奖励。
- \(s_3\)是终点，不考虑从\(s_3\)出发的动作。

初始策略\(\pi\)随机选择动作\(a_1\)或\(a_2\)，折扣因子\(\gamma = 0.9\)。

### 初始设置

初始状态价值\(V(s) = 0\)，对于所有\(s \in S\)。初始策略随机，即在每个状态下选择每个动作的概率相等。

### 策略迭代步骤

#### 策略评估

使用贝尔曼期望方程对当前策略进行评估，直到\(V(s)\)收敛。对于本例，由于策略是随机的，我们可以假设在每个状态下采取每个动作的概率是相等的。

1. **状态\(s_1\)的更新**：

\[V(s_1) = 0.5 \times (-1 + 0.9 \times V(s_2)) + 0.5 \times (-2 + 0.9 \times V(s_1))\]

2. **状态\(s_2\)的更新**：

\[V(s_2) = 0.5 \times (10 + 0.9 \times V(s_3)) + 0.5 \times (-2 + 0.9 \times V(s_1))\]

由于\(s_3\)是终点，\(V(s_3) = 0\)。

假设经过计算，我们得到：
- \(V(s_1) = -1.8\)
- \(V(s_2) = 8.1\)
- \(V(s_3) = 0\)

#### 策略改进

基于当前的价值函数，更新策略以最大化每个状态的动作价值。

- 对于\(s_1\)，比较：
  - \(a_1\)：\(-1 + 0.9 \times V(s_2) = -1 + 0.9 \times 8.1 = 6.29\)
  - \(a_2\)：\(-2 + 0.9 \times V(s_1) = -2 + 0.9 \times -1.8 = -3.62\)
  
  选择\(a_1\)因为它产生更大的值。

- 对于\(s_2\)，比较：
  - \(a_1\)：\(10 + 0.9 \times V(s_3) = 10\)
  - \(a_2\)：\(-2 + 0.9 \times V(s_1) = -2 + 0.9 \times -1.8 = -3.62\)
  
  同样，选择\(a_1\)因为它产生更大的值。

#### 重复步骤

重复策略评估和策略改进步骤，直到策略不再改变，此时我们找到了最优策略：

- 对于\(s_1\)，最优动作是\(a_1\)（前进到\(s_2\)）。
- 对于\(s_2\)，最优动作是\(a_1\)（前进到\(s_3\)）。

### 结论

通过策略迭代，我们得到了每个状态的最优动作和最优状态价值。这个例子简单地说明了策略迭代如何通过交替进行策略评估和策略改进来逐渐找到最优策略。在实际应用中，这种方法可以应用于更复杂的环境和决策问题，是强化学习中寻找最优策略的一个重要工具。