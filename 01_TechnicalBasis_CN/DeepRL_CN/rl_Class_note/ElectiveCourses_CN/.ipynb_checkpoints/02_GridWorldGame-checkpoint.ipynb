{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694d0af0-6295-4b37-b539-f8527257e111",
   "metadata": {},
   "source": [
    "### 创建交互环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3926086-41f0-4316-877e-8b068a923d94",
   "metadata": {},
   "source": [
    "首先需要创建一个网格环境，其中包含障碍物、起点和终点，每个元素都有不同的颜色表示。这个环境可以用一个二维数组来表示，其中不同的数字代表不同类型的格子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db84c9-6100-478f-97f6-7965ed91c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "class GridEnvironment:\n",
    "    def __init__(self, grid_size=16, impassable_obstacles=None, negative_obstacles=None, start=(0, 0), goal=(15, 15)):\n",
    "        self.grid_size = grid_size\n",
    "        self.impassable_obstacles = impassable_obstacles if impassable_obstacles else []\n",
    "        self.negative_obstacles = negative_obstacles if negative_obstacles else []\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.state = start\n",
    "        self.grid = np.zeros((grid_size, grid_size))\n",
    "\n",
    "        for obstacle in self.impassable_obstacles:\n",
    "            self.grid[obstacle] = -1  # Represent impassable obstacles with -1\n",
    "        for obstacle in self.negative_obstacles:\n",
    "            self.grid[obstacle] = -2  # Represent negative reward obstacles with -2\n",
    "        self.grid[goal] = 1  # Represent the goal with 1\n",
    "\n",
    "    def render(self):\n",
    "        cmap = colors.ListedColormap(['white', 'green', 'red', 'blue'])\n",
    "        bounds = [-2, -1, 0, 1, 2]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(self.grid, cmap=cmap, norm=norm)\n",
    "        ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
    "        ax.set_xticks(np.arange(-.5, self.grid_size, 1))\n",
    "        ax.set_yticks(np.arange(-.5, self.grid_size, 1))\n",
    "\n",
    "        # Draw the start and goal positions\n",
    "        ax.text(self.start[0], self.start[1], 'S', va='center', ha='center', color='black', fontsize=12)\n",
    "        ax.text(self.goal[0], self.goal[1], 'G', va='center', ha='center', color='black', fontsize=12)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def step(self, action):\n",
    "        action_mappings = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        next_state = tuple(np.add(self.state, action_mappings[action]))\n",
    "\n",
    "        # Check for boundaries and obstacles\n",
    "        if (0 <= next_state[0] < self.grid_size and\n",
    "            0 <= next_state[1] < self.grid_size and\n",
    "            self.grid[next_state] != -1):\n",
    "            self.state = next_state\n",
    "        else:\n",
    "            next_state = self.state  # No change in state if it's an invalid move\n",
    "\n",
    "        # Define rewards\n",
    "        reward = -1  # Default reward\n",
    "        if self.grid[next_state] == -2:\n",
    "            reward = -5  # Negative reward for negative obstacles\n",
    "        elif next_state == self.goal:\n",
    "            reward = 100  # High reward for reaching the goal\n",
    "            return next_state, reward, True  # Episode ends\n",
    "\n",
    "        return next_state, reward, False\n",
    "\n",
    "    def render_path(self, path):\n",
    "        path_grid = self.grid.copy()\n",
    "        for step in path:\n",
    "            if step != self.start and step != self.goal and path_grid[step] == 0:\n",
    "                path_grid[step] = 0.5  # Represent the path with a different value\n",
    "\n",
    "        cmap = colors.ListedColormap(['white', 'green', 'red', 'blue', 'yellow'])\n",
    "        bounds = [-2, -1, 0, 0.5, 1, 2]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(path_grid, cmap=cmap, norm=norm)\n",
    "        ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
    "        ax.set_xticks(np.arange(-.5, self.grid_size, 1))\n",
    "        ax.set_yticks(np.arange(-.5, self.grid_size, 1))\n",
    "\n",
    "        # Draw the start and goal positions\n",
    "        ax.text(self.start[0], self.start[1], 'S', va='center', ha='center', color='black', fontsize=12)\n",
    "        ax.text(self.goal[0], self.goal[1], 'G', va='center', ha='center', color='black', fontsize=12)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Example of how to create and use the environment\n",
    "env = GridEnvironment(impassable_obstacles=[(12, 2), (3, 13), (4, 4), (8, 7), (6, 6), (6, 7)], \n",
    "                      negative_obstacles=[(8, 2), (2, 11), (3, 14), (7, 7), (7, 8), (8, 8), (8, 9), (8, 10), (8, 13), (7, 13), (7, 14)] )\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d90df7-d31d-4f3c-8fd3-221732b357c9",
   "metadata": {},
   "source": [
    "### 构建 Q-learning 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9d337-e8a4-4d05-b9cf-bee538a42917",
   "metadata": {},
   "source": [
    "Q-learning 是一种无模型的强化学习算法，它使用一个表格（Q-table）来存储在给定状态下采取不同动作的预期收益。我们的目标是通过探索环境来更新这个 Q-table，从而找到最优策略。\n",
    "\n",
    "在这个网格环境中，每个状态可以表示为网格上的一个位置，动作则是从一个格子移动到另一个格子。动作通常是：上、下、左、右移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55d1b8-8386-44f1-8c5c-d76f75a8fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((env.grid_size, env.grid_size, 4))  # 4 actions: up, down, left, right\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice([0, 1, 2, 3])  # Explore: choose a random action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state[0], state[1]])  # Exploit: choose the best known action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state[0], next_state[1], best_next_action]\n",
    "        td_error = td_target - self.q_table[state[0], state[1], action]\n",
    "        self.q_table[state[0], state[1], action] += self.learning_rate * td_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91528a44-39d2-4bc1-b20a-2661d1e500d9",
   "metadata": {},
   "source": [
    "### 定义训练过程（算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19145574-580c-4b33-968b-81d575b0e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the train_agent function to record the path\n",
    "def train_agent(env, agent, episodes=500):\n",
    "    for episode in range(episodes):\n",
    "        state = env.start\n",
    "        env.state = state\n",
    "        done = False\n",
    "        path = [state]  # Initialize the path list\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            path.append(state)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}: Agent is learning...\")\n",
    "            env.render_path(path)  # Visualize the path\n",
    "\n",
    "# Example usage\n",
    "env = GridEnvironment()\n",
    "agent = QLearningAgent(env)\n",
    "train_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6b76b-989b-4c36-9e29-65ebcbc0737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(rewards):\n",
    "    # Calculate and plot the rolling average of rewards\n",
    "    rolling_avg = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rolling_avg)\n",
    "    plt.title(\"Rewards Rolling Average Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.show()\n",
    "\n",
    "# 可视化训练结果\n",
    "visualize_training_results(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c35c5-0bbc-475e-a834-acc079aae2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308482c3-b4f8-4b43-9efe-0557af9fc576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
