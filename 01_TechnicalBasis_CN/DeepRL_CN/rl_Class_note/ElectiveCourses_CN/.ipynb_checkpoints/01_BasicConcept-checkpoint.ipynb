{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c53492b-d36c-4e2f-aec1-44195cbd98a5",
   "metadata": {},
   "source": [
    "## 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1faa072-53fd-455c-958d-49b2e150c135",
   "metadata": {},
   "source": [
    "**基础知识：**\n",
    "\n",
    "线性代数、概率论和统计学、最优化理论\n",
    "\n",
    "Python和Pytorch基础\n",
    "\n",
    "深度学习基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffb162-6e11-4e84-bda0-b98ad98b2823",
   "metadata": {},
   "source": [
    "## 1. 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94905243-a72b-4055-8496-161fd2a26818",
   "metadata": {},
   "source": [
    "强化学习（Reinforcement Learning，简称RL）是一种机器学习方法，用于让智能系统（如机器人或计算机程序）通过与环境互动来学习如何做出决策，以最大化某种累积奖励信号。强化学习的主要特点是它涉及到智能体（agent）、环境（environment）和奖励信号（reward signal）之间的交互。强化学习的核心任务就是最优策略函数，这个函数可以使用神经网络来拟合，获得更好的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d7d117-5694-489f-b5f9-d6819c16b787",
   "metadata": {},
   "source": [
    "**智能体（Agent）：** 智能体是学习者或决策制定者，它需要学会如何在环境中采取一系列的行动来实现某种目标。智能体通常具有一个决策策略，用于根据观测到的环境状态来选择行动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc96e4-5ba1-40a0-956f-9e488cb0eda2",
   "metadata": {},
   "source": [
    "**环境（Environment）：** 环境包括智能体所处的外部世界或问题领域。环境的状态可能会随着时间的推移而改变，智能体的行动会影响到环境的演化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c62dd1-3672-4a09-b7a0-2c89df3cc836",
   "metadata": {},
   "source": [
    "### 1.1 状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801ff3c-3e17-4cd6-b276-b669f52471ca",
   "metadata": {},
   "source": [
    "**状态（State）：**状态是指智能体在某一时刻的状态，用S表示。状态可以是离散的，也可以是连续的。例如，船舶智能体在某一时刻的位置、速度、航向等信息，就可以用一个向量来表示，这个向量就是状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b398d8-9122-489c-8da9-524a24dc39e7",
   "metadata": {},
   "source": [
    "**离散状态表示：**适用于状态空间较小的情况。例如，在棋盘游戏中，每个棋盘的布局可以被视为一个独特的状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015fdc4-58f8-471a-99b9-aa8e553aa5bd",
   "metadata": {},
   "source": [
    "$S = \\{s_1, s_2, \\dots, s_n\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10cfca4-ba8a-4ad4-b7a8-a245f2d9f320",
   "metadata": {},
   "source": [
    "**连续状态表示：**对于拥有连续状态空间的环境，如自动驾驶中的位置和速度，通常使用实数向量来表示状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc0d6d-cb1f-4475-9149-4ea097c1b7db",
   "metadata": {},
   "source": [
    "$\\begin{align*}\n",
    "\\text{船舶动态信息:} \\quad & \\mathbf{p} = (x, y, z), \\quad \\mathbf{v} = (v_x, v_y, v_z), \\quad \\mathbf{a} = (a_x, a_y, a_z), \\quad \\theta \\\\\n",
    "\\text{周围环境信息:} \\quad & \\text{附近船舶位置速度, 导航标识, 声音信号, 气象水文条件} \\\\\n",
    "\\text{船舶自身状态:} \\quad & \\text{货仓压力, 温度， 燃油量, 电池电量} \\\\\n",
    "\\text{传感器数据:} \\quad & \\text{雷达, AIS, 摄像头数据}\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c2c85-b63f-462e-b7a7-bb7517105d53",
   "metadata": {},
   "source": [
    "**特征表示（Feature Representation）：**将原始状态通过特征提取转换为特征向量。例如，在图像处理的任务中，可以使用卷积神经网络（CNN）提取图像的特征向量作为状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be969b99-1355-412b-84d9-3c64a93b365e",
   "metadata": {},
   "source": [
    "$\\begin{align*}\n",
    "\\text{船舶动态特性:} \\quad & \\mathbf{p} = (x, y), \\quad \\mathbf{v} = (v_x, v_y), \\quad 航向角\\theta, \\quad 转向率r \\\\\n",
    "\\text{环境因素:} \\quad & \\text{风速风向, 海流速度方向, 海浪高度周期} \\\\\n",
    "\\text{航行状态:} \\quad & \\text{负载状况, 机械系统状态} \\\\\n",
    "\\text{传感器数据:} \\quad & \\text{雷达, 声纳数据}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316a27c-6ada-454c-8118-b2e9ad1aa280",
   "metadata": {},
   "source": [
    "**时间序列表示：**对于需要考虑历史信息的环境，可以使用历史状态序列来表示当前状态，如使用循环神经网络（RNN）处理的序列数据。特别适用于那些需要考虑历史信息以理解当前状态的场景。在时间序列表示中，当前状态不仅依赖于当前的观测，还依赖于过去一系列的观测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272fb5e-ea47-4bfa-9f3f-600806dca446",
   "metadata": {},
   "source": [
    "$\\mathbf{s}_t = \\{s_{t-m}, \\dots, s_{t-1}, s_t\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c5d98-9095-4765-8d34-93d03061fa4f",
   "metadata": {},
   "source": [
    "**混合表示：** 在某些复杂环境中，可能需要结合以上几种方法来表示状态，以更全面地捕捉环境信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1bea0-b09d-4faa-83bf-2e398928d270",
   "metadata": {},
   "source": [
    "$\\mathbf{s} = f(s_{\\text{离散}}, s_{\\text{连续}}, \\phi(\\text{原始状态}), \\{s_{t-m}, \\dots, s_t\\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671de30c-e7b4-496e-87b7-95dc8b89e5b5",
   "metadata": {},
   "source": [
    "### 1.2 动作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b7855-cc3d-46aa-84c9-72f6cdb86b66",
   "metadata": {},
   "source": [
    "动作空间表示了智能体（agent）可以采取的所有可能动作的集合。动作空间可以是离散的，也可以是连续的，这取决于具体的应用场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da9c0ce-19ca-425d-8efd-2fe5bacc15e2",
   "metadata": {},
   "source": [
    "**离散动作空间:** 智能体可以选择有限数量的、明确区分的动作。例如，在棋盘游戏中，每次移动一个棋子的动作就构成了离散动作空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e2699-26f1-4f2a-a98d-a11e7a0efa7e",
   "metadata": {},
   "source": [
    "$A = \\{a_1, a_2, \\dots, a_m\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87acd7-e9d0-4df7-baae-06a261ac0160",
   "metadata": {},
   "source": [
    "**连续动作空间:** 在连续动作空间中，动作可以在连续范围内取值。例如，在自动驾驶领域，车辆的转向角度或加速度就是连续动作空间的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbd6cd-3c67-4321-b1b0-879733169c13",
   "metadata": {},
   "source": [
    "$A = [a_{1_{\\text{min}}}, a_{1_{\\text{max}}}] \\times [a_{2_{\\text{min}}}, a_{2_{\\text{max}}}] \\times \\cdots \\times [a_{n_{\\text{min}}}, a_{n_{\\text{max}}}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ac490-54b9-4bdb-9297-c96bb43b510a",
   "metadata": {},
   "source": [
    "### 1.3 奖励函数 R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9eee1a-3f15-4346-855f-8c287513e4f9",
   "metadata": {},
   "source": [
    "奖励信号是智能体从环境中获得的反馈，用于评估智能体的行动。奖励通常表示智能体的目标或性能标准，智能体的目标是最大化累积奖励。奖励函数定义了智能体（agent）在特定状态下采取特定动作所获得的即时回报（reward）。奖励函数是智能体学习如何在环境中表现的主要指导，它反映了特定行为的好坏。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb1bb7-305c-450b-b48a-256e1c0db71d",
   "metadata": {},
   "source": [
    "$R(s, a, s') = \\text{即时奖励}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28bb08-a22b-4c5b-a878-b05a8d7064bd",
   "metadata": {},
   "source": [
    "### 1.4 回报 G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424c7d7-a640-47a8-a819-faf322379feb",
   "metadata": {},
   "source": [
    "回报（Return）是指从某一时刻开始到未来某个时刻或无限远的未来，智能体获得的总奖励。回报是评估智能体在一系列时间步骤中表现的关键指标。它通常根据获得的即时奖励和未来奖励的累积来计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71cebe-46b9-4b55-8602-d622aed7d6a5",
   "metadata": {},
   "source": [
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e93e9b-c60e-43cd-a953-01e992b4b247",
   "metadata": {},
   "source": [
    "注意：\n",
    "\n",
    "回报（Return）是指单个trajectory获得的discount奖励总和。\n",
    "\n",
    "状态价值（State Value）是多个trajectory获得回报的期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64baa103-3928-4736-afd3-e94e3a0adab9",
   "metadata": {},
   "source": [
    "### 1.5 策略 $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efadd45-8df9-4161-8cb3-37a525d1271f",
   "metadata": {},
   "source": [
    "策略（Policy）是智能体（agent）决定如何行动的指导原则。它定义了在给定状态下选择何种动作的概率分布。策略可以是确定性的（Deterministic Policy）或随机性的（Stochastic Policy）。强化学习的目标之一是找到最优策略，即能够最大化累积奖励的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441cb1b-9e33-4aa0-b476-354e293b98c2",
   "metadata": {},
   "source": [
    "确定性策略:\n",
    "在确定性策略下，对于每个状态 s，策略 π 选择一个特定的动作 a。用函数形式表示为：$a = \\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea0da6-3318-4567-950f-27ca6444f3ae",
   "metadata": {},
   "source": [
    "随机性策略:\n",
    "在随机性策略下，策略 π 为每个状态和动作对指定一个选择该动作的概率。用函数形式表示为：$\\pi(a|s) = P[A = a | S = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa557d20-1a80-4847-a2a6-14e2a4183329",
   "metadata": {},
   "source": [
    "### 1.6 值函数 V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64f8a23-50b4-412a-814a-c04757c84f29",
   "metadata": {},
   "source": [
    "值函数（Value Function）：值函数用于估计在给定策略下，智能体在不同状态或状态-行动对下可以获得的预期奖励。值函数可以帮助智能体评估不同策略的好坏。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2695d3be-2861-415a-8889-908aae10c97c",
   "metadata": {},
   "source": [
    "$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d8220-d924-439e-9429-027c73fc30b6",
   "metadata": {},
   "source": [
    "### 1.7 动作价值函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cefb1-9e38-4d79-a279-4ac39053c678",
   "metadata": {},
   "source": [
    "动作值函数（Action-Value Function），通常表示为 Q 函数或 Q 值函数，是在强化学习中用来评估在给定状态下采取不同行动的价值函数。动作值函数用于衡量智能体在特定状态下，采取不同行动后可以获得的预期回报或累积奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b33a80-a62c-46ce-9dba-abdb2cfba8c4",
   "metadata": {},
   "source": [
    "$Q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S = s, A = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957af4f-2729-498f-beb3-2f6fbf99884b",
   "metadata": {},
   "source": [
    "## 2. 动态规划（Dynamic Programming）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e6f99-c7a3-4830-8345-9e66d091ad87",
   "metadata": {},
   "source": [
    "动态规划（Dynamic Programming，简称DP）是一种解决优化问题的数学方法，它通常用于解决具有以下两个特征的问题：\n",
    "\n",
    "1. 重叠子问题（Overlapping Subproblems）：问题可以分解成许多重复性的子问题，这些子问题在解决整体问题时会被多次使用。动态规划的关键思想是将这些子问题的解存储起来，以避免重复计算，从而提高效率。\n",
    "\n",
    "2. 最优子结构（Optimal Substructure）：问题的最优解可以通过子问题的最优解来构建。这意味着可以通过求解子问题的最优解来找到整体问题的最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdebe5c-897d-493a-8d69-2c2d4d00a363",
   "metadata": {},
   "source": [
    "动态规划通常包括以下步骤：\n",
    "\n",
    "1. 确定问题的状态：将问题划分为若干个子问题，并明确定义每个子问题的状态。状态通常用一个或多个变量来表示问题的局部信息。\n",
    "\n",
    "2. 定义状态转移方程：确定每个子问题之间的关系，即如何从一个子问题的解转移到下一个子问题。这一步骤通常通过递归式或迭代式的方程来完成。\n",
    "\n",
    "3. 初始化：确定初始状态的值或解。\n",
    "\n",
    "4. 计算顺序：确定计算子问题的顺序。通常采用自底向上的方式，从最小的子问题开始，逐步计算到整体问题。\n",
    "\n",
    "5. 存储中间结果：为了避免重复计算，需要将每个子问题的解存储在一个数据结构中，通常使用数组或表格来实现。\n",
    "\n",
    "6. 求解整体问题：通过计算所有子问题的解，得到整体问题的最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f12e2-152a-4d36-8b52-d84dff2b8673",
   "metadata": {},
   "source": [
    "### 2.1 Floyd-Warshall算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37789ad-a974-44e4-9abb-30d2f4b0e0ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945cda73-99a9-49c6-8168-f10f63eea150",
   "metadata": {},
   "source": [
    "### 2.2 Dijkstra算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3872a88-7056-4a4f-b85f-606c1285fe8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db2842c9-a3a2-4942-bf76-c8ea2208195c",
   "metadata": {},
   "source": [
    "### 2.3 Bellman-Ford算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaaacc-aa0a-4782-844f-6f6f67079098",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34257969-32f7-45d7-81a0-bc6baf186a06",
   "metadata": {},
   "source": [
    "### 2.4 0/1背包问题解法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de193cd2-3c58-4847-b071-6bdc5a22247f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc805950-1409-45f4-b50d-97849191fffc",
   "metadata": {},
   "source": [
    "## 3. 马尔科夫决策过程（MDP）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691fe81-cf98-4c83-8f80-9b4d8504658f",
   "metadata": {},
   "source": [
    "马尔科夫决策过程（Markov Decision Process，简称MDP）是一种用于描述和解决序贯决策问题的数学框架。MDP用于建模在不确定环境中进行决策的问题，其中智能体需要根据其当前状态和可能的行动来选择策略以最大化累积奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8a6c9-a465-45bb-9ce3-083c41946205",
   "metadata": {},
   "source": [
    "马尔科夫过程（Markov Process）： 马尔科夫性通常与马尔科夫过程相关联。马尔科夫过程是一个数学模型，描述了一个随机过程中状态随时间的演变，满足马尔科夫性。具体来说，一个马尔科夫过程在某一时刻 t 的状态转移概率只取决于当前状态，并且与过去的状态历史无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611281b8-60cb-4c7a-94dc-e72956febbe9",
   "metadata": {},
   "source": [
    "马尔科夫链（Markov Chain）： 马尔科夫链是一种特殊类型的马尔科夫过程，它具有离散状态和离散时间步的特征。在马尔科夫链中，状态之间的转移概率只取决于当前状态，不受过去状态序列的影响。马尔科夫链通常用状态转移矩阵来表示，其中每个元素表示从一个状态到另一个状态的转移概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1652ee0-8464-47dd-9752-4166a124d12e",
   "metadata": {},
   "source": [
    "$P(S_t = s | S_0, S_1, \\ldots, S_{t-1}) = P(S_t = s | S_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbb7ff-fbb9-45f6-943a-51f0183a8f0d",
   "metadata": {},
   "source": [
    "### 3.1 贝尔曼方程小例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19575b9-3698-4867-9fa4-3185ba62ba68",
   "metadata": {},
   "source": [
    "贝尔曼方程基于这样一个原理：一个问题的最优解包含其子问题的最优解。在动态规划中，我们将一个大问题分解为相似的小问题，并利用这些小问题的解来构造大问题的解。假设你有一系列决策，每个决策都有其相应的收益和后续状态。贝尔曼方程帮助你找到一系列决策，使得总收益最大化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd326976-9c09-44ae-b5c7-c0bcc85dd414",
   "metadata": {},
   "source": [
    "例子：假设你正在玩一个简单的游戏，你可以选择向前走一步或两步。每走一步，你会获得一定的分数，目标是最大化总分数。这里的贝尔曼方程可以表达为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9365e-172d-47fb-b61f-aa15b1a72e4a",
   "metadata": {},
   "source": [
    "$ V(n) = max(V(n-1) + score(n-1), V(n-2) + score(n-2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d11c0-324d-4128-8e01-4a15e539cfee",
   "metadata": {},
   "source": [
    "这个方程的意思是，到达第 n 步的最大分数是由前一步或前两步的最大分数加上当前步的分数中的最大值决定的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99623c0-4177-478b-8e34-2b812462eb44",
   "metadata": {},
   "source": [
    "首先，我们会定义一个函数来计算达到每一步的最大分数。这个函数将会使用动态规划的方法，基于之前的步骤来决定当前步骤的最优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3eaf6c0-1389-47d4-94bb-0c201b520e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大可获得的分数是: 35\n"
     ]
    }
   ],
   "source": [
    "def max_score(scores):\n",
    "    # 检查分数列表是否为空\n",
    "    if not scores:\n",
    "        return 0\n",
    "\n",
    "    # 初始化动态规划数组\n",
    "    dp = [0] * (len(scores) + 1)\n",
    "\n",
    "    # 初始化前两步的分数\n",
    "    dp[1] = scores[0]\n",
    "    if len(scores) > 1:\n",
    "        dp[2] = scores[0] + scores[1]\n",
    "\n",
    "    # 动态规划计算每一步的最大分数\n",
    "    for i in range(3, len(scores) + 1):\n",
    "        dp[i] = max(dp[i - 1] + scores[i - 1], dp[i - 2] + scores[i - 2])\n",
    "\n",
    "    # 返回最后一步的最大分数\n",
    "    return dp[-1]\n",
    "\n",
    "# 测试例子\n",
    "scores = [5, 6, 7, 8, 9]\n",
    "print(\"最大可获得的分数是:\", max_score(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7de55-bfdf-42ed-b2e3-53a593fa3adc",
   "metadata": {},
   "source": [
    "这段代码首先进行了输入的基本检查，然后使用了一个数组 dp 来存储到达每一步时可能获得的最大分数。它通过遍历每一步，并选择从前一步或前两步迈向当前步时能得到的最大分数来更新这个数组。测试用例使用了一个简单的分数列表 [5, 6, 7, 8, 9]。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f21ad-0465-455c-bcd6-3766c6b4931e",
   "metadata": {},
   "source": [
    "### 3.2 贝尔曼方程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743760d4-3948-4a8d-89fc-30f3fc84a217",
   "metadata": {},
   "source": [
    "在强化学习中，贝尔曼方程提供了一种递归的方式来计算状态的价值或者某个策略的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdfdf8-4a55-46ab-80a5-68b0e993c1c6",
   "metadata": {},
   "source": [
    "#### 状态价值函数 $V^{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89bfc45-e5ed-4ad8-907f-9b3de682905f",
   "metadata": {},
   "source": [
    "$V^{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_t = s\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2a9f6-6350-4c8e-8ec6-a48118e846d1",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "$V^{\\pi}(s)$表示在策略$\\pi$下状态$s$的价值。\n",
    "\n",
    "$\\mathbb{E}_\\pi$表示在策略$\\pi$下的期望值。\n",
    "\n",
    "$\\gamma$是折扣因子，范围在 0 到 1 之间，用于减少未来奖励的影响。\n",
    "\n",
    "$R_{t+k+1}$是在时刻$t+k+1$获得的奖励。\n",
    "\n",
    "$S_t$是在时刻 $t$ 的状态。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c2a6b-a3fd-4371-a32e-cd635deb4ffc",
   "metadata": {},
   "source": [
    "**贝尔曼方程为状态价值函数提供了一个递归的表达方式：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88c71c-f104-4dd4-82c0-5f71b8805ae1",
   "metadata": {},
   "source": [
    "$V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V^{\\pi}(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3a092-31b2-41b0-88b7-12cdd41bc6fd",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "$\\pi(a|s)$ 是在状态$s$下采取动作$a$的策略概率。\n",
    "\n",
    "$p(s', r | s, a)$ 是从状态$s$ 采取动作$a$到达状态$s'$并获得奖励$r$的概率。\n",
    "\n",
    "$r$ 和 $s'$ 分别是获得的即时奖励和转移到的新状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8bfa8c-d677-43dc-9d04-d5f54a3b6b26",
   "metadata": {},
   "source": [
    "**注意：**\n",
    "\n",
    "状态价值函数 $V^{\\pi}(s)$提供了评估策略 $\\pi$的一个方法，通过计算在该策略下从特定状态$s$开始所能获得的预期总回报。通过贝尔曼方程，我们可以看到状态价值不仅取决于当前状态和动作，还取决于后续状态的价值。在实际应用中，强化学习算法通常会尝试估计或近似这个状态价值函数，以指导智能体的决策过程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72436a1-ae98-4adc-8a68-0e6510d9eaee",
   "metadata": {},
   "source": [
    "#### 动作价值函数 $Q^{\\pi}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13929d87-4ae7-48ce-bacf-9591fd1997cb",
   "metadata": {},
   "source": [
    "动作价值函数（Action-Value Function），也称为 Q 函数，是强化学习中的一个基本概念，用于评估在给定状态下采取特定动作的预期回报。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500d567-591f-45c6-a996-9df31a26f775",
   "metadata": {},
   "source": [
    "$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_t = s, A_t = a\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452cf4e7-316c-4522-a3ab-4ca1d2f9cd1e",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "$Q^{\\pi}(s, a)$ 表示在策略$\\pi$下，在状态$s$采取动作$a$的价值。\n",
    "\n",
    "$\\mathbb{E}_\\pi$表示在策略$\\pi$下的期望值。\n",
    "\n",
    "$\\gamma$ 是折扣因子，范围在 0 到 1 之间，用于减少未来奖励的影响。\n",
    "\n",
    "$R_{t+k+1}$是在时刻$t+k+1$获得的奖励。\n",
    "\n",
    "$S_t$和$A_t$分别是在时刻 $t$ 的状态和采取的动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52ed3b-4680-40b0-a60e-b5f6caa44cd8",
   "metadata": {},
   "source": [
    "**贝尔曼方程为动作价值函数提供了一个递归的表达方式：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95be324-7d6a-4949-9236-a0e8101c24a0",
   "metadata": {},
   "source": [
    "$Q^{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\sum_{a' \\in A} \\pi(a'|s') Q^{\\pi}(s', a')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28718f-35bd-4b21-b514-c12feecdcc00",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "$p(s', r | s, a)$ 是从状态$s$ 采取动作$a$到达状态$s'$并获得奖励$r$的概率。\n",
    "\n",
    "$\\sum_{a' \\in A} \\pi(a'|s') Q^{\\pi}(s', a')$ 表示在下一个状态$S'$下，根据策略$\\pi$选择所有可能动作 $a'$的期望价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e001572-69ee-400e-8eb2-29c302256785",
   "metadata": {},
   "source": [
    "动作价值函数 $Q^{\\pi}(s, a)$提供了评估策略$\\pi$的一个方法，通过计算在该策略下从特定状态$s$开始并采取特定动作$a$所能获得的预期总回报。\n",
    "贝尔曼方程表明，动作价值不仅取决于当前状态和动作，还取决于后续状态和动作的价值。\n",
    "在强化学习的应用中，算法通常会尝试估计或近似这个动作价值函数，以指导智能体的行为决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709a256-1062-443b-89ea-317d0caeb84f",
   "metadata": {},
   "source": [
    "### 3.3 贝尔曼最优公式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819df10-6df9-41da-a31d-50abf411b77b",
   "metadata": {},
   "source": [
    "#### 贝尔曼最优状态价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339e73b-3034-45cb-9a14-6608e5918499",
   "metadata": {},
   "source": [
    "贝尔曼最优状态价值函数描述了在最优策略下，某状态的最大预期回报。它定义为：\n",
    "\n",
    "$V^*(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma V^*(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271008e-e8bb-4586-beec-e10fa293e3c2",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "$V^*(s)$ 是在状态$s$下最优策略的价值。\n",
    "\n",
    "$max_a$ 表示对所有可能的动作进行最大化。\n",
    "\n",
    "$p(s', r | s, a)$ 是在状态$s$下采取动作$a$后，转移到状态$s'$并接收到奖励$r$的概率\n",
    "\n",
    "$\\gamma$ 是折扣因子，用于减少未来奖励的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b585a-7a36-4893-ad36-1966ef7303a1",
   "metadata": {},
   "source": [
    "#### 贝尔曼最优动作价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f513b79b-8516-41e9-ac5b-49a72e23a4d7",
   "metadata": {},
   "source": [
    "贝尔曼最优动作价值函数描述了在最优策略下，某状态采取某动作的最大预期回报。它定义为：\n",
    "\n",
    "$Q^*(s, a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\max_{a'} Q^*(s', a')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c020d-b292-4540-aaca-fa8b3c649c58",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "$Q^*(s, a)$ 是在状态$s$下采取动作 $a$在最优策略下的价值。\n",
    "\n",
    "$\\sum_{s', r} $ 表示对所有可能的下一个状态$s'$和奖励$r$的求和。\n",
    "\n",
    "$\\max_{a'}$表示对所有可能的下一个动作$a'$进行最大化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf07acc-5c14-42e5-9719-45a138c0478e",
   "metadata": {},
   "source": [
    "## 4. 时序差分（TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc84e0d-201e-408f-8021-566604c0e847",
   "metadata": {},
   "source": [
    "#### 时序差分\n",
    "\n",
    "时序差分（Temporal Difference, TD）学习是一种在强化学习领域中非常重要的方法，它结合了蒙特卡洛方法和动态规划的思想。TD学习的关键特点在于它如何估计和更新价值函数（例如状态价值或动作价值），这些价值函数反映了在给定策略下从特定状态或状态-动作对开始的预期回报。\n",
    "\n",
    "**TD学习的主要特点包括：**\n",
    "\n",
    "1. 基于差分更新：在TD学习中，价值函数的更新是基于当前估计和实际观察到的回报之间的差异（即“时序差分”错误）。\n",
    "\n",
    "2. 自举（Bootstrapping）：TD方法通过部分依赖于现有价值估计来更新其预测。这种自举方法意味着TD算法不需要等待最终结果就可以进行学习，与蒙特卡洛方法相比，这是一个显著的不同点。\n",
    "\n",
    "3. 在线更新：TD学习可以在从环境中获取每一个经验（例如状态转换）后进行更新，不需要等待整个序列结束。这使得TD学习非常适合于连续的、没有明确终点的任务。\n",
    "\n",
    "4. 策略评估和控制：TD学习既可以用于策略评估（即估计给定策略的价值），也可以用于策略控制（即寻找最优策略）。在策略控制中，常见的TD学习方法有Q学习和SARSA。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53ccb8-b534-44d6-9841-ecdbc829b38e",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\text{在时序差分 (TD) 学习中, 基本更新规则如下:} & \\\\\n",
    "V(S_t) &\\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] \\\\\n",
    "\\text{其中:} & \\\\\n",
    "V(S_t) &\\text{ 是在时间 } t \\text{ 的状态 } S_t \\text{ 的价值估计。} \\\\\n",
    "\\alpha &\\text{ 是学习率, 决定了新信息覆盖旧信息的速度。} \\\\\n",
    "R_{t+1} &\\text{ 是在时间 } t \\text{ 到 } t+1 \\text{ 之间获得的奖励。} \\\\\n",
    "\\gamma &\\text{ 是折扣因子, 用于调节未来奖励的重要性。} \\\\\n",
    "V(S_{t+1}) &\\text{ 是下一个状态 } S_{t+1} \\text{ 的价值估计。}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ee4cf-4b70-43bb-a748-068fa8fa27eb",
   "metadata": {},
   "source": [
    "#### 蒙特卡洛方法\n",
    "\n",
    "蒙特卡洛方法是一种基于随机样本来解决各种计算问题的方法。在强化学习领域，蒙特卡洛方法特指用于估计和优化策略的价值函数的技术。这些方法通过分析经历的完整序列（如一局游戏的开始到结束）来学习，而不是依赖于模型预测或部分序列。\n",
    "\n",
    "特卡洛方法需要等到一个序列（episode）结束后，才根据整个序列获得的总回报来更新价值估计。这种方法适用于有明确起点和终点的情景，如棋类游戏。它不依赖于对环境动态的了解，即不需要知道状态转移概率和奖励函数，直接通过经验来学习价值函数或策略。蒙特卡洛方法可以用于策略评估（估计给定策略下的状态或动作价值）和策略优化（找到最优策略）。蒙特卡洛方法通常依赖于探索性的初始策略来确保覆盖所有可能的状态或动作。这是因为它们的学习完全基于从实际经验中获取的样本。与时序差分方法不同，蒙特卡洛方法在更新价值估计时不依赖于现有的价值估计，而是直接使用实际回报。\n",
    "\n",
    "缺点：由于基于完整序列的回报来更新价值估计，蒙特卡洛方法可能会有较高的方差，尤其是在序列长度变化很大的情况下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989b3dd-3303-4930-a45f-380db1110dac",
   "metadata": {},
   "source": [
    "**TD算法和蒙特卡洛方法的共同点和不同点**\n",
    "\n",
    "相同点：\n",
    "\n",
    "1. 基于样本的更新：蒙特卡洛方法和TD算法都使用从环境中采集的样本（即状态转换、奖励等）来更新价值估计。这与基于完整环境模型的动态规划方法不同，动态规划需要知道所有可能的状态转换和奖励。\n",
    "\n",
    "2. 无需环境模型：两者都不需要事先知道环境的完整模型，这使得它们适用于更广泛的、特别是模型未知的场景。\n",
    "\n",
    "3. 对策略的依赖：TD学习和蒙特卡洛方法都可以用于评估给定策略下的状态价值，或者在控制问题中寻找最优策略。\n",
    "\n",
    "4. 使用奖励信号：它们都直接使用从环境中获得的奖励来更新价值估计。\n",
    "\n",
    "不同之处：\n",
    "\n",
    "1. 更新时机：蒙特卡洛方法需要等到一个完整的序列（如一局游戏的结束）完成后，才能根据序列中获得的总回报来更新价值估计。而TD学习可以在每个时间步进行更新，不需要等待序列结束。\n",
    "\n",
    "2. 估计的基础：蒙特卡洛方法基于实际回报来估计价值，而TD学习则使用当前的价值估计来预测未来的价值，这就是所谓的自举（bootstrapping）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6221dfc-b24c-4b23-9939-bd4ddfb4dd3a",
   "metadata": {},
   "source": [
    "### 4.1 SARSA（State-Action-Reward-State-Action）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a42a3a-9c02-4b95-b2db-6058987e4f6c",
   "metadata": {},
   "source": [
    "SARSA算法是一种在强化学习中用于策略控制的时序差分（Temporal Difference, TD）学习方法。它属于一种在线策略学习算法，即在学习过程中，它根据当前策略来决定其行动。SARSA名称来自于算法使用的五个数据元素：状态（State），动作（Action），奖励（Reward），下一个状态（Next State），以及在此状态下采取的下一个动作（Next Action）。\n",
    "\n",
    "SARSA算法的更新规则可以使用以下 LaTeX 公式表示：\n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269261f6-5eda-4fe7-bf26-c35b4c3e07ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634abe81-309d-4178-ba9a-7246c2e065f0",
   "metadata": {},
   "source": [
    "### 4.2 Q学习（Q-Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688fef8-69bf-481e-8f72-4d2b25848d1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f483ed-3de8-4be1-8a4c-70235851903a",
   "metadata": {},
   "source": [
    "### 4.3 TD(λ)（Temporal Difference Learning with Eligibility Traces）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42338e77-1f37-434f-97ca-9a96327e7ed3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5168d1e8-2484-4f39-9ee5-a4c0b7edfba2",
   "metadata": {},
   "source": [
    "### 4.4 Deep Q-Network（DQN）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8355b-9c3a-4b1b-8406-21b224486c6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b167284-6a4c-4389-be7e-03a1ef07f6e8",
   "metadata": {},
   "source": [
    "### 4.5 Actor-Critic （A2C、A3C、DDPG、PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7774c2-127d-4489-b5c2-d685c1705581",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4689649b-16b7-44be-81dd-fcdfde1dcea4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
