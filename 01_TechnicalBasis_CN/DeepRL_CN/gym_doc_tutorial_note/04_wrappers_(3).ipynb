{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21be2ae8",
   "metadata": {},
   "source": [
    "## 导读\n",
    "\n",
    "此部分包括 Action wrappers, Observation wrappers和 Reward wrappers。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3601e1",
   "metadata": {},
   "source": [
    "# Action Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ae2d0",
   "metadata": {},
   "source": [
    "## Base Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6c170",
   "metadata": {},
   "source": [
    "###  class gymnasium.ActionWrapper(env: Env[ObsType, ActType])\n",
    "\n",
    "`gymnasium.ActionWrapper`是一个用于修改动作（action）的超类，可以在将动作传递给基础环境的`step()`方法之前进行修改。如果你希望在动作被传递给环境之前应用某个函数，你可以通过继承`ActionWrapper`并重写`action()`方法来实现这个转换。在`action()`方法中定义的转换必须接受基础环境动作空间中的值。然而，其定义域可能与原始动作空间不同。在这种情况下，你需要通过在包装器的`__init__()`方法中设置`action_space`来指定新的动作空间。\n",
    "\n",
    "Gymnasium提供了如`gymnasium.wrappers.ClipAction`和`gymnasium.wrappers.RescaleAction`等动作包装器，用于裁剪和缩放动作。\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "\n",
    "### 方法\n",
    "- **action(action: WrapperActType) → ActType**:\n",
    "  - **参数**:\n",
    "    - **action**: 原始的`step()`方法中的动作。\n",
    "  - **返回**: 修改后的动作。\n",
    "\n",
    "### 使用示例\n",
    "下面是一个简单的示例，展示如何创建一个自定义的动作包装器，将所有动作的数值加倍后再传递给环境：\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DoubleAction(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DoubleAction, self).__init__(env)\n",
    "        # 假设原始环境的动作空间是连续的（Box）\n",
    "        self.action_space = spaces.Box(low=env.action_space.low*2, high=env.action_space.high*2, shape=env.action_space.shape, dtype=env.action_space.dtype)\n",
    "\n",
    "    def action(self, action):\n",
    "        # 将动作的每个维度值加倍\n",
    "        return action * 2\n",
    "\n",
    "# 创建一个环境并应用DoubleAction包装器\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = DoubleAction(env)\n",
    "\n",
    "# 重置环境并采取一个动作\n",
    "obs = env.reset()\n",
    "action = env.action_space.sample()  # 从新的动作空间采样\n",
    "obs, reward, done, info = env.step(action)  # 执行加倍后的动作\n",
    "```\n",
    "在这个示例中，`DoubleAction`包装器将所有传递给环境的动作值加倍，这对于调整动作空间范围或实验不同的动作策略等情况非常有用。通过继承`ActionWrapper`，你可以轻松地对动作进行各种自定义修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c582746f",
   "metadata": {},
   "source": [
    "### 另一个例子：\n",
    "\n",
    "假设我们正在开发一个强化学习环境，其中的代理需要操作一个机械臂去抓取物体。原始环境接受的是连续动作空间中的动作，表示机械臂的每个关节旋转角度。为了简化问题，我们决定将动作空间离散化，让代理只能选择预定义的一组动作来控制机械臂。我们将创建一个自定义的动作包装器`DiscretizeActionWrapper`，它将连续的动作空间映射到离散的动作空间，并在执行动作之前将离散动作转换回连续动作。\n",
    "\n",
    "### 自定义动作包装器：DiscretizeActionWrapper\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DiscretizeActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, num_actions=5):\n",
    "        super(DiscretizeActionWrapper, self).__init__(env)\n",
    "        self.num_actions = num_actions\n",
    "        # 假设原始环境的动作空间是一维连续的（Box），这里我们将其离散化\n",
    "        self.action_space = spaces.Discrete(num_actions)\n",
    "\n",
    "        # 预定义的动作集，这里简化为等间隔地选择动作空间中的值\n",
    "        self.action_set = np.linspace(env.action_space.low[0], env.action_space.high[0], num_actions)\n",
    "\n",
    "    def action(self, action):\n",
    "        # 将离散动作转换回连续动作\n",
    "        continuous_action = np.array([self.action_set[action]])\n",
    "        return continuous_action\n",
    "\n",
    "# 创建一个原始环境\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "# 应用DiscretizeActionWrapper\n",
    "env = DiscretizeActionWrapper(env)\n",
    "\n",
    "# 重置环境并采取一个动作\n",
    "obs = env.reset()\n",
    "for _ in range(10):\n",
    "    # 从离散的动作空间采样\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"Selected discrete action: {action}\")\n",
    "\n",
    "    # 执行转换后的连续动作\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"Performed continuous action: {env.action_set[action]}\")\n",
    "```\n",
    "\n",
    "在这个例子中，`DiscretizeActionWrapper`将原始环境的连续动作空间离散化为5个动作。每个动作对应于机械臂关节旋转角度的一种预设值。当代理选择一个离散动作时，包装器将其转换为对应的连续动作，然后传递给原始环境执行。这种方法简化了代理的决策过程，使其更容易学习如何控制机械臂，同时仍然保持了一定程度的控制精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07299d1c",
   "metadata": {},
   "source": [
    "## 其他可用的动作包装器 Available Action Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd711a6",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.TransformAction(env: gym.Env[ObsType, ActType], func: Callable[[WrapperActType], ActType], action_space: Space[WrapperActType] | None)\n",
    "\n",
    "`gymnasium.wrappers.TransformAction`是一个包装器，用于在将动作传递给环境的`step()`函数之前对其应用一个函数。这个功能允许用户对动作进行自定义的转换或修改，例如缩放、偏移或其他任何需要的转换，以适应环境的特定要求或实验不同的策略。\n",
    "\n",
    "### 参数\n",
    "- **env** (`gym.Env`): 要包装的环境。\n",
    "- **func** (`Callable[[WrapperActType], ActType]`): 应用于`step()`方法动作的函数。这个函数接受一个动作作为输入，并返回修改后的动作。\n",
    "- **action_space** (`Space[WrapperActType]` 或 `None`): 给定转换函数后，包装器的新动作空间。如果提供，这将替代原始环境的动作空间。\n",
    "\n",
    "### 使用场景\n",
    "- **动作缩放**：将动作值缩放到不同的范围，以适应环境的特定要求。\n",
    "- **动作偏移**：对动作值进行偏移，可能用于实验不同的控制策略。\n",
    "- **动作转换**：将动作从一种格式转换为环境能够接受的另一种格式，例如，从离散动作到连续动作的转换。\n",
    "\n",
    "### 示例代码\n",
    "下面的示例展示了如何使用`TransformAction`包装器将动作的值缩小一半并加上0.1的偏移，然后传递给环境：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "# 定义动作转换函数：将动作值缩小一半并加上0.1的偏移\n",
    "def transform_action(a):\n",
    "    return 0.5 * a + 0.1\n",
    "\n",
    "# 应用TransformAction包装器\n",
    "env = gym.wrappers.TransformAction(env, transform_action, env.action_space)\n",
    "\n",
    "# 重置环境\n",
    "_ = env.reset(seed=123)\n",
    "\n",
    "# 执行转换后的动作\n",
    "obs, *_= env.step(np.array([0.0, 1.0]))\n",
    "\n",
    "# 打印观察结果\n",
    "print(obs)\n",
    "```\n",
    "\n",
    "在这个例子中，`TransformAction`包装器使得动作在传递给`step()`方法之前先被转换。这种方法可以在不修改原始环境代码的情况下，灵活地实验不同的动作处理策略。\n",
    "\n",
    "\n",
    "\n",
    "通过使用`TransformAction`包装器，用户可以非常灵活地控制如何修改和处理传递给环境的动作，这对于实验和优化强化学习策略非常有帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951f174",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.ClipAction(env: Env[ObsType, ActType])\n",
    "\n",
    "`gymnasium.wrappers.ClipAction`是一个包装器，用于在将动作传递给环境的`step()`方法之前对其进行裁剪，确保动作位于环境的动作空间范围内。这个包装器非常实用，特别是在动作可能超出环境预期范围的情况下，它可以自动将动作调整到有效范围，防止可能的错误或异常行为。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import ClipAction\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个环境\n",
    "env = gym.make(\"Hopper-v4\", disable_env_checker=True)\n",
    "\n",
    "# 应用ClipAction包装器\n",
    "env = ClipAction(env)\n",
    "\n",
    "# 打印动作空间，注意到动作空间可能是无限的，但实际执行的动作会被裁剪到有效范围\n",
    "print(env.action_space)  # 输出：Box(-inf, inf, (3,), float32)\n",
    "\n",
    "# 重置环境\n",
    "_ = env.reset(seed=42)\n",
    "\n",
    "# 尝试执行一个超出动作空间的动作\n",
    "_ = env.step(np.array([5.0, -2.0, 0.0], dtype=np.float32))\n",
    "# 实际执行的动作将被裁剪到环境动作空间的有效范围内\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "\n",
    "\n",
    "通过使用`ClipAction`包装器，可以简化处理动作空间限制的逻辑，确保所有传递给环境的动作都在有效范围内，从而避免潜在的问题。此外，这个包装器还提供了一个向量化版本`gymnasium.wrappers.vector.ClipAction`，可以用于同时处理多个环境实例，进一步提高了处理效率和灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d4d4d",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.RescaleAction(env: gym.Env[ObsType, ActType], min_action: float | int | np.ndarray, max_action: float | int | np.ndarray)\n",
    "\n",
    "`gymnasium.wrappers.RescaleAction`是一个包装器，用于在将动作传递给环境的`step()`方法之前，线性地将Box类型的动作空间重新缩放到指定的[min_action, max_action]范围内。这允许开发者调整动作的范围，以适应环境或模型的特定需求，无需修改环境本身的代码。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个环境\n",
    "env = gym.make(\"Hopper-v4\", disable_env_checker=True)\n",
    "\n",
    "# 重置环境并执行一组动作\n",
    "_ = env.reset(seed=42)\n",
    "obs, _, _, _, _ = env.step(np.array([1, 1, 1], dtype=np.float32))\n",
    "\n",
    "# 重新设置环境，应用RescaleAction包装器来调整动作空间的范围\n",
    "_ = env.reset(seed=42)\n",
    "min_action = -0.5\n",
    "max_action = np.array([0.0, 0.5, 0.75], dtype=np.float32)\n",
    "wrapped_env = RescaleAction(env, min_action=min_action, max_action=max_action)\n",
    "\n",
    "# 在包装环境中执行动作，并验证观察结果是否与原始环境一致\n",
    "wrapped_env_obs, _, _, _, _ = wrapped_env.step(max_action)\n",
    "assert np.all(obs == wrapped_env_obs), \"The observations should be equal.\"\n",
    "\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **min_action** (`float`, `int` 或 `np.ndarray`): 每个动作的最小值。这可以是一个numpy数组或一个标量。\n",
    "- **max_action** (`float`, `int` 或 `np.ndarray`): 每个动作的最大值。这可以是一个numpy数组或一个标量。\n",
    "\n",
    "\n",
    "`RescaleAction`包装器使得在不改变原始环境动作空间定义的情况下，通过简单地包装环境即可实现动作范围的调整。这对于那些需要将环境集成到具有不同动作范围需求的强化学习模型中的开发者来说，是一个非常有用的工具。此外，这个包装器还提供了一个向量化版本`gymnasium.wrappers.vector.RescaleAction`，支持同时处理多个环境实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a5819",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.StickyAction(env: Env[ObsType, ActType], repeat_action_probability: float)\n",
    "\n",
    "`gymnasium.wrappers.StickyAction`是一个包装器，用于在环境中添加动作重复的概率。这意味着在每次调用`step()`方法时，有一定的概率（由`repeat_action_probability`参数指定）当前的动作会被重复执行，而不是执行新的动作。这种行为模仿了某些现实世界场景中的不确定性，比如控制器的滞后或响应不一致等问题。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import StickyAction\n",
    "\n",
    "# 创建环境并应用StickyAction包装器\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = StickyAction(env, repeat_action_probability=0.9)\n",
    "\n",
    "# 重置环境\n",
    "obs, info = env.reset(seed=123)\n",
    "print(obs)  # 打印初始观察\n",
    "\n",
    "# 执行一系列动作，注意到由于StickyAction的效果，某些动作可能会被重复执行\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()  # 随机选择一个动作\n",
    "    obs, reward, done, truncated, info = env.step(action)  # 执行动作\n",
    "    print(obs, reward, done, truncated)  # 打印观察结果和奖励\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **repeat_action_probability** (`float`): 重复前一个动作的概率。这个值应该在0到1之间，表示重复动作的概率。\n",
    "\n",
    "\n",
    "通过使用`StickyAction`包装器，开发者可以模拟动作执行中的不确定性，这对于测试和训练更加健壮和适应性强的强化学习模型非常有用。该包装器按照Machado等人在2018年提出的实现进行实现，特别适合于那些需要考虑动作执行不确定性的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf6c6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772e6be2",
   "metadata": {},
   "source": [
    "# Observation Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1017ec",
   "metadata": {},
   "source": [
    "### class gymnasium.ObservationWrapper(env: Env[ObsType, ActType])\n",
    "\n",
    "`gymnasium.ObservationWrapper`是Gymnasium库中的一个包装器，用于修改环境的观察值。这个包装器允许开发者在观察值被传递给学习算法之前应用一个自定义的转换函数。通过继承`ObservationWrapper`并重写`observation()`方法，可以实现观察值的转换。这种转换可以包括归一化、重塑观察空间、转换数据类型、添加特征等操作。\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "\n",
    "### 方法\n",
    "- **observation(observation: ObsType) → WrapperObsType**:\n",
    "  - **参数**:\n",
    "    - **observation**: 环境产生的原始观察值。\n",
    "  - **返回**: 修改后的观察值。\n",
    "\n",
    "### 使用示例\n",
    "\n",
    "假设我们有一个环境，其观察值是一个图像（例如，一个`210x160x3`的数组，代表一个带有RGB通道的210x160像素图像）。现在，我们想要将这个观察值转换为灰度图像，并将其大小重塑为`84x84`，以减少模型的输入维度。\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import ObservationWrapper\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ResizeAndGrayscaleObservation(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ResizeAndGrayscaleObservation, self).__init__(env)\n",
    "        # 设置新的观察空间维度\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # 将观察值转换为灰度并重塑大小\n",
    "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        observation = cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        return observation\n",
    "\n",
    "# 创建环境并应用自定义观察包装器\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env = ResizeAndGrayscaleObservation(env)\n",
    "\n",
    "# 重置环境并获得转换后的观察值\n",
    "observation = env.reset()\n",
    "print(\"Transformed Observation Shape:\", observation.shape)\n",
    "```\n",
    "\n",
    "通过使用`ObservationWrapper`，开发者可以轻松地对环境产生的观察值进行必要的转换，以适应不同的学习模型或实验设置，而不需要修改环境本身的代码。这提高了代码的复用性和灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134f9d5",
   "metadata": {},
   "source": [
    "## Implemented Wrappers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb015e8",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.TransformObservation(env: gym.Env[ObsType, ActType], func: Callable[[ObsType], Any], observation_space: gym.Space[WrapperObsType] | None)\n",
    "\n",
    "\n",
    "`gymnasium.wrappers.TransformObservation`是一个包装器，用于在将观察值从环境的`Env.reset()`和`Env.step()`返回给用户之前，对其应用一个自定义的函数`func`。这可以用于观察值的预处理，比如特征工程、数据归一化、添加噪声等。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformObservation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 重置环境并获取原始观察值\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(\"Original observation:\", obs)\n",
    "\n",
    "# 创建并应用TransformObservation包装器\n",
    "# 添加随机噪声到观察值\n",
    "env = TransformObservation(env, lambda obs: obs + 0.1 * np.random.random(obs.shape))\n",
    "\n",
    "# 重置包装后的环境并获取转换后的观察值\n",
    "transformed_obs, _ = env.reset(seed=42)\n",
    "print(\"Transformed observation:\", transformed_obs)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **func** (`Callable[[ObsType], Any]`): 将被应用到所有观察值的转换函数。如果`func`返回的观察值超出了原始环境观察空间的界限，那么应该提供一个更新后的`observation_space`。\n",
    "- **observation_space** (`gym.Space[WrapperObsType]` 或 `None`): 包装器的观察空间。如果为`None`，则假定与原始环境的观察空间相同。\n",
    "\n",
    "\n",
    "`TransformObservation`包装器使得在不修改环境内部代码的情况下，对环境产生的观察值进行自定义处理变得简单快捷。这个功能对于实验不同的观察值处理方法，或者对观察值进行必要的预处理以适应特定的学习算法非常有用。此外，这个包装器还提供了一个向量化版本`gymnasium.wrappers.vector.TransformObservation`，支持对向量化环境中的观察值进行转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb0f5e",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.DelayObservation(env: Env[ObsType, ActType], delay: int)\n",
    "\n",
    "`gymnasium.wrappers.DelayObservation`是一个包装器，用于在环境返回的观察值中添加延迟。这意味着，在达到指定的延迟步数之前，环境将返回一个与观察空间形状相同的零数组。这种设置模拟了现实世界中可能遇到的观察延迟现象，例如，从传感器接收数据的延迟。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import DelayObservation\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 重置环境并获取初始观察值\n",
    "obs, _ = env.reset(seed=123)\n",
    "print(\"Initial observation without delay:\", obs)\n",
    "\n",
    "# 应用DelayObservation包装器并设置延迟为2步\n",
    "env = DelayObservation(env, delay=2)\n",
    "\n",
    "# 重置包装后的环境并获取延迟后的观察值\n",
    "delayed_obs, _ = env.reset(seed=123)\n",
    "print(\"Initial observation with delay:\", delayed_obs)\n",
    "\n",
    "# 执行两步动作，观察返回的观察值\n",
    "for _ in range(2):\n",
    "    delayed_obs, _, _, _, _ = env.step(env.action_space.sample())\n",
    "    print(\"Delayed observation:\", delayed_obs)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **delay** (`int`): 观察值的延迟步数。在这些步数之前，环境将返回零数组作为观察值。\n",
    "\n",
    "### 注意\n",
    "- 这个包装器不支持随机延迟值。如果用户对此功能感兴趣，请在Gymnasium的GitHub仓库中提出问题或提交拉取请求。\n",
    "\n",
    "\n",
    "通过使用`DelayObservation`包装器，开发者可以模拟环境中的观察延迟，这对于测试强化学习算法在面对不完美信息时的鲁棒性非常有用。这种设置特别适用于模拟现实世界场景，如无人驾驶汽车或机器人控制，其中传感器数据的延迟可能对系统性能产生重大影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5450b",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.DtypeObservation(env: Env[ObsType, ActType], dtype: Any)\n",
    "\n",
    "\n",
    "`gymnasium.wrappers.DtypeObservation` 是一个包装器，用于将环境观察值数组的数据类型修改为指定的`dtype`。这个包装器特别有用于那些需要将环境观察值的数据类型与模型输入数据类型对齐的情况，例如，将观察值从默认的`float64`转换为`float32`以减少计算资源消耗或与特定机器学习框架的数据类型要求相匹配。\n",
    "\n",
    "### 注意\n",
    "- 这个包装器仅与`Box`, `Discrete`, `MultiDiscrete` 和 `MultiBinary` 观察空间兼容。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import DtypeObservation\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 查看原始观察空间的数据类型\n",
    "print(\"Original observation dtype:\", env.observation_space.dtype)\n",
    "\n",
    "# 应用DtypeObservation包装器并设置新的数据类型为float32\n",
    "env = DtypeObservation(env, dtype='float32')\n",
    "\n",
    "# 查看修改后的观察空间的数据类型\n",
    "print(\"Modified observation dtype:\", env.observation_space.dtype)\n",
    "\n",
    "# 重置环境并获取观察值\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(\"Observation dtype after reset:\", obs.dtype)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **dtype** (`Any`): 观察值数组的新数据类型。\n",
    "\n",
    "通过使用`DtypeObservation`包装器，开发者可以确保环境观察值的数据类型满足特定的需求，进而提高数据处理的效率和模型训练的兼容性。此外，这个包装器还提供了一个向量化版本`gymnasium.wrappers.vector.DtypeObservation`，支持对向量化环境中的观察值进行数据类型转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c3ca9",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.FilterObservation(env: gym.Env[ObsType, ActType], filter_keys: Sequence[str | int])\n",
    "\n",
    "\n",
    "`gymnasium.wrappers.FilterObservation` 是一个包装器，用于过滤 `Dict` 或 `Tuple` 观察空间中的特定子空间，只保留一组指定的键（对于 `Dict`）或索引（对于 `Tuple`）。这对于在观察空间中只关注部分信息时非常有用，可以帮助减少模型的输入维度，专注于重要的特征。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FilterObservation, TimeAwareObservation\n",
    "\n",
    "# 创建环境并添加时间感知观察\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = TimeAwareObservation(env, flatten=False)\n",
    "\n",
    "# 显示添加时间感知后的观察空间\n",
    "print(\"Observation space after adding time awareness:\", env.observation_space)\n",
    "\n",
    "# 应用FilterObservation包装器，只保留\"time\"键\n",
    "env = FilterObservation(env, filter_keys=['time'])\n",
    "\n",
    "# 重置环境并查看过滤后的观察值\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(\"Filtered observation after reset:\", obs)\n",
    "\n",
    "# 执行动作并查看过滤后的观察值\n",
    "obs, _, _, _, _ = env.step(0)\n",
    "print(\"Filtered observation after step:\", obs)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **filter_keys** (`Sequence[str | int]`): 要包含的子空间的键集合。对于 `Dict` 观察空间使用字符串列表，对于 `Tuple` 观察空间使用整数列表。\n",
    "\n",
    "### 更改日志\n",
    "- `v0.12.3`: 最初添加，最初命名为 `FilterObservationWrapper`。\n",
    "- `v1.0.0`: 重命名为 `FilterObservation` 并添加对 `Tuple` 观察空间的支持，使用整数 `filter_keys`。\n",
    "\n",
    "通过使用 `FilterObservation` 包装器，开发者可以简化观察空间，去除不必要的信息，从而减轻学习算法的负担。此外，这个包装器还提供了一个向量化版本 `gymnasium.wrappers.vector.FilterObservation`，支持对向量化环境中的观察值进行过滤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87595b",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.FlattenObservation(env: Env[ObsType, ActType])\n",
    "\n",
    "`gymnasium.wrappers.FlattenObservation` 是一个包装器，用于平展化环境的观察空间以及来自`reset`和`step`函数的每个观察值。这个包装器特别适用于将高维的、结构化的观察空间（如图像、多维数组或字典）转换成一维数组，以便简化数据处理和模型输入。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "\n",
    "# 查看原始观察空间的形状\n",
    "print(\"Original observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "# 应用FlattenObservation包装器\n",
    "env = FlattenObservation(env)\n",
    "\n",
    "# 查看平展化后的观察空间的形状\n",
    "print(\"Flattened observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "# 重置环境并获取平展化后的观察值\n",
    "obs, _ = env.reset()\n",
    "print(\"Shape of flattened observation after reset:\", obs.shape)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "\n",
    "\n",
    "通过使用`FlattenObservation`包装器，开发者可以将复杂的观察空间转换为简单的一维数组形式，从而降低数据预处理的复杂性并提高模型处理的效率。此外，这个包装器还提供了一个向量化版本`gymnasium.wrappers.vector.FlattenObservation`，支持对向量化环境中的观察值进行平展化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad2198",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.FrameStackObservation(env: gym.Env[ObsType, ActType], stack_size: int, *, padding_type: str | ObsType = 'reset')\n",
    "\n",
    "`gymnasium.wrappers.FrameStackObservation` 是一个包装器，用于将最近 N 个时间步的观察值堆叠在一起。这种方法对于那些需要观察值中的时间序列信息以进行决策的场景特别有用，例如，在视频游戏或序列预测任务中。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "import numpy as np\n",
    "\n",
    "# 创建环境并应用FrameStackObservation包装器\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "env = FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "# 查看堆叠后的观察空间\n",
    "print(\"Stacked observation space:\", env.observation_space)\n",
    "\n",
    "# 重置环境并获取堆叠后的观察值\n",
    "obs, _ = env.reset()\n",
    "print(\"Shape of stacked observation after reset:\", obs.shape)\n",
    "\n",
    "# 使用不同的填充观察选项\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "stacked_env = FrameStackObservation(env, 3, padding_type=\"zero\")\n",
    "obs, _ = stacked_env.reset(seed=123)\n",
    "print(\"Zero-padded stacked observation shape:\", obs.shape)\n",
    "\n",
    "stacked_env = FrameStackObservation(env, 3, padding_type=np.array([1, -1, 0, 2], dtype=np.float32))\n",
    "obs, _ = stacked_env.reset(seed=123)\n",
    "print(\"Custom-padded stacked observation:\", obs)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **stack_size** (`int`): 要堆叠的帧数。\n",
    "- **padding_type** (`str | ObsType`): 堆叠观察值时使用的填充类型。选项包括\"reset\"（默认，重置值重复）、\"zero\"（使用零填充）和自定义观察值实例。\n",
    "\n",
    "### 更改日志\n",
    "- `v0.15.0`: 最初添加为`FrameStack`，支持 lz4 压缩。\n",
    "- `v1.0.0`: 重命名为`FrameStackObservation`，移除 lz4 和 LazyFrame 支持，同时添加`padding_type`参数。\n",
    "\n",
    "通过使用`FrameStackObservation`包装器，开发者可以在模型输入中包含多个时间步的信息，从而使模型能够基于时间序列数据做出更准确的预测或决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ecc23",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.GrayscaleObservation(env: Env[ObsType, ActType], keep_dim: bool = False)\n",
    "\n",
    "\n",
    "`gymnasium.wrappers.GrayscaleObservation` 是一个用于将环境中的图像观察值从 RGB 转换为灰度图的包装器。这个转换有助于减少处理图像数据所需的计算资源，同时保持对环境状态的关键信息。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "\n",
    "# 查看原始观察空间形状\n",
    "print(\"Original observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "# 应用GrayscaleObservation包装器，不保留通道维度\n",
    "grayscale_env = GrayscaleObservation(env)\n",
    "print(\"Grayscale observation space shape without keeping dimension:\", grayscale_env.observation_space.shape)\n",
    "\n",
    "# 应用GrayscaleObservation包装器，保留通道维度\n",
    "grayscale_env_keep_dim = GrayscaleObservation(env, keep_dim=True)\n",
    "print(\"Grayscale observation space shape with keeping dimension:\", grayscale_env_keep_dim.observation_space.shape)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要包装的环境。\n",
    "- **keep_dim** (`bool`): 是否保留通道维度。如果设置为`True`，观察值将保持三维形状（高度，宽度，1）；否则，观察值将是二维的（高度，宽度）。\n",
    "\n",
    "### 更改日志\n",
    "- `v0.15.0`: 最初添加为`GrayScaleObservation`。\n",
    "- `v1.0.0`: 更名为`GrayscaleObservation`。\n",
    "\n",
    "通过使用`GrayscaleObservation`包装器，开发者可以简化图像处理流程并减少模型训练和推断所需的计算资源，尤其在图像数据占主导地位的环境中。此外，这个包装器提供了灵活性，允许开发者根据自己的需要选择是否保留通道维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b7eb3",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.MaxAndSkipObservation(env: Env[ObsType, ActType], skip: int = 4)\n",
    "\n",
    "`gymnasium.wrappers.MaxAndSkipObservation` 是一个包装器，用于在环境中跳过指定数量的帧（观察值），并在跳过的帧中选择最大值作为最终的观察值返回。这种方法主要用于降低决策频率，并通过考虑连续观察值的最大值来减少可能的视觉闪烁影响，这在某些游戏环境中可能会对性能产生显著影响。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 对连续4个动作的观察值进行手动计算\n",
    "obs0, *_ = env.reset(seed=123)\n",
    "obs1, *_ = env.step(1)\n",
    "obs2, *_ = env.step(1)\n",
    "obs3, *_ = env.step(1)\n",
    "obs4, *_ = env.step(1)\n",
    "\n",
    "# 计算最后两个观察值的最大值\n",
    "skip_and_max_obs = np.max(np.stack([obs3, obs4], axis=0), axis=0)\n",
    "\n",
    "# 应用 MaxAndSkipObservation 包装器\n",
    "wrapped_env = MaxAndSkipObservation(env, skip=4)\n",
    "wrapped_obs0, *_ = wrapped_env.reset(seed=123)\n",
    "\n",
    "# 执行一个动作，观察结果\n",
    "wrapped_obs1, *_ = wrapped_env.step(1)\n",
    "\n",
    "# 验证原始观察值与包装器处理后的观察值是否相同\n",
    "print(np.all(obs0 == wrapped_obs0))  # 应为 True\n",
    "print(np.all(wrapped_obs1 == skip_and_max_obs))  # 应为 True\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要应用包装器的环境。\n",
    "- **skip** (`int`): 需要跳过的帧数。默认为4，即在每个动作之后跳过三个帧，然后返回第四个帧和第三个帧的最大值。\n",
    "\n",
    "通过使用 `MaxAndSkipObservation` 包装器，开发者可以在不牺牲太多决策质量的情况下，有效降低环境中的决策频率。这种方法在处理那些具有高时间冗余性的环境时尤其有用，如某些 Atari 游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd3f70",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.NormalizeObservation(env: Env[ObsType, ActType], epsilon: float = 1e-8)\n",
    "\n",
    "\n",
    "`gymnasium.wrappers.NormalizeObservation` 是一个用于标准化环境观察值的包装器，它可以将观察值调整为以均值为中心、单位方差的分布。这对于那些观察值具有不同量级或分布的环境特别有用，因为它可以帮助学习算法更快地收敛。\n",
    "\n",
    "### 示例代码\n",
    "```python\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# 创建环境并获取一次观察值，用于展示未标准化前的观察值\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, info = env.reset(seed=123)\n",
    "term, trunc = False, False\n",
    "while not (term or trunc):\n",
    "    obs, _, term, trunc, _ = env.step(1)\n",
    "\n",
    "print(\"Observation before normalization:\", obs)\n",
    "\n",
    "# 应用 NormalizeObservation 包装器，并再次获取观察值，用于展示标准化后的观察值\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = NormalizeObservation(env)\n",
    "obs, info = env.reset(seed=123)\n",
    "term, trunc = False, False\n",
    "while not (term or trunc):\n",
    "    obs, _, term, trunc, _ = env.step(1)\n",
    "\n",
    "print(\"Observation after normalization:\", obs)\n",
    "```\n",
    "\n",
    "### 参数\n",
    "- **env** (`Env`): 要应用包装器的环境。\n",
    "- **epsilon** (`float`): 一个稳定性参数，用于在缩放观察值时防止除以0。默认值为 `1e-8`。\n",
    "\n",
    "### 注意事项\n",
    "- 观察值的标准化依赖于历史轨迹，如果包装器是新实例化的，或者最近更改了策略，则观察值可能无法正确标准化。\n",
    "- 通过 `update_running_mean` 属性，可以在评估时禁用更新运行均值/标准差，这对于使用先前计算的统计数据而不是实时更新它们特别有用。\n",
    "\n",
    "该包装器适用于那些观察值分布广泛或在不同尺度上的环境，有助于提高学习效率和稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f6237",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.AddRenderObservation(env: Env[ObsType, ActType], render_only: bool = True, render_key: str = 'pixels', obs_key: str = 'state')\n",
    "\n",
    "`gymnasium.wrappers.AddRenderObservation` 包装器可以将环境的渲染观测值（例如，RGB图像）包含在环境的观测值中。这对于需要同时处理原始环境状态和渲染图像的情况特别有用。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "**替换观测值为渲染图像**:\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# 创建环境并应用包装器，仅返回渲染图像作为观测值\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = AddRenderObservation(env, render_only=True)\n",
    "print(\"Observation space after adding render:\", env.observation_space)\n",
    "\n",
    "# 重置环境并比较观测值和渲染图像\n",
    "obs, _ = env.reset(seed=123)\n",
    "image = env.render()\n",
    "assert np.all(obs == image), \"The observation should be the rendered image\"\n",
    "\n",
    "# 执行动作并再次比较\n",
    "obs, *_ = env.step(env.action_space.sample())\n",
    "image = env.render()\n",
    "assert np.all(obs == image), \"The observation should be the rendered image after step\"\n",
    "```\n",
    "\n",
    "**将渲染图像添加到原始观测值**:\n",
    "```python\n",
    "# 创建环境并应用包装器，同时保留原始观测值和渲染图像\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = AddRenderObservation(env, render_only=False)\n",
    "print(\"Observation space after adding render:\", env.observation_space)\n",
    "\n",
    "# 重置环境并检查观测值字典的键\n",
    "obs, info = env.reset(seed=123)\n",
    "print(\"Keys in observation:\", obs.keys())\n",
    "\n",
    "# 检查原始状态和渲染图像是否正确包含在观测值中\n",
    "assert \"state\" in obs and \"pixels\" in obs, \"Observation should contain 'state' and 'pixels'\"\n",
    "assert np.all(obs[\"pixels\"] == env.render()), \"The 'pixels' key should contain the rendered image\"\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要包装的环境实例。\n",
    "- **render_only** (`bool`): 如果为`True`，则只返回渲染图像作为观测值；如果为`False`，则在观测值字典中既包含原始观测值也包含渲染图像。\n",
    "- **render_key** (`str`): 用于指定包含渲染图像的键名，默认为`\"pixels\"`。\n",
    "- **obs_key** (`str`): 用于指定包含原始观测值的键名，默认为`\"state\"`。\n",
    "\n",
    "### 注意事项\n",
    "- 当`render_only=True`时，原始观测值将被丢弃，只保留渲染图像作为观测值。\n",
    "- 当`render_only=False`时，观测值将是一个字典，包含原始状态（`\"state\"`键）和渲染图像（`\"pixels\"`键）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c35e1",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.ResizeObservation(env: Env[ObsType, ActType], shape: tuple[int, int])\n",
    "\n",
    "`gymnasium.wrappers.ResizeObservation` 包装器允许调整环境观测值图像的大小。这是通过使用 OpenCV 库来完成的，可以将图像重新缩放到指定的维度。这个包装器特别适合于需要将图像输入到模型中但图像尺寸不匹配的情况。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import ResizeObservation\n",
    "\n",
    "# 创建环境并应用包装器以调整观测图像的大小\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "print(\"Original observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "# 应用 ResizeObservation 包装器以将观测图像大小调整为 32x32\n",
    "resized_env = ResizeObservation(env, (32, 32))\n",
    "print(\"Resized observation space shape:\", resized_env.observation_space.shape)\n",
    "\n",
    "# 重置环境并获取调整大小后的观测值\n",
    "obs, _ = resized_env.reset()\n",
    "print(\"Shape of resized observation:\", obs.shape)\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要包装的环境实例。\n",
    "- **shape** (`tuple[int, int]`): 调整后的图像观测值的大小，必须提供为两个整数的元组，表示新的图像高度和宽度。\n",
    "\n",
    "### 注意事项\n",
    "- 这个包装器通过使用 OpenCV 库来调整图像的大小，因此需要确保已安装了 `opencv-python` 包。\n",
    "- `shape` 参数指定了调整后的图像尺寸，通常用于将图像尺寸标准化以适应深度学习模型的输入要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859cf36",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.ReshapeObservation(env: gym.Env[ObsType, ActType], shape: int | tuple[int, ...])\n",
    "\n",
    "`gymnasium.wrappers.ReshapeObservation` 包装器允许将基于数组的观测值重塑为指定的形状。这可以用于调整观测值的维度，以适应特定模型或处理流程的需要。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import ReshapeObservation\n",
    "\n",
    "# 创建环境并查看原始观测空间的形状\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "print(\"Original observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "# 应用 ReshapeObservation 包装器以改变观测值的形状\n",
    "reshape_env = ReshapeObservation(env, (24, 4, 96, 1, 3))\n",
    "print(\"Reshaped observation space shape:\", reshape_env.observation_space.shape)\n",
    "\n",
    "# 重置环境并获取重塑后的观测值\n",
    "obs, _ = reshape_env.reset()\n",
    "print(\"Shape of reshaped observation:\", obs.shape)\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要包装的环境实例。\n",
    "- **shape** (`int | tuple[int, ...]`): 期望的新形状。可以是一个整数或一个整数的元组，表示新的观测数组维度。\n",
    "\n",
    "### 注意事项\n",
    "- `shape` 参数定义了观测值的新形状，需要确保重塑后的形状与原始数据中的总元素数相匹配。例如，如果原始观测值的形状是 `(96, 96, 3)`（即总共有 27648 个元素），则重塑形状 `(24, 4, 96, 1, 3)` 也应该包含相同数量的元素。\n",
    "- 此包装器特别适用于需要调整观测数据形状以适应特定神经网络架构的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4ed3a",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.RescaleObservation(env: gym.Env[ObsType, ActType], min_obs: np.floating | np.integer | np.ndarray, max_obs: np.floating | np.integer | np.ndarray)\n",
    "\n",
    "`gymnasium.wrappers.RescaleObservation` 包装器允许您对环境中的观测空间进行仿射（线性）缩放，以便观测值位于 `[min_obs, max_obs]` 范围内。这可以用来标准化观测值，使其适应模型的输入要求或提高学习算法的效率。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleObservation\n",
    "import numpy as np\n",
    "\n",
    "# 创建环境并查看原始观测空间\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "print(\"Original observation space:\", env.observation_space)\n",
    "\n",
    "# 应用 RescaleObservation 包装器以改变观测空间的范围\n",
    "env = RescaleObservation(env, np.array([-2, -1, -10], dtype=np.float32), np.array([1, 0, 1], dtype=np.float32))\n",
    "print(\"Rescaled observation space:\", env.observation_space)\n",
    "\n",
    "# 重置环境并获取重塑后的观测值\n",
    "obs, _ = env.reset()\n",
    "print(\"Rescaled observation:\", obs)\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要包装的环境实例。\n",
    "- **min_obs** (`np.floating | np.integer | np.ndarray`): 新的最小观测界限。可以是标量或与环境观测空间形状相同的数组。\n",
    "- **max_obs** (`np.floating | np.integer | np.ndarray`): 新的最大观测界限。同样，可以是标量或与环境观测空间形状相同的数组。\n",
    "\n",
    "### 注意事项\n",
    "- 此包装器特别适用于需要调整观测数据范围以适应特定神经网络架构的场景。\n",
    "- 在设置 `min_obs` 和 `max_obs` 时，确保其形状与环境的观测空间相匹配，或者使用标量值来统一调整所有维度。\n",
    "- 通过重新缩放观测值，可以帮助改进学习算法的性能，特别是在观测值的原始范围很大或非常不均匀时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ba978",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.TimeAwareObservation(env: Env[ObsType, ActType], flatten: bool = True, normalize_time: bool = False, *, dict_time_key: str = 'time')\n",
    "\n",
    "`gymnasium.wrappers.TimeAwareObservation` 包装器通过增加步骤计数来扩展观测空间，以便在每个观测中包含当前在一集中进行的步数信息。这可以帮助代理了解它在一集中的进度，有时可以改善学习效果。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeAwareObservation\n",
    "\n",
    "# 创建环境并应用 TimeAwareObservation 包装器\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = TimeAwareObservation(env, flatten=False, normalize_time=True)\n",
    "\n",
    "# 查看扩展后的观测空间\n",
    "print(\"Observation space after TimeAwareObservation:\", env.observation_space)\n",
    "\n",
    "# 重置环境并获取观测值\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(\"Initial observation with time:\", obs)\n",
    "\n",
    "# 执行一个动作并获取新的观测值\n",
    "obs, _, _, _, _ = env.step(env.action_space.sample())\n",
    "print(\"Observation with time after one step:\", obs)\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要应用包装器的环境。\n",
    "- **flatten** (`bool`): 如果设置为True，则将观测值展平为单一维度的Box。默认值为True。\n",
    "- **normalize_time** (`bool`): 如果设置为True，则时间返回值在[0,1]范围内，代表距离截断前的剩余时间步数的归一化值。如果为False，则返回时间为整数步数。默认值为False。\n",
    "- **dict_time_key** (`str`): 对于具有Dict观测空间的环境，时间空间的键值。默认为“time”。\n",
    "\n",
    "### 注意事项\n",
    "- 此包装器可以提高代理的时间感知能力，尤其是在需要考虑一集中的进度时。\n",
    "- 如果环境已经具有`Dict`类型的观测空间，时间信息将作为新的键值对加入；否则，观测空间将转换为包含原始观测和时间信息的`Dict`。\n",
    "- 使用`flatten`参数可以将观测空间简化为一维数组，这可能有助于某些类型的学习模型处理观测数据。\n",
    "- 通过`normalize_time`参数，您可以选择将时间信息归一化，以便于模型处理不同长度集的统一表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c597296",
   "metadata": {},
   "source": [
    "# Reward Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39ec40",
   "metadata": {},
   "source": [
    "### class gymnasium.RewardWrapper(env: Env[ObsType, ActType])\n",
    "\n",
    "\n",
    "`gymnasium.RewardWrapper` 类为开发者提供了一种简便的方法来修改环境在每一步骤后返回的奖励值。通过继承这个类并重写 `reward` 方法，可以根据特定的需求调整奖励机制，这对于奖励形状的设计（reward shaping）来说非常有用，因为它允许开发者引入额外的奖励逻辑来帮助代理更快地学习或者优化其学习策略。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "下面的示例演示了如何创建一个自定义奖励包装器，该包装器将环境返回的原始奖励乘以一个固定的因子：\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.wrappers import RewardWrapper\n",
    "\n",
    "class MultiplyReward(RewardWrapper):\n",
    "    def __init__(self, env: Env, multiplier: float):\n",
    "        super().__init__(env)\n",
    "        self.multiplier = multiplier\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # 将环境返回的奖励乘以指定的因子\n",
    "        return reward * self.multiplier\n",
    "\n",
    "# 创建环境实例\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 应用自定义的奖励包装器，将所有奖励乘以2\n",
    "wrapped_env = MultiplyReward(env, multiplier=2.0)\n",
    "\n",
    "# 使用包装后的环境\n",
    "obs = wrapped_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = wrapped_env.action_space.sample()  # 随机选择动作\n",
    "    obs, reward, done, _ = wrapped_env.step(action)\n",
    "    print(f\"Modified reward: {reward}\")\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要被包装的环境实例。\n",
    "- **reward** (`SupportsFloat`): 环境在一步操作后返回的原始奖励值。\n",
    "\n",
    "### `reward` 方法\n",
    "- 输入参数 `reward` 是环境在一步操作后返回的原始奖励值。\n",
    "- 返回值是修改后的奖励值，可以根据实际需要进行任意形式的转换，例如放大、缩小、加固定值等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b0f63",
   "metadata": {},
   "source": [
    "## Implemented Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93ba71",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.TransformReward(env: Env[ObsType, ActType], func: Callable[[SupportsFloat], SupportsFloat])\n",
    "\n",
    "\n",
    "`gymnasium.wrappers.TransformReward` 类允许开发者对环境在每一步操作后返回的奖励值进行自定义变换。通过这种方式，可以根据需要调整奖励机制，比如引入额外的奖励逻辑或对奖励值进行缩放、平移等操作，以帮助代理学习更有效的策略。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "下面的代码示例展示了如何使用 `TransformReward` 包装器将所有奖励值乘以2再加1：\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformReward\n",
    "\n",
    "# 定义一个函数，对奖励进行变换\n",
    "def modify_reward(reward):\n",
    "    return 2 * reward + 1\n",
    "\n",
    "# 创建环境实例\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 应用TransformReward包装器，传入自定义的奖励变换函数\n",
    "wrapped_env = TransformReward(env, func=modify_reward)\n",
    "\n",
    "# 重置环境并进行一步动作\n",
    "obs = wrapped_env.reset()\n",
    "action = wrapped_env.action_space.sample()  # 随机选择动作\n",
    "_, reward, _, _ = wrapped_env.step(action)\n",
    "\n",
    "print(f\"Modified reward: {reward}\")\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要被包装的环境实例。\n",
    "- **func** (`Callable[[SupportsFloat], SupportsFloat]`): 应用于每一步返回奖励的函数。这个函数接受一个浮点数（原始奖励值）作为输入，并返回一个浮点数（变换后的奖励值）作为输出。\n",
    "\n",
    "`TransformReward` 包装器通过将 `func` 函数应用于环境的每一步奖励，使得奖励值的变换变得简单而灵活，为奖励设计提供了广泛的可能性。这可以是非常有用的，特别是在需要对原始奖励进行调整以提高学习效率或改进策略性能的场景中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d5637",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.NormalizeReward(env: Env[ObsType, ActType], gamma: float = 0.99, epsilon: float = 1e-8)\n",
    "\n",
    "`gymnasium.wrappers.NormalizeReward` 类用于标准化环境返回的即时奖励，使得奖励的指数移动平均具有固定的方差。这种处理方式旨在帮助学习算法更稳定和有效地学习，尤其是在奖励尺度变化大的环境中。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "以下代码示例演示如何使用 `NormalizeReward` 包装器来标准化奖励：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import NormalizeReward\n",
    "\n",
    "# 创建环境实例并应用NormalizeReward包装器\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "wrapped_env = NormalizeReward(env, gamma=0.99, epsilon=1e-8)\n",
    "\n",
    "# 重置环境并开始一个新的episode\n",
    "_ = wrapped_env.reset(seed=123)\n",
    "_ = wrapped_env.action_space.seed(123)\n",
    "\n",
    "# 初始化奖励记录器\n",
    "episode_rewards = []\n",
    "terminated, truncated = False, False\n",
    "\n",
    "# 运行一个episode，收集并记录奖励\n",
    "while not (terminated or truncated):\n",
    "    _, reward, terminated, truncated, _ = wrapped_env.step(wrapped_env.action_space.sample())\n",
    "    episode_rewards.append(reward)\n",
    "\n",
    "# 计算并打印标准化后的奖励方差\n",
    "print(\"Variance of normalized rewards:\", np.var(episode_rewards))\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要被包装的环境实例。\n",
    "- **gamma** (`float`): 在指数移动平均中使用的折扣因子。默认值为 `0.99`。\n",
    "- **epsilon** (`float`): 稳定性参数，用于防止除以零的情况。默认值为 `1e-8`。\n",
    "\n",
    "使用 `NormalizeReward` 包装器可以帮助处理奖励值跨度很大或分布不均的问题，通过将奖励标准化到一个较小的范围内，有助于提高学习算法的稳定性和性能。在实际使用时，可能需要根据具体任务调整 `gamma` 和 `epsilon` 参数以达到最佳效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f1f4d",
   "metadata": {},
   "source": [
    "### class gymnasium.wrappers.ClipReward(env: gym.Env[ObsType, ActType], min_reward: float | np.ndarray | None = None, max_reward: float | np.ndarray | None = None)\n",
    "\n",
    "\n",
    "`gymnasium.wrappers.ClipReward` 类用于将环境返回的奖励裁剪到指定的上下界之间。这个包装器在奖励值范围过大或希望将奖励限制在特定范围内时非常有用，有助于改善学习算法的稳定性和性能。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "以下代码示例展示了如何使用 `ClipReward` 包装器来裁剪奖励：\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import ClipReward\n",
    "\n",
    "# 创建环境实例并应用ClipReward包装器\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "wrapped_env = ClipReward(env, min_reward=0, max_reward=0.5)\n",
    "\n",
    "# 重置环境并开始一个新的episode\n",
    "_ = wrapped_env.reset()\n",
    "\n",
    "# 执行一个动作，并观察裁剪后的奖励值\n",
    "_, reward, _, _, _ = wrapped_env.step(1)\n",
    "\n",
    "# 打印裁剪后的奖励值\n",
    "print(\"Clipped reward:\", reward)\n",
    "```\n",
    "\n",
    "### 参数解读\n",
    "- **env** (`Env`): 要被包装的环境实例。\n",
    "- **min_reward** (`float` 或 `np.ndarray`): 奖励的下界。如果设置为 `None`，则不应用下界裁剪。\n",
    "- **max_reward** (`float` 或 `np.ndarray`): 奖励的上界。如果设置为 `None`，则不应用上界裁剪。\n",
    "\n",
    "使用 `ClipReward` 包装器可以帮助控制学习过程中奖励的范围，特别是在奖励的大小直接影响学习过程的环境中。通过限制奖励的范围，可以防止学习算法因为极端奖励值而出现不稳定的情况。在实际使用时，可能需要根据具体任务调整 `min_reward` 和 `max_reward` 参数以达到最佳学习效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
