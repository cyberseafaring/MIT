{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e00dcc",
   "metadata": {},
   "source": [
    "# 基础用法 Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d54cb6",
   "metadata": {},
   "source": [
    "在Jupyter环境下使用Gymnasium库进行基础的深度强化学习实验是一个直观且高效的方法。以下是Gymnasium库的基本使用方法，这将帮助你快速开始在Jupyter Notebook中的强化学习项目。\n",
    "\n",
    "## 初始化环境\n",
    "\n",
    "首先，需要导入Gymnasium库并使用`make()`函数初始化一个环境。这里以\"CartPole-v1\"为例：\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "# 初始化环境\n",
    "env = gym.make('CartPole-v1')\n",
    "```\n",
    "\n",
    "`make()`函数返回一个环境对象，你可以通过这个对象与环境进行交互。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f801e",
   "metadata": {},
   "source": [
    "## 智能体如何与环境交互\n",
    "\n",
    "在强化学习中，智能体（agent）与环境（environment）的交互遵循一个典型的循环：智能体接收环境的观察（observation），基于观察做出行动（action），环境根据行动返回新的观察、奖励（reward）和是否终止（terminated）的标志。\n",
    "\n",
    "以下是一个简单的示例，展示了如何在Jupyter Notebook中实现这一交互过程：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa37d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请在vscode中运行！！\n",
    "\n",
    "# 需要安装swig和gymnasium[box2d]，可以通过pip安装\n",
    "# pip install swig\n",
    "# pip install gymnasium[box2d]\n",
    "\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # 选择一个随机动作\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff6775",
   "metadata": {},
   "source": [
    "示例代码演示了如何使用Gymnasium库在深度强化学习中创建和交互一个环境。下面详细解释代码中的关键部分，特别是`make()`, `Env.reset()`, `Env.step()`和`Env.render()`方法的使用。\n",
    "\n",
    "### 初始化环境\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "```\n",
    "\n",
    "- `import gymnasium as gym`：导入Gymnasium库，并简称为`gym`以便于使用。\n",
    "- `env = gym.make(\"LunarLander-v2\", render_mode=\"human\")`：使用`gym.make()`函数创建一个环境实例。这里的`\"LunarLander-v2\"`是环境的ID，指定了要创建的具体游戏或任务。`render_mode=\"human\"`参数指定环境应该如何被渲染。在这个例子中，它设置为`\"human\"`模式，意味着环境会以一种适合人类观察的方式进行渲染。在Jupyter Notebook中，可能需要调整渲染模式以兼容显示输出。\n",
    "\n",
    "### 重置环境\n",
    "\n",
    "```python\n",
    "observation, info = env.reset()\n",
    "```\n",
    "\n",
    "- `env.reset()`：在开始新的回合（episode）之前，需要重置环境到初始状态。`reset()`方法返回环境的初始观察值。在某些环境中，`reset()`还会返回额外的信息`info`，这些信息可能包含环境的特定状态信息，但并不直接影响决策过程。\n",
    "\n",
    "### 与环境交互\n",
    "\n",
    "```python\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # 使用观察和信息的智能体策略\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "```\n",
    "\n",
    "- `action = env.action_space.sample()`：选择一个动作。在这个示例中，动作是通过调用环境的动作空间中的`sample()`方法随机选取的。这代表了一个简单的“策略”，即在可用动作中随机选择。\n",
    "- `observation, reward, terminated, truncated, info = env.step(action)`：执行动作，并接收环境的反馈。`env.step(action)`方法接收一个动作作为输入，并返回新的观察值`observation`、此动作获得的奖励`reward`、是否达到终止状态`terminated`、是否因为某些条件而提前终止`truncated`，以及额外的信息`info`。`terminated`表示是否达到了环境的自然结束条件，如任务完成或失败；`truncated`表示回合是否因为达到了步数限制或其他非自然结束条件而终止。\n",
    "- `episode_over = terminated or truncated`：使用`terminated`和`truncated`的逻辑或结果来决定是否结束当前回合。如果任一为`True`，则结束回合。\n",
    "\n",
    "### 渲染环境\n",
    "\n",
    "在示例代码中没有直接使用`Env.render()`方法，因为在初始化环境时已经通过`render_mode=\"human\"`指定了渲染模式。然而，`Env.render()`方法是用来更新和渲染环境的状态，使用户能够以视觉方式观察智能体在环境中的行为。在一些应用场景中，如需要在每个时间步后可视化环境的状态，可以在循环中调用`env.render()`。\n",
    "\n",
    "### 结束环境\n",
    "\n",
    "```python\n",
    "env.close()\n",
    "```\n",
    "\n",
    "- `env.close()`：在完成环境交互后，使用`close()`方法关闭环境。这是一个好习惯，特别是在使用渲染或开启了外部资源（如窗口）的环境时，可以通过此方法释放这些资源。\n",
    "\n",
    "这个示例提供了使用Gymnasium进行强化学习实验的基础框架，包括初始化环境、进行观察和动作选择、处理环境反馈以及渲染和关闭环境。在实际应用中，智能体的策略会更加复杂，不仅仅是随机选择动作，而是基于观察结果来做出决策，以达到最大化累积奖励的目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90035b4e",
   "metadata": {},
   "source": [
    "## 动作和状态空间 Action and observation spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad3c1a",
   "metadata": {},
   "source": [
    "在Gymnasium库中，每个环境都有定义好的动作空间（Action Space）和观察空间（Observation Space），这些空间为智能体与环境之间的交互提供了结构化的界面。理解这些空间的表示方法对于设计和实现强化学习算法至关重要。\n",
    "\n",
    "### 动作空间（Action Space）\n",
    "\n",
    "动作空间定义了智能体可以在环境中执行的所有可能动作。根据不同的环境，动作可以是离散的、连续的，或者是更复杂的结构。以下是几种常见的动作空间表示方法：\n",
    "\n",
    "1. **离散空间（Discrete）**：在这种空间中，存在一个有限的动作集合，智能体在每一步可以从中选择一个动作执行。例如，在一个简单的游戏中，动作可能包括向上、向下、向左、向右移动，每个动作都对应一个离散的值。\n",
    "\n",
    "2. **连续空间（Box）**：用于表示动作可以取任何值的情况，通常在一定范围内。例如，控制一个机器人的关节角度，动作值可能是一个连续范围内的实数。\n",
    "\n",
    "3. **多离散空间（MultiDiscrete）**：这种空间由多个离散动作空间组成，每个空间有不同的动作数量。比如，一个智能体同时控制多个独立的部分，每部分的动作选择是离散的。\n",
    "\n",
    "4. **多二进制空间（MultiBinary）**：在这种空间中，每个动作都是二进制的，即0或1，表示开或关的状态。适用于需要同时判断多个是/否条件的情况。\n",
    "\n",
    "### 状态空间（Observation Space）\n",
    "\n",
    "观察空间定义了智能体可以接收到的环境状态信息的可能值。这些观察值可以是环境的直接表示，也可以是环境状态的一些抽象或加工。以下是一些常见的观察空间表示方法：\n",
    "\n",
    "1. **盒子空间（Box）**：这是最常见的观察空间类型，用于表示观察值可以在连续区间内取任意值。观察值可以是多维的，例如图像数据可以表示为高度、宽度和颜色通道的三维数组。\n",
    "\n",
    "2. **离散空间（Discrete）**：用于表示观察值是有限个离散状态中的一个。例如，在一个迷宫游戏中，每个位置可以用一个离散值表示。\n",
    "\n",
    "3. **字典空间（Dict）**：这种空间允许将多个不同类型的空间组合成一个复合观察空间。例如，一个观察可以同时包含视觉图像（Box空间）和智能体的健康值（Discrete空间）。\n",
    "\n",
    "4. **元组空间（Tuple）**：类似于字典空间，元组空间允许将多个空间组合起来，但不为每个空间命名。它简单地将它们组织成一个序列。\n",
    "\n",
    "5. **其他空间**：包括`MultiDiscrete`和`MultiBinary`等，也可以用于观察空间，方法与动作空间相同。\n",
    "\n",
    "### 举例说明\n",
    "\n",
    "假设有一个自动驾驶模拟环境：\n",
    "\n",
    "- **动作空间**可能是一个`Box`空间，代表汽车的加速度和转向角度，这两个动作都可以在连续的范围内取值。\n",
    "- **观察空间**可能是一个`Box`空间，代表从汽车摄像头捕\n",
    "\n",
    "获的图像数据，这些数据可以是一个三维数组，包含图像的高度、宽度和颜色通道。\n",
    "\n",
    "### 实用性\n",
    "\n",
    "了解动作和观察空间的表示方法对于设计智能体的学习算法非常重要，因为算法需要根据观察空间处理输入数据，并在动作空间内生成响应的动作。此外，这些空间的定义还帮助智能体理解其可以执行的动作种类以及可以期待接收的环境反馈类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca376061",
   "metadata": {},
   "source": [
    "## 修改环境\n",
    "\n",
    "Gymnasium提供了包装器（wrappers）来修改现有环境，例如修改观察的格式或奖励结构。以下是如何应用一个简单的包装器来修改环境的观察空间：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# 使用FlattenObservation包装器修改观察空间\n",
    "wrapped_env = FlattenObservation(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088f32d",
   "metadata": {},
   "source": [
    "这样，我们可以根据需要修改环境，以适应强化学习模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c53a24",
   "metadata": {},
   "source": [
    "在Gymnasium库中，包装器（Wrappers）是用来修改环境的一种非常灵活的方法，它们可以改变环境的行为或观察/动作空间，而不需要直接修改环境本身的代码。这使得实验设置更加模块化和可重用。下面详细说明几种常用包装器的用法。\n",
    "\n",
    "### TimeLimit\n",
    "\n",
    "`TimeLimit`包装器用于在环境中设置一个最大时间步数限制。如果在一个回合（episode）中达到了这个限制，环境会发出一个截断信号（`truncated`），表明回合被非自然原因终止了。这对于防止智能体在某些环境中陷入无限循环非常有用。\n",
    "\n",
    "**示例代码**：\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "# 创建环境并应用TimeLimit包装器\n",
    "env = gym.make('CartPole-v1')\n",
    "env = TimeLimit(env, max_episode_steps=100)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if 'TimeLimit.truncated' in info:\n",
    "        print(\"回合被截断，因为达到了最大步数限制。\")\n",
    "        break\n",
    "```\n",
    "\n",
    "### ClipAction\n",
    "\n",
    "`ClipAction`包装器确保所有传递给`step`函数的动作都位于基础环境的动作空间内。这在处理连续动作空间时特别有用，可以防止智能体尝试执行超出环境允许范围的动作。\n",
    "\n",
    "**示例代码**：\n",
    "\n",
    "```python\n",
    "from gymnasium.wrappers import ClipAction\n",
    "\n",
    "# 假设env是一个连续动作空间的环境\n",
    "env = ClipAction(env)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # 这里的动作会被自动裁剪到合法范围\n",
    "    observation, reward, done, info = env.step(action)\n",
    "```\n",
    "\n",
    "### RescaleAction\n",
    "\n",
    "`RescaleAction`包装器应用一个仿射变换（线性缩放加上位移），将动作空间中的动作线性缩放到新的上下界。这对于将智能体输出的动作调整到环境期望的动作范围内非常有用。\n",
    "\n",
    "**示例代码**：\n",
    "\n",
    "```python\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "\n",
    "# 假设env是一个连续动作空间的环境\n",
    "# 将动作空间从[-1, 1]缩放到[0, 1]\n",
    "env = RescaleAction(env, min_action=0, max_action=1)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # 这里的动作会被自动缩放到新的范围\n",
    "    observation, reward, done, info = env.step(action)\n",
    "```\n",
    "\n",
    "### TimeAwareObservation\n",
    "\n",
    "`TimeAwareObservation`包装器向观察添加了当前时间步的索引信息。这在某些情况下有助于确保状态转换满足马尔可夫性质（即下一个状态仅依赖于当前状态和动作）。\n",
    "\n",
    "**示例代码**：\n",
    "\n",
    "```python\n",
    "from gymnasium.wrappers import TimeAwareObservation\n",
    "\n",
    "# 创建环境并应用TimeAwareObservation包装器\n",
    "env = TimeAwareObservation(env)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(f\"步数: {step}, 观察: {observation}\")\n",
    "    step += 1\n",
    "```\n",
    "\n",
    "通过这些包装器，可以轻松地对环境进行定制化修改，以满足不同的实验需求或算法要求，而无需改动环境本身的实现代码。这\n",
    "\n",
    "种方法提高了代码的重用性和实验的灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64197f6",
   "metadata": {},
   "source": [
    "# 训练一个智能体 Training an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57289c5b",
   "metadata": {},
   "source": [
    "我们将通过构建一个基于表格的Q学习智能体来解决Gymnasium库中的Blackjack v1环境。Q学习是一种无模型的、离线策略学习算法，适用于具有离散动作空间的环境。下面，将逐步说明如何训练这个智能体。\n",
    "\n",
    "### 了解Blackjack和Q学习\n",
    "\n",
    "- **Blackjack**是一种流行的赌场纸牌游戏，本版本使用无限牌堆（即有替换地抽牌），所以计牌策略在我们的模拟游戏中不可行。观察是一个元组，包含玩家当前的牌面总和、庄家面朝上的牌的值和一个布尔值（表示玩家是否持有可用的王牌）。\n",
    "- **Q学习**是Watkins在1989年提出的一种模型自由、离线策略学习算法，适用于离散动作空间的环境，并在特定条件下首次证明了算法收敛到最优策略。\n",
    "\n",
    "### 执行动作\n",
    "\n",
    "在接收到第一个观察后，我们将使用`env.step(action)`函数与环境互动。此函数接受一个动作作为输入，在环境中执行该动作，并返回四个有用的变量：下一个观察、奖励、是否终止和是否截断。\n",
    "\n",
    "### 构建智能体\n",
    "\n",
    "我们将构建一个Q学习智能体，使用ε-贪婪策略来确保既能探索环境又能利用当前知识。以下是智能体的代码实现：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e585cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 请在vscode中运行！！！\n",
    "\n",
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class BlackjackAgent:\n",
    "    def __init__(self, env, learning_rate, initial_epsilon, epsilon_decay, final_epsilon, discount_factor=0.95):\n",
    "        self.env = env\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        self.q_values[obs][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c90490",
   "metadata": {},
   "source": [
    "\n",
    "### 训练智能体\n",
    "\n",
    "为了训练智能体，我们将让它一次玩一局（一个完整的游戏称为一个回合），并在每个回合后更新它的Q值。智能体需要经历许多回合来充分探索环境。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b04b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(env=gym.make(\"Blackjack-v1\"), learning_rate=learning_rate, initial_epsilon=start_epsilon, epsilon_decay=epsilon_decay, final_epsilon=final_epsilon)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b9ed7",
   "metadata": {},
   "source": [
    "### 可视化策略\n",
    "\n",
    "在训练完成后，我们可以通过评估智能体在一定数量的回合中的表现来可视化其学习到的策略。注意，由于渲染会显著降低训练速度，所以最好在训练循环外单独进行可视化。\n",
    "\n",
    "通过这个指导，我们应该能够理解如何使用Gymnasium环境与强化学习算法交互，并开始解决更多RL挑战。建议自己解决这个环境，应用最喜欢的离散RL算法，或尝试使用Monte Carlo ES（在Sutton & Barto的书《强化学习：介绍》第5.3节中有介绍），这样可以直接将结果与书中的结果进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ba887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd171c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89128d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea7197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "622d601f",
   "metadata": {},
   "source": [
    "# 记录智能体 Recording Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73171d6f",
   "metadata": {},
   "source": [
    "在训练或评估智能体时，记录智能体在整个回合（episode）的行为和累积的总奖励是很有趣的。这可以通过两个包装器（Wrappers）实现：`RecordEpisodeStatistics`和`RecordVideo`。前者跟踪回合数据，如总奖励、回合长度和所需时间；后者生成智能体使用环境渲染的mp4视频。\n",
    "\n",
    "### 记录每个回合\n",
    "\n",
    "如果想在评估期间记录多个回合来观察智能体的行为，可以使用以下示例脚本：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "num_eval_episodes = 4\n",
    "\n",
    "# 创建环境并应用包装器\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  # 替换为custom环境\n",
    "env = RecordVideo(env, video_folder=\"cartpole-agent\", name_prefix=\"eval\",\n",
    "                  episode_trigger=lambda x: True)  # 记录每个回合\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)  # 记录统计信息\n",
    "\n",
    "for episode_num in range(num_eval_episodes):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # 替换为实际智能体\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "env.close()\n",
    "\n",
    "# 打印统计信息\n",
    "print(f'Episode time taken: {env.time_queue}')\n",
    "print(f'Episode total rewards: {env.return_queue}')\n",
    "print(f'Episode lengths: {env.length_queue}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b128f",
   "metadata": {},
   "source": [
    "在上述脚本中，`RecordVideo`包装器用于指定视频保存的文件夹、视频文件的前缀，以及每个回合是否记录的触发条件。`RecordEpisodeStatistics`仅需要指定缓冲区长度，即内部`time_queue`、`return_queue`和`length_queue`的最大长度。\n",
    "\n",
    "### 训练期间记录智能体\n",
    "\n",
    "在训练期间，智能体将经历数百或数千个回合，因此不可能为每个回合都记录视频。但开发者可能仍然想知道智能体在训练的不同阶段的行为，因此可以在训练期间定期记录回合，同时记录每个回合的统计数据。以下脚本展示了如何在训练期间定期记录智能体的回合，同时记录每个回合的统计信息：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "training_period = 250  # 每250个回合记录一次智能体的回合\n",
    "num_training_episodes = 10000  # 总训练回合数\n",
    "\n",
    "# 创建环境并应用包装器\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  # 替换为你的环境\n",
    "env = RecordVideo(env, video_folder=\"cartpole-agent\", name_prefix=\"training\",\n",
    "                  episode_trigger=lambda x: x % training_period == 0)  # 定期记录视频\n",
    "env = RecordEpisodeStatistics(env)  # 记录统计信息\n",
    "\n",
    "for episode_num in range(num_training_episodes):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # 替换为实际智能体\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    logging.info(f\"episode-{episode_num}\", info[\"episode\"])\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ba6a0",
   "metadata": {},
   "source": [
    "\n",
    "这种方式允许开发者在不影响训练效率的同时，了解智能体在不同训练阶段的表现。此外，通过`RecordEpisodeStatistics`包装器收集的统计数据可以用于分析智能体的学习进度和行为模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d7d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55c044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d65736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a7404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a16f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ad5720",
   "metadata": {},
   "source": [
    "# 提升训练速度方法 Speeding Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd63c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cdf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df8058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce0992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a3d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922d503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69767e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
