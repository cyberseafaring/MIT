{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc42a36a",
   "metadata": {},
   "source": [
    "# 回调函数（Callbacks）\n",
    "\n",
    "\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "在强化学习环境中，模型通常需要大量的迭代来训练以获得良好的性能。在此过程中，监控模型的中间状态（例如，性能、奖励等）对于了解训练进展和调试至关重要。此外，高效地对模型进行保存、调整和干预同样重要。这些正是回调存在的用途。\n",
    "\n",
    "### 参数说明\n",
    "\n",
    "回调机制可能接收以下类型的参数或设置：\n",
    "\n",
    "- **回调触发条件**：定义了各个回调函数在训练过程中的触发时机，例如每迭代N次后或在每个周期结束时。\n",
    "- **内部状态信息**：定义了在执行回调函数时可以获取或修改的RL模型的内部状态信息，例如观测值、回报值、训练迭代数等。\n",
    "\n",
    "### 实现功能\n",
    "\n",
    "我们可以归结出几个关键的功能点：\n",
    "\n",
    "- **监控**：动态跟踪训练过程中的关键性能指标。\n",
    "- **自动保存**：在满足既定条件时自动保存RL模型，如每N次迭代自动存档。\n",
    "- **模型操控**：可以在训练进行中动态调整RL模型的参数或结构。\n",
    "- **进度反馈**：通过进度条直观了解训练过程的完成度。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "这里我们将给出一个基本示例，使用`stable_baselines3`的回调机制来监控一个简单RL模型的平均奖励，并且实现一个定期自动保存机制。这个过程模拟了在解析上述英文材料时的代码实践。\n",
    "\n",
    "首先，需要确保你的环境中已经正确安装了`stable_baselines3`和`gym`环境。然后，参考以下代码来实践：\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gym\n",
    "\n",
    "# 初始化环境\n",
    "env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 创建Checkpoint和Eval回调\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/', name_prefix='rl_model')\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/best_model/', log_path='./logs/', eval_freq=500)\n",
    "\n",
    "# 开始训练，同时加入我们的回调\n",
    "model.learn(total_timesteps=10000, callback=[checkpoint_callback, eval_callback])\n",
    "```\n",
    "\n",
    "这个简单的例证解释了如何在`stable_baselines3`的PPO算法中结合使用了监控、自动保存等功能的回调。在实际使用过程中，你可以根据自身的需求灵活设计和结合不同的回调函数。希望通过本例你能对回调的强大之处有所体会。如果需要进阶使用或详细参数配置，请参照`stable_baselines3`的[官方文档](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html)。你可以在Jupyter Notebook上运行这些代码来查看实践效\n",
    "\n",
    "果。如有任何问题，欢迎咨询。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746cc3b",
   "metadata": {},
   "source": [
    "## 自定义回调函数Custom Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda9f3a",
   "metadata": {},
   "source": [
    "自定义回调（Custom Callback）允许根据训练期间的不同事件执行自定义的动作或检查。\n",
    "\n",
    "### 什么是自定义回调\n",
    "\n",
    "自定义回调（Custom Callback）是根据基本回调（BaseCallback）派生出的类，它能够插入到模型训练的关键时间点上，如训练开始、每一步之后、每轮rollout开始和结束时，以及训练结束时。通过重载基本回调中的特定方法，你可以访问并操纵训练期间的模型状态、环境和进度信息。\n",
    "\n",
    "### 实现功能\n",
    "\n",
    "自定义回调通过以下几个功能点让用户可以实现个性化监控和干预：\n",
    "\n",
    "1. **监控训练进度**：可以通过回调跟踪模型在不同时间点上的行为和性能。\n",
    "2. **改变行为**：根据中间结果或特定条件有能力调整学习进程或改变RL模型的参数。\n",
    "3. **记录日志和保存最优模型**：例如，根据奖励自动存储性能较好的RL模型或在Tensorboard上记录关键变量。\n",
    "\n",
    "### 示例代码详解\n",
    "\n",
    "以下提供的示例代码定义了一个名为`CustomCallback`的类，它示范了如何从`BaseCallback`派生出一个自定义回调。本例借鉴给出的伪代码结构，一步步详述了几个关键的自定义点。\n",
    "\n",
    "```python\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        # 初始化时，这些成员变量是通过基类获得的，供自定义回调使用\n",
    "        # self.model：RL模型\n",
    "        # self.training_env：训练环境\n",
    "        # self.n_calls：回调被调用的次数\n",
    "        # self.num_timesteps：环境（env.step()）被调用的总次数\n",
    "        # self.locals：当地作用域变量\n",
    "        # self.globals：全局作用域变量\n",
    "        # self.logger：用于报告终端日志的logger对象\n",
    "        # self.parent：有时，子事件可能需要有对父对象的引用\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        # 训练之前第一次rollout开始前调用\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self):\n",
    "        # 在采集新样本之前调用\n",
    "        pass\n",
    "\n",
    "    def _on_step(self):\n",
    "        # 在env.step()后调用，可根据需要返回False提前中止训练\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        # 策略更新之前调用\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        # 在learn()方法退出前调用\n",
    "        pass\n",
    "```\n",
    "\n",
    "### 应用扩展\n",
    "\n",
    "该自定义回调类骨架为我们搭好了在关键时间节点自定义逻辑的平台。例如，你可以在`_on_step`方法中，依据实时的累积奖励指定一个自定义的提前终止训练的条件，或在`_on_training_end`方法中自动保存一份关键的分析报告。\n",
    "\n",
    "### 注意点\n",
    "\n",
    "- `self.num_timesteps`对于多环境训练进程有细微的考量：计数方式为环境数乘以`env.step()`调用次数。\n",
    "- 对于非策略算法（如SAC, DDPG, TD3或DQN），回合（rollout）这个词是指两次更新之间环境所采取的步骤。\n",
    "\n",
    "通过该示例代码，希望你能对`stable_baselines3`中如何实现自定义回调有了基本的了解。这项高级功能给予了研究人员和工程师极大的灵活性，让他们能够微调、监控和干预学习流程，实现模型调参的自动化和智能化。在实践这个例程序时，你可以考虑到训练的实时监控和干预需求，并尝试设计和实现符合自己项目需求的自定义回调类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ade8c",
   "metadata": {},
   "source": [
    "为向你展示如何完整而实践性的应用自定义回调，我们来设计一个名为`TrainingMonitorCallback`的自定义回调。这个自定义回调旨在实现以下几个目的：\n",
    "\n",
    "1. **跟踪奖励**：记录整个训练期间的累积平均奖励。\n",
    "2. **终止条件检查**：在累积平均奖励超过一个特定的界限时自动结束训练。\n",
    "3. **存档日志**：在关键时间节点自动存档（checkpoint）关键的训练日志。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "首先，确保你的环境中已正确安装`stable_baselines3`和`gym`环境。然后，你可以将以下代码粘贴到Jupyter Notebook中并执行。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "class TrainingMonitorCallback(Base: .Callback):\n",
    "    \"\"\"\n",
    "    自定义的RL训练监控器。\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=1, check_freq=100, success_threshold=195):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.success_threshold = success_threshold\n",
    "        # 用于跟踪所有周期累积的平均奖励\n",
    "        self.best_mean_reward = -np.inf\n",
    "        # 在检查点出触发时，累积奖励的列表\n",
    "        self.reward_history = []\n",
    "\n",
    "    def _on_step(self):\n",
    "        # 每一个self.check_freq步骤检查一次平均奖励\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # 获取环境的奖励和平均奖励\n",
    "            rewards = self.model.ep_info_buffer\n",
    "            mean_reward = np.mean([info['r'] for info in rewards]) if rewards else 0\n",
    "            self.reward_history.append(mean_reward)\n",
    "            \n",
    "            # 打印平均奖励\n",
    "            if self.verbose > 0:\n",
    "                print(f\"步骤数: {self.n_calls}, 平均奖励: {mean_reward:.2f}\")\n",
    "            \n",
    "            # 如果获得的平均奖励超过了我们设定的门槛\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                if mean_reward > self.success_threshold:\n",
    "                    print(\"成功的平均奖励门槛达成，早期停止...\")\n",
    "                    return False  # 返回False可以让学习流程立即中止\n",
    "        return True\n",
    "\n",
    "# 初始化环境\n",
    "env = make_vec_env('CartPole-v1', n_envs=8)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 创建自定义回调实例\n",
    "monitor_callback = TrainingMonitorCallback(verbose=1, check_freq=100, success_threshold=195)\n",
    "\n",
    "# 开始训练模型，加入监控器\n",
    "model.learn(total_timesteps=10000, callback=monitor_callback)\n",
    "```\n",
    "\n",
    "### 代码详解\n",
    "\n",
    "1. **TrainingMonitorCallback 类**：\n",
    "    - 继承自`BaseCallback`，旨在通过几个关键的复写函数实现累积平均奖励的监控和提前训练中止的机制。\n",
    "    - 在构造函数中，接受几个关键参数来界定检查频率（`check_freq`）和成功界限（`success_threshold`）。\n",
    "    - 通过`_on_step`复写方法，利用`self.n_calls`（表示此方法累积被调用的次数）在关键频率节点抽取环境信息，从而追踪平均奖励。\n",
    "\n",
    "2. **监控平均奖励和训练中止**：\n",
    "    - 代码集成在给定的检查频率时点对平均奖励进行跟踪，并在满足某个既定的高性能门槛时\n",
    "\n",
    "自动提前中止训练。\n",
    "    - 这对于节省资源并能够自动保留较好的性能快照是非常有效的。\n",
    "\n",
    "3. **实例化和训练**：\n",
    "    - 创建模型实例后，我们实例化`TrainingMonitorCallback`并将它传递给`model.learn`方法。\n",
    "    - 这样，我们的自定义回调就会嵌入到训练过程中，按需起作用。\n",
    "\n",
    "### 应用\n",
    "\n",
    "通过本示例，你可以基于不同环境和场景开发出复杂度不一的自定义监控器，这有助于深入理解模型的学习动态并实现基于进展的即时干预。希望这为你在实现自定义监控和干预逻辑上提供有益的启示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609fc70",
   "metadata": {},
   "source": [
    "`BaseCallback`在`stable_baselines3`中扮演着非常基础和重要的角色，它是所有回调(callbacks)的基类。通过继承`BaseCallback`，你可以创建自定义的回调来监控训练过程、保存模型、修改学习率等。这个机制让你能够在训练过程的关键时刻插入自定义逻辑，从而实现更加灵活和个性化的训练控制。\n",
    "\n",
    "### 主要方法和参数\n",
    "\n",
    "- **`__init__(self, verbose=0)`**：\n",
    "  - `verbose`：决定了回调中信息输出的详细程度。`0`表示没有输出，`1`用于普通信息消息，`2`用于调试信息。\n",
    "  \n",
    "- **`init_callback(self, model)`**：\n",
    "  - 在训练开始前初始化回调。这个方法允许回调保存对RL模型和训练环境的引用，以便后续使用。这在需要根据模型的当前状态或环境信息来决定动作时非常有用。\n",
    "\n",
    "- **`on_step(self)`**：\n",
    "  - 每次调用`env.step()`之后，模型都会调用这个方法。这是实现自定义逻辑的主要地方。如果这个方法返回`False`，训练会提前终止。这可以用来实现基于特定条件的训练早停。\n",
    "\n",
    "- **`update_child_locals(self, locals_)`**：\n",
    "  - 在具有子回调的事件回调(`EventCallback`)中，用于更新子回调的局部变量引用。这个方法在需要在不同回调之间共享数据时非常有用。\n",
    "\n",
    "- **`update_locals(self, locals_)`**：\n",
    "  - 用于更新当前回调中局部变量的引用。这是另一种在回调中访问和修改训练过程局部变量的方式。\n",
    "\n",
    "### 实用示例\n",
    "\n",
    "虽然上述描述提供了`BaseCallback`类的概览，但实际使用时，你通常会通过继承`BaseCallback`来创建具体的自定义回调。下面是一个简单的示例，展示如何创建一个每个训练步骤打印一条信息的自定义回调：\n",
    "\n",
    "```python\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class CustomPrintCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "    \n",
    "    def on_step(self):\n",
    "        print(\"Another step completed!\")\n",
    "        # 返回 True 表示训练继续\n",
    "        return True\n",
    "```\n",
    "\n",
    "使用这个自定义回调的方法如下：\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 实例化自定义回调\n",
    "custom_callback = CustomPrintCallback(verbose=1)\n",
    "\n",
    "# 在训练中使用回调\n",
    "model.learn(total_timesteps=10000, callback=custom_callback)\n",
    "```\n",
    "\n",
    "通过这种方式，你可以在训练过程中插入几乎任何类型的自定义逻辑，从而使训练过程既灵活又具有可扩展性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510faf6",
   "metadata": {},
   "source": [
    "## 事件回调函数Event Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ce6d9",
   "metadata": {},
   "source": [
    "在`stable_baselines3`中，与Keras不同，存在第二种形式的基础回调（`BaseCallback`），名为`EventCallback`，专门设计用来触发事件。当某个事件被激发时，便调起一个子回调（child callback）。这为模型提供在满足一定条件时自动反应的灵活能力，高度个性化了训练流程和干预手段。\n",
    "\n",
    "### EventCallback 使用场景\n",
    "\n",
    "`EventCallback`的概念在动态、基于事件的回调机制中尤为关键。例如，在训练期间根据某个性能衡量自动应对，这种设定保障了在关键时机采取相应措施（如自动调参、保存性能最优的模型等），而无需中断或人工干预整个训练进程。\n",
    "\n",
    "### 示例阐述\n",
    "\n",
    "`EvalCallback`就是一个`EventCallback`的例证，当检验到一个全新的、性能更佳的RL模型版本时，便会触发它的子回调。这些子回调中有例如`StopTrainingOnRewardThreshold`，后者在RL模型获得的平均奖励超过既定的门槛值时中止训练。此设置极大便利了开发人员针对实时监测到的关键结果采取即时措施。\n",
    "\n",
    "### 实现机制\n",
    "\n",
    "以下伪代码介绍了如何用`EventCallback`触发基于事件的控制动作：\n",
    "\n",
    "```python\n",
    "from stable_baselines3.common.callbacks import EventCallback, BaseCallback\n",
    "\n",
    "class EventCallback(BaseCallback):\n",
    "    def __init__(self, callback: BaseCallback, verbose: int = 0):\n",
    "        super().__init__(verbose=verbose)\n",
    "        # 子回调实例\n",
    "        self.callback = callback\n",
    "        # 使子回调可以获取到上层父回调的上下文环境\n",
    "        self.callback.parent = self\n",
    "        ...\n",
    "\n",
    "    def _on_event(self) -> bool:\n",
    "        # 当满超有定事件时被调用\n",
    "        return self.callback()\n",
    "```\n",
    "\n",
    "该伪代码阐述了`EventCallback`的大体框架，可视为对某事件应答的“骨架”实现。在具体实施中，开发人员应基于此设计模式，根据具体情形高度定制满足己需的事件触发逻辑与子回调逻辑。\n",
    "\n",
    "### 应用推荐\n",
    "\n",
    "开发时，推荐先参阅`EvalCallback`和`StopTrainingOnReward Threshold`等现有实例的源码，可借鉴其事件检验与回调机理的实现方式。通过理解现有事件触发逻辑和事件反应措施的实现方式，你可以深入领会事件触发型回调的灵活用途与潜能。\n",
    "\n",
    "高阶用法下的`EventCallback`机制无疑加深了对强化学习任务可调性与自适应能力的掌控，值得在实操项目中予以深化应用与实践。通过精细打造针对关键事件的自定义触发与响应，既能显著提升模型性能，又提高了整个训练流程的智能化水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df59ec",
   "metadata": {},
   "source": [
    "本示例旨在阐述如何使用`EventCallback`结构在强化学习（RL）训练中根据关键事件自动保存表现最佳的RL模型。作为实践，我们将会实现一个名为`EvalCallback`的事件触发型回调，及其子回调`StopTrainingOnRewardThreshold`，在训练RL模型的平均奖励超出既定门槛时终止训练。\n",
    "\n",
    "**目标**：\n",
    "\n",
    "1. **自动化模型监控**：在每轮迭代期满自动对RL模型性能进行验证。\n",
    "2. **按需早期中止**：基于验证结果，若RL模型性能满足或超过既定目标，自动终止模型进一步迭代。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "为实现代码示例，你需要确保环境中已正确安装`stable_baselines3`和`gym`环境。此例依旧聚焦在使用PPO算法和CartPole环境，我们来实现这两个自定义的回调。\n",
    "\n",
    "首先，假定你需要定期评估所训练的RL模型，并仅在评估结果表现良好时中止训练。此任务一方面验证了所提出评估机制的普适性，另一方面也检验RL模型的健壮性和实时性。\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# 首先，初始化训练环境\n",
    "env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "\n",
    "# 接着，初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 设置早期中止回调。这是我们希望在评估期望中满足特定平均奖励界限的回调。\n",
    "reward_threshold = 190.0\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=rewardreshold, verbose=1)\n",
    "\n",
    "# 接着，设置评估期望以定期检验RL模型。\n",
    "eval_env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1, best_model_save_path='./logs/', eval_freq=1000, deterministic=True)\n",
    "\n",
    "# 开始训练PPO模型，并在适当时中止训练\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "```\n",
    "\n",
    "### 代码详解\n",
    "\n",
    "1. **环境初始化**：\n",
    "    - 使用`make_vec_env`快速构建并包装好适用于并发学习的环境。\n",
    "2. **PPO模型实例化**：\n",
    "    - 利用PPO策略和MLP策略网络来实例化你的PPO强化学习模型。\n",
    "3. **中止条件与早期中止逻辑**：\n",
    "    - 设置`StopTrainingOnRewardThreshold`为早期中止的触发条件，定下了奖励分界点`reward_threshold`。\n",
    "    - 该中止条件作为`EvalCallback`实例中的参数之一，`EvalCallback`的功能即在规定迭代评伽频率时会考察RL模型性能是否超越了既定门槛。\n",
    "4. **评估周期设置与监控**：\n",
    "    - `EvalCallback`通过在一独立于主学习循环的验证环境上运行，周期地测量RL模型在给定环境设定下的平均表现。\n",
    "    - 当性能评估满足提早中止条的时候，自动引发学习过程的中止，并触发了性能良好的模型快照存档。\n",
    "\n",
    "### 应用\n",
    "\n",
    "通过本实践示例，旨在说明了在复杂环境和不断变化的目标情境中，智能化的模型监控与按需干预，乃至个性化地回调机制设置在深度强化学习任务自适应调参与动态性决策实践的必要之处与关键优势。希望这有助于你深入了解并灵活应用`stable_baselines3`的动态特性与能力，促进了强化学习研究与实践在多样化与个性化层面的深度开拓与丰富多彩的实现。如有任何问题或需要帮助，请随时提问。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ec370",
   "metadata": {},
   "source": [
    "`EventCallback` 在 `stable_baselines3` 中是设计用来在特定事件发生时触发回调的基类。这为在训练过程中的关键时刻执行自定义操作提供了一个灵活的机制。通过继承 `EventCallback` 并实现其方法，你可以创建能够响应各种训练事件的自定义回调。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`callback`** (`BaseCallback` | `None`)：当事件被触发时，将调用的回调。这允许你指定当特定事件发生时应该执行哪个回调逻辑。\n",
    "- **`verbose`** (`int`)：控制输出信息的详细程度，`0` 表示没有输出，`1` 用于普通信息消息，`2` 用于调试信息。\n",
    "\n",
    "### 主要方法\n",
    "\n",
    "- **`init_callback(self, model)`**：通过保存对RL模型和训练环境的引用来初始化回调，这为回调的执行提供了便利。\n",
    "- **`update_child_locals(self, locals_)`**：更新子回调中局部变量的引用。这个方法允许你在子回调中访问和修改训练过程中的局部变量。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "`EventCallback` 是设计来响应如模型训练开始、结束，或是每个训练步骤后等特定事件的。你可以通过继承 `EventCallback` 来创建能够在这些事件发生时执行特定操作的自定义回调。例如，你可能希望在训练的每个阶段结束时评估模型的性能，或者在训练过程中定期保存模型。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "下面是一个简化的示例，展示如何创建一个在每次调用 `env.step()` 后打印消息的自定义 `EventCallback`：\n",
    "\n",
    "```python\n",
    "from stable_baselines3.common.callbacks import EventCallback, BaseCallback\n",
    "\n",
    "class PrintMessageCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "\n",
    "    def on_step(self) -> bool:\n",
    "        print(\"Step completed!\")\n",
    "        return True\n",
    "\n",
    "class MyEventCallback(EventCallback):\n",
    "    def __init__(self, callback=None, verbose=0):\n",
    "        super().__init__(callback, verbose)\n",
    "\n",
    "    # 你可以重载其他方法来适应特定的事件，如 on_training_start, on_training_end 等\n",
    "\n",
    "# 使用示例\n",
    "from stable_baselines3 import PPO\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 创建并使用自定义 EventCallback\n",
    "print_callback = PrintMessageCallback()\n",
    "event_callback = MyEventCallback(callback=print_callback)\n",
    "model.learn(total_timesteps=10000, callback=event_callback)\n",
    "```\n",
    "\n",
    "在这个例子中，`PrintMessageCallback` 是一个简单的回调，它在每个步骤后打印一条消息。`MyEventCallback` 继承自 `EventCallback`，并在其内部使用了 `PrintMessageCallback` 作为触发的回调。通过这种方式，你可以创建复杂的事件驱动的回调逻辑，以响应训练过程中的各种情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73342f2e",
   "metadata": {},
   "source": [
    "# 回调集合Callback Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa330e73",
   "metadata": {},
   "source": [
    "### 回调集合\n",
    "\n",
    "Stable Baselines 为您提供了一组常见的回调：\n",
    "\n",
    "定期保存模型（CheckpointCallback）\n",
    "\n",
    "定期评估模型并保存最好的模型（EvalCallback）\n",
    "\n",
    "链接回调（CallbackList）\n",
    "\n",
    "触发事件回调（Event Callback、EveryNTimesteps）\n",
    "\n",
    "根据奖励阈值提前停止训练 (StopTrainingOnRewardThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0901b",
   "metadata": {},
   "source": [
    "## CheckpointCallback\n",
    "\n",
    "`CheckpointCallback`特别适合在长时间的、多迭代的学习进程里面，按设定的频率透明、无干扰的把关键的中间模型快照持久化存档。\n",
    "\n",
    "### 核心特性\n",
    "\n",
    "- **按迭代频率自动存档**：基于环境中调用的步骤数（`env.step()`），自动在每设定频率（`save_freq`）时保存一个快照。\n",
    "- **存档位置和命名前缀**：你需要提供存档文件夹的路径（`save_path`）和（选择性地）存档文件的命名前缀（`name_prefix`，默认为`\"rl_model\"`）。\n",
    "- **复现缓存存档**：如果你的RL模型拥有重放缓存（replay buffer），通过`save_replay_buffer=True`设置即可选择性地连同保存重放缓存，这对中止和恢复后的训练连续性可能很重要。\n",
    "- **归一化统计存档**：如果环境使用了`VecNormalize`封装，那么通过`save_vecnormalize=True`设置可以一并存档归一化所需的跟环境相关统计数据。\n",
    "\n",
    "### 警示提醒\n",
    "\n",
    "- **关于使用并发环境**：特别值得注意的是，当使用并发环境（例如通过`n_envs`参数创建的环境）时，每次调用`env.step()`相当于进行了`n_envs`步操作。因此，实际保存频率会通过你为`save_freq`指定的次数相对并发环境总数加以调节，以保证无论并发环境的规模如何调整，实质性的保存周期都保持大致不变。\n",
    "\n",
    "### 示例代码详解\n",
    "\n",
    "以下是如何将`CheckpointCallback`整合到SAC算法在Pendulum环境上的训练中的样例。这一例证阐明了自动存档快照文件的设置流程和实现机制。\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# 每迭代1000步骤后自动存档\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1000,  # 保存快照的触发频率\n",
    "    save_path=\"./logs/\",  # 存档路径\n",
    "    name_prefix=\"rl_model\",  # 存档命名的前缀\n",
    "    save_replay_buffer=True,  # 选选地同时存档replay buffer\n",
    "    save_vecnormalize=True  # 选择性同时存档归一化所需的跟环境相关统计数据\n",
    ")\n",
    "\n",
    "# 初始化SAC模型和环境\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\")\n",
    "# 在训练中施用之前配置的checkpoint存档逻辑\n",
    "model.learn(2000, callback=checkpoint_callback)\n",
    "```\n",
    "\n",
    "通过使用`CheckpointCallback`，无需等待整个训练完成，你就可以轻松在关键时间节点拿到、查看或分析已存档快照的行为和表现，乃至进行调参实验，从而调节以观察即时的模型性能影响，有效监控和干预整个学习进程。此乃实践中灵活利用存档快照实现半自动或全自动训练调控与干预分析的关键方式之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc5a53",
   "metadata": {},
   "source": [
    "本例中，我们将会看到如何将`CheckpointCallback`应用在A2C算法上，并用于解`LunarLander-v2`环境的任务。目标是针对长时间运行的RL任务中自动地保存关键的训练快照，提供终止和重启后无缝对接继续学习的便利性。\n",
    "\n",
    "### 场景描述\n",
    "\n",
    "`LunarLander-v2`环境的任务是训练智能体去稳妥地在两旗间的地面上降落。我们关注以下三个方面：\n",
    "\n",
    "1. **按迭代频率自动存档**：训练进程每迭代10000次自动保存快照。\n",
    "2. **存档目的地**：定制存档文件夹来组织存档快照。\n",
    "3. **存档频率的合理调整**：验证存档频率自适应并发环境设定的考量。\n",
    "\n",
    "### 具体实践\n",
    "\n",
    "以下是采用A2C算法，并通过应用`CheckpointCallback`以自动保存关键快照的示例代码：\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# 1. 初始化环境\n",
    "env_id = \"LunarLander-v2\"\n",
    "env = make_vec_env(env_id, n_envs=4)  # 利用并发环境以加速学习\n",
    "\n",
    "# 2. 设置Checkpoint保存逻辑\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,  # 保存频率，针对总迭代步骤，而不局限于每个并发环境\n",
    "    save_path=\"./logs/\",  # 存档快照路径\n",
    "    name_prefix=\"a2c_lunarlander\"  # 存档快照的文件名前缀\n",
    ")\n",
    "\n",
    "# 3. 初始化A2C模型\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 4. 使用Checkpoint存档逻辑开始训练\n",
    "total_timesteps = 200000  # 定义总训练迭代步数\n",
    "model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)\n",
    "```\n",
    "\n",
    "### 代码详解\n",
    "\n",
    "- **环境初始化**：这个代码片段首先使用`make_vec_env`快速创建了一个并发环境实例。选用`LunarLander-v2`环境并行置于4个并发环境中，以加速训练过程。\n",
    "- **存档快照设置**：随后，通过实例化`CheckpointCallback`并指定了存档路径（`save_path`）、存档快照名称前缀（`name_prefix`），以便轻松组织和定位所存档模型。设置存档快照频率为10000步骤，该频率是针对跨越所有并发环境的综合步骤而定。\n",
    "- **A2C模型初始化**：借由适用多层感知机（MLP）类型作策略网络的`MlpPolicy`对A2C算法进行实例化，并选取前述并发环境实例为训练环境。\n",
    "- **训练与存档快照**：指定了大量训练步骤，并定义快照按既定规则全自动创建存档。\n",
    "\n",
    "### 应用与好处\n",
    "\n",
    "通过本实践例，尤其是针对复杂任务和场景的长周期学习，自动化地保存训练快照可以：\n",
    "\n",
    "- 在实验研究中，让实验者能轻松还原关键时间点上的模型表现。\n",
    "- 有利于实验过程的灵活管理，譬如突发性地中断实验而不致误了珍贵的已获进展。\n",
    "- 简化在并发环境训练结构中就不同并发数量情境自动调整训练弹性需见识的要领，故确保跨任务转移和存档灵活对接的兼容性。\n",
    "\n",
    "希望本实践回答有助于你在动手操作机器学习任务时，深化理解并灵活地操控自动化存档快照技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3972ec",
   "metadata": {},
   "source": [
    "`CheckpointCallback`是`stable_baselines3`库中用于在训练过程中定期保存模型的一个回调。这个回调使得在长时间训练或者在关键训练阶段后能够保存模型状态，为后续的模型恢复、评估或者进一步训练提供便利。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`save_freq`** (int)：定义了多少次`env.step()`调用后保存一次模型。这是控制保存频率的关键参数。\n",
    "- **`save_path`** (str)：指定了模型保存的文件夹路径。\n",
    "- **`name_prefix`** (str)：保存模型时文件名的公共前缀，默认为`'rl_model'`。\n",
    "- **`save_replay_buffer`** (bool)：是否保存模型的重放缓冲区（replay buffer）。这对于基于经验回放的算法（如DQN）来说特别重要。\n",
    "- **`save_vecnormalize`** (bool)：是否保存`VecNormalize`的统计数据。当使用`VecNormalize`环境包装器进行训练时，这可以帮助保持环境的正常化参数。\n",
    "- **`verbose`** (int)：控制输出信息的详细程度，`0`表示没有输出，`2`表示在保存模型检查点时输出提示信息。\n",
    "\n",
    "### 使用注意事项\n",
    "\n",
    "- **多环境训练**：当使用多个环境进行训练时（即`n_envs`>1），每次调用`env.step()`实际上会对应`n_envs`步。因此，为了确保在多环境设置下保持期望的保存频率，可能需要调整`save_freq`参数的值。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "假设我们正在使用PPO算法训练`CartPole-v1`环境，并希望每1000个训练步骤保存一次模型：\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import gym\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 初始化模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 初始化CheckpointCallback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./logs/\", name_prefix=\"ppo_cartpole\")\n",
    "\n",
    "# 开始训练，并使用CheckpointCallback\n",
    "model.learn(total_timesteps=20000, callback=checkpoint_callback)\n",
    "```\n",
    "\n",
    "在这个示例中，`CheckpointCallback`被配置为每1000次`env.step()`调用保存一次模型到`\"./logs/\"`目录下，文件名以`\"ppo_cartpole\"`为前缀。如果在训练过程中使用了多个环境，还需要根据环境数量调整`save_freq`的值，以保持实际的保存频率。\n",
    "\n",
    "通过使用`CheckpointCallback`，可以方便地管理模型的保存和恢复，特别是在进行长时间训练或需要在特定时间点评估模型性能的场景中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc06c4",
   "metadata": {},
   "source": [
    "## EvalCallback\n",
    "\n",
    "`EvalCallback`是`stable_baselines3`中一个特别的功能，允许你周期性地用一个独立的测试环境来衡量智能体的表现。这对于监测和理解智能体随时间上的进展非常关键。例如，你可能想了解智能体的表现是否逐步接近理想状态，亦或是想追踪哪个版本的智能体算法最为高效。`EvalCallback`恰好能帮你实现这样的需求。\n",
    "\n",
    "### 核心功能\n",
    "\n",
    "- **独立测试环境表现衡量**：使用跟训练环境独立的测试环境来检验智能体的策略表现。\n",
    "- **最优模型自动保存**：如果配置好了保存路径`best_model_save_path`，那么能自动为你保存性能表现最佳的智能体模型。\n",
    "- **性能衡量结果存档**：当指定了`log_path`文件夹路径时，所有评估结果会自动地保存到一个名为`evaluations.npz`的NumPy归档文件中。\n",
    "\n",
    "### 代码实践\n",
    "\n",
    "本例阐述了如何把`EvalCallback`整合到`sac`算法用于`Pendulum-v1`环境的例中，代码解读如下：\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# 为了性能衡量，建立一个跟训练环境相独立的测试环境\n",
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# 配置EvalCallback\n",
    "eval_callback = EvalCallback(eval_env, \n",
    "                             best_model_save_path=\"./logs/\",  # 指定表现最优智能体存档路径\n",
    "                             log_path=\"./logs/\",  # 测试性能结果存档路径\n",
    "                             eval_freq=500,  # 测试性能的触发频率\n",
    "                             deterministic=True,  # 测试智能体策略动作是否确定性选取\n",
    "                             render=False)  # 测试过程是否实时渲染环境\n",
    "\n",
    "# 初始化SAC模型\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\")\n",
    "\n",
    "# 带评估触发地进行模型学习\n",
    "model.learn(5000, callback=eval_callback)\n",
    "```\n",
    "\n",
    "### 代码详解\n",
    "\n",
    "- **测试环境**：首先创建了与训练环境独立的`Pendulum-v1`测试环境实例，保证了智能体的性能衡量在没有参与过学习的环境下进行，以提升测评的普适性和说服力。\n",
    "- **周期性性能衡量**：通过`EvalCallback`设置性能周期性检测，每隔`500`个时间步骤自动触发测评，并结合设置保证结果的持久化存储与潜在智能体行为的确定性（`deterministic=True`）考量。\n",
    "- **智能体模型与回调绑定**：采用SAC算法初始化`Pendulum-v1`的学习智能体，并以`eval_callback`为控制回调来提供准确而实时的性能跟踪反馈。\n",
    "- **性能最优智能体快照存档**：若有智能体的表现超越之前所有周期性能评估中最好成绩，即被自动以保存至\"./logs/\"路径下，便于之后加载分析或复现研究。\n",
    "\n",
    "### 附注和强调\n",
    "\n",
    "- **环境匹配**：测试环境应确保与训练环境在处理流程、封装等方面具备高度一致性，差异化因素可能会干扰比较的公允性。\n",
    "- **存档与跟踪**：自动化跟踪与存档有助于算法调参与效能比对，在实际研究与部署智能体时非常实用。\n",
    "\n",
    "通过此实跑实践，旨在强调深度学习工程实操中测试环境独立设定的模型测评跟踪与实时最优快照归档特性，有望帮你深化强化学习应用部署的技术阵地。针对所含说明与关键环节若有不清晰或疑问，请遂时咨询。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f232dc",
   "metadata": {},
   "source": [
    "我们来看一个使用`EvalCallback`在DQN算法上应用的示例，针对`CartPole-v1`环境。这个示例的目的是周期性评估智能体在`CartPole-v1`环境下的性能，并在测得表现较好时自动保存该智能体作为最优智能体。此示例还提供实时反馈和监控，有利于了解智能体的即时学习表现和随时间的改善情况。\n",
    "\n",
    "### 代码结构\n",
    "\n",
    "代码大致会遵循以下流程：\n",
    "\n",
    "1. **环境设置**：分别为训练和测试创建`CartPole-v1`环境实例。\n",
    "2. **DQN智能体初始化**：针对任务初始化DQN算法和模型策略。\n",
    "3. **定期测试与自动存档**：加入`EvalCallback`，在满足既定周期性能测试之后，自动考核是否存档为最佳智能体。\n",
    "4. **实施学习与反馈**：智能体开始学习流程，期间会触发周期性表现测试，并基于测评自动考量保存性能较优的智能体快照。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# 1. 环境设置\n",
    "train_env = gym.make(\"CartPole-v1\")\n",
    "eval_env = gym.make(\"CartPole-v1\")  # 测试环境\n",
    "\n",
    "# 2. 智能体初始化\n",
    "model = DQN(\"MlpPolicy\", train_env, verbose=1)\n",
    "\n",
    "# 3. 添加周期性能测试和自动存档智能体的最佳快照\n",
    "eval_callback = EvalCallback(eval_env, \n",
    "                             best_model_save_path=\"./logs/best_model\", \n",
    "                             log_path=\"./logs/results\", \n",
    "                             eval_freq=1000,  # 测试智能体表现的迭代周期\n",
    "                             deterministic=True,  # 测评时采用确定性策略\n",
    "                             render=False)  # 测评期间不渲染环境\n",
    "\n",
    "# 4. 开始学习\n",
    "total_timesteps = 10000\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "```\n",
    "\n",
    "### 代码逐步详解\n",
    "\n",
    "- **环境设置**：创建了两个环境实例，`train_env`用于训练智能体，`eval_env`在固定的迭代周期上独立测试已学习智能体的性能。\n",
    "- **DQN智能体初始化**：本示例采用`stable_baselines3`的`DQN`算法，选择\"MLPPolicy\"策略网络，针对`CartPole-v1`任务迅速开始训练。\n",
    "- **周期性测试与存档**：`EvalCallback`提供按既定设置自动于独立测试环境考量表现并于关键节点存档关键智能体数据。期间如果性能创最佳，则驱动存档最优快照至`\"./logs/best_model\"`路径。\n",
    "- **模型训练**：智能体遵循指令按既设的10000个时间步骤对照目标开始尝试与优化其解法策略。期间自触评估监听会动态跟进性能衡量与关键成就存档。\n",
    "\n",
    "### 应用与反馈\n",
    "\n",
    "本示例借帮你深化认识和实操怎样行之有效结合`EvalCallback`触发RL智能体策略行之有效的学习效能独立考量与关键性绩效点智能体策略保存。有鉴于周期性客观检验有利于全面了智能体跨时间的习得与稳健性提升，合计包括参数调适及算法比对选型不无裨益。\n",
    "\n",
    "通过实操，希望这有利于启迪并帮力深层实现复杂RL任务场景中主观能动采模智能体高能效习得与科学优化调参策略的直观介入与考量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8b2a2",
   "metadata": {},
   "source": [
    "`EvalCallback`是`stable_baselines3`库中用于评估智能体性能的一个回调。它允许定期在一个独立的评估环境中测试智能体，并基于评估结果执行特定的操作，如保存表现最佳的模型。这个回调对于监控和验证训练过程中智能体的学习进展非常有用。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`eval_env`**：用于评估的环境，可以是`Env`或`VecEnv`。这应该与训练环境相分离，以确保评估的公正性。\n",
    "- **`callback_on_new_best`**：当找到一个新的最佳模型时（基于`mean_reward`），触发的回调。这可以用来执行如保存模型等特定操作。\n",
    "- **`callback_after_eval`**：每次评估完成后触发的回调。\n",
    "- **`n_eval_episodes`**：评估智能体时运行的集数。\n",
    "- **`eval_freq`**：调用此回调的频率，以训练步数（`env.step()`调用次数）计。\n",
    "- **`log_path`**：保存评估结果（`evaluations.npz`）的路径。每次评估都会更新这个文件。\n",
    "- **`best_model_save_path`**：保存根据评估环境性能最佳的模型的路径。\n",
    "- **`deterministic`**：评估时是否使用确定性策略来选择动作。\n",
    "- **`render`**：是否在评估期间渲染环境。\n",
    "- **`verbose`**：信息输出的详细程度。`0`表示没有输出，`1`表示输出评估结果等信息。\n",
    "- **`warn`**：如果评估环境没有使用`Monitor`包装器，则是否发出警告。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "`EvalCallback`特别适合于那些需要在训练过程中定期监测和评估智能体性能的情况。通过定期评估，你可以了解智能体是否在学习并改进其策略，以及它是否达到了特定的性能标准。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "假设我们正在使用PPO算法训练`CartPole-v1`环境的智能体，并希望每10000步评估一次其性能：\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# 创建训练环境\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 创建评估环境\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# 初始化EvalCallback\n",
    "eval_callback = EvalCallback(eval_env, eval_freq=10000, n_eval_episodes=5,\n",
    "                             best_model_save_path='./best_model/', log_path='./logs/',\n",
    "                             deterministic=True, render=False, verbose=1)\n",
    "\n",
    "# 初始化模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 开始训练\n",
    "model.learn(total_timesteps=50000, callback=eval_callback)\n",
    "```\n",
    "\n",
    "这个例子中，`EvalCallback`被配置为每10000个训练步骤在评估环境中测试智能体5次，并将评估结果保存到指定的路径。如果在评估期间发现了一个新的最佳模型，则该模型将被保存。\n",
    "\n",
    "通过这种方式，`EvalCallback`不仅提供了智能体性能的定期检查机制，而且还自动管理了模型的保存工作，使得训练过程更加高效和系统化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216533c9",
   "metadata": {},
   "source": [
    "## 进度条回调ProgressBarCallback\n",
    "\n",
    "`ProgressBarCallback`是`stable_baselines3`（简称SB3）库中的一个实用功能，旨在提供实时的训练进度反馈。当你启动训练流程时，`ProgressBarCallback`会显示一个进度条，其中包含了当前进度、已经过去的时间以及预估的剩余时间，这能够帮助你直观地把握训练进度和效率。\n",
    "\n",
    "### 核心特性\n",
    "\n",
    "- **进度展示**：实时展示训练进度，包括已完成的部分和总体任务量。\n",
    "- **时间反馈**：显示自训练开始以来已消耗的时间和预计还需的时间。\n",
    "\n",
    "### 安装要求\n",
    "\n",
    "使用`ProgressBarCallback`需要确保`tqdm`和`rich`这两个包已经被安装。如果你是通过`pip install stable-baselines3[extra]`安装的SB3，那么这两个依赖包会自动被安装。\n",
    "\n",
    "### 代码示例\n",
    "\n",
    "以下代码展示了如何在使用PPO算法训练`Pendulum-v1`环境时启用进度条。这里演示的是通过`model.learn()`方法中的`progress_bar`参数直接启用进度条的简便方式。\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 初始化PPO模型，选择MLP策略网络和Pendulum-v1环境\n",
    "model = PPO(\"MlpPolicy\", \"Pendulum-v1\")\n",
    "\n",
    "# 使用进度条回调启动训练流程，直接通过progress_bar参数启用\n",
    "model.learn(100_000, progress_bar=True)\n",
    "```\n",
    "\n",
    "在这个示例中，训练进度条将在你执行`model.learn(100_000, progress_bar=True)`这行代码时自动显示，无需显式创建`ProgressBarCallback`实例。这种方式既简洁又直观，能够让你在训练大规模模型时，轻松监控训练进度和时间消耗。\n",
    "\n",
    "### 应用场景\n",
    "\n",
    "`ProgressBarCallback`特别适用于那些需要长时间运行的训练任务，让用户可以实时监控训练状态。无论是在本地机器还是云端服务器上运行训练任务时，进度条都能提供关键的实时反馈，帮助评估训练进度和效率，从而作出相应的调整。\n",
    "\n",
    "通过引入这样的直观工具，`stable_baselines3`进一步降低了强化学习实践的门槛，使得训练过程更加友好和可控。如果你计划进行大规模的强化学习训练，考虑使用`ProgressBarCallback`或通过`progress_bar`参数启用进度条将会是一个明智的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf229a",
   "metadata": {},
   "source": [
    "咱们以A2C算法和`Acrobot-v1`环境为基础，编写一个展示进度条回调用法的更全面实操示范。A2C算法是一种并发算法（Advantage Actor-Critic），适合处理连续和离散动作空间的问题，而`Acrobot-v1`环境则是解双摆问题的一个典型环境，可以很好的体现算法解复杂决策问题的性能。\n",
    "\n",
    "### 代码撰写要旨\n",
    "\n",
    "1. **环境初始化**：选定并实例化目标环境为`Acrobot-v1`。\n",
    "2. **A2C算法初始化**：设置好针对所实例化环境的A2C算法配置。\n",
    "3. **启动训练进程**：在`.learn()`方法调用中通过`progress_bar`参数来触发实时训练进度的图形化反馈。\n",
    "4. **直观表征**：进度反馈中会同步显示关于当前迭代比进度、预估剩余训练时间等的信息。\n",
    "\n",
    "### 具体实践示范\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import A2C\n",
    "import gym\n",
    "\n",
    "# 1. 环境初始化\n",
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "# 2. A2C算法初始化\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# 3. 训练智能体模型并显示进度条\n",
    "total_timesteps = 25000  # 设置训练迭代的步长\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "```\n",
    "\n",
    "### 代码详尽介绍\n",
    "\n",
    "- **环境初始化**：本实践首选`'Acrobot-v1'`环境，这是一个要求智能体操控一个两关节、两臂的机械臂在最小步骤内达到给定高度的任务环境，被广泛用于考量算法的解策略质量和效能。\n",
    "- **A2C算法初始化**：这一环节中，设以`'MlpPolicy'`（即多层感知机策略网络）针对指定环境实例进行训练。A2C属并发Actor-Critic算法之一，用以强化对并发环境决策最佳出路的指令感悟与决策效率。\n",
    "- **进度条功能调起**：再通过`.learn(total_timesteps, progress_bar=True)`调起并自带展现了实时迭代训练过程展示栏。设置本次尝试教智能体的迭代步骤长度为`25000`步。\n",
    "\n",
    "### 重要提示\n",
    "\n",
    "- **图形化环境依赖**：引入并正确置入工作环境的`tqdm`和`rich`第三方库对实现迭代进度图形化渲染至关重要。确保你的工作环境正确包含它们以便充分享受实时图形化反馈之便。\n",
    "- **环境对等待遇**：确保为了合法比对，训练环境与求测环境全等设或等效同构至关重要。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "这样的实操练习能在实时跟踪算法逐步习得决策敏锐度和因应各类环境扰动的韧性中起到了实时监着和引导干预作用。希望这有益于帮你全方位而实时了解智能体习得新本能与新本领的实时概况与趋变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0b60c",
   "metadata": {},
   "source": [
    "## 回调列表 CallbackList\n",
    "\n",
    "`CallbackList`是`stable_baselines3`中一个非常有用的类，它允许你将多个回调函数串联起来，这样它们就会按顺序被调用。这一功能特别适合于那些需要在训练过程中同时进行多项任务的情况，比如同时监控训练进度、定期保存模型快照以及评估模型性能等。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "- **组合多种回调功能**：当你想要在训练过程中使用多种回调来实现不同的功能时（例如，保存检查点、评估性能等）。\n",
    "- **保持代码整洁**：通过将多个回调组合为一个列表，可以让你的训练启动代码更加整洁和易于管理。\n",
    "\n",
    "### 代码示例详解\n",
    "\n",
    "在下面的示例中，我们将使用`SAC`算法在`Pendulum-v1`环境上训练一个模型，同时使用`CheckpointCallback`和`EvalCallback`来分别保存模型和评估模型性能。然后，我们使用`CallbackList`将这两个回调组合起来。\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "\n",
    "# 设置保存检查点的回调\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./logs/\")\n",
    "\n",
    "# 设置评估模型性能的回调\n",
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/results\", eval_freq=500)\n",
    "\n",
    "# 创建回调列表，组合上面定义的两个回调\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# 初始化SAC模型\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\")\n",
    "\n",
    "# 使用回调列表开始训练模型\n",
    "model.learn(5000, callback=callback)\n",
    "```\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- **CheckpointCallback**：每隔`1000`步训练，就自动保存一次模型到`\"./logs/\"`路径下。\n",
    "- **EvalCallback**：每隔`500`步训练，在独立的评估环境`eval_env`上评估模型性能，如果是新的最佳模型，则保存到`\"./logs/best_model\"`，同时将评估结果保存到`\"./logs/results\"`。\n",
    "- **CallbackList**：通过`CallbackList`将`checkpoint_callback`和`eval_callback`组合成一个回调列表，然后在`model.learn()`方法中使用。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "使用`CallbackList`可以有效地组织和管理训练过程中的多个监控和控制任务，使得训练过程既高效又易于监控。此外，这种方式还能够让代码结构更加清晰，便于维护和复用。\n",
    "\n",
    "通过以上示例和解析，希望你能够理解和掌握如何在`stable_baselines3`中使用`CallbackList`来组合和管理多个回调。这种方法不仅提高了训练过程的灵活性，也使得训练监控和干预更加方便和直观。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79581f1f",
   "metadata": {},
   "source": [
    "让我们通过一个实际的例子来进一步理解如何在`stable_baselines3`中使用`CallbackList`来组合多个回调。在这个例子中，我们将使用TD3（Twin Delayed DDPG）算法在`MountainCarContinuous-v0`环境上训练一个模型，并且同时使用`CheckpointCallback`、`EvalCallback`和`ProgressBarCallback`来进行模型保存、性能评估和进度展示。\n",
    "\n",
    "### 场景描述\n",
    "\n",
    "- **目标环境**：`MountainCarContinuous-v0`，一个连续动作空间的环境，目标是驱动一辆车爬上山顶。\n",
    "- **训练算法**：TD3，一种适用于连续动作空间的强化学习算法。\n",
    "- **回调功能**：\n",
    "  - `CheckpointCallback`：定期保存模型。\n",
    "  - `EvalCallback`：评估模型的性能，并在有新的最佳模型时保存。\n",
    "  - `ProgressBarCallback`：显示训练进度条。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback, ProgressBarCallback\n",
    "\n",
    "# 初始化环境\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "# 初始化回调\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\"./logs/\", name_prefix=\"td3_model\")\n",
    "eval_env = gym.make(\"MountainCarContinuous-v0\")\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/results\", eval_freq=1000,\n",
    "                             deterministic=True)\n",
    "progress_bar_callback = ProgressBarCallback()\n",
    "\n",
    "# 组合回调\n",
    "callback_list = CallbackList([checkpoint_callback, eval_callback, progress_bar_callback])\n",
    "\n",
    "# 初始化TD3模型\n",
    "model = TD3(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 开始训练\n",
    "model.learn(total_timesteps=20000, callback=callback_list)\n",
    "```\n",
    "\n",
    "### 代码解析\n",
    "\n",
    "- **环境初始化**：为`MountainCarContinuous-v0`环境创建两个实例，一个用于训练，另一个用于评估。\n",
    "- **回调初始化**：\n",
    "  - `CheckpointCallback`每5000个训练步骤保存一次模型。\n",
    "  - `EvalCallback`每1000个训练步骤在评估环境上测试模型性能，并保存最佳模型。\n",
    "  - `ProgressBarCallback`提供实时的训练进度条显示。\n",
    "- **组合回调**：通过`CallbackList`将上述三个回调组合成一个列表。\n",
    "- **模型训练**：使用TD3算法和MLP策略网络在指定环境上训练模型，训练过程中应用组合的回调列表。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "通过使用`CallbackList`组合多个回调，你可以同时监控训练进度、自动保存模型以及定期评估模型性能。这种方法使得训练过程更加透明和高效，同时也便于你在训练中实施必要的干预和调整。此外，这种方式还提高了代码的组织性和可读性，使得训练配置和管理更加灵活和简洁。\n",
    "\n",
    "以上示例和解释希望能帮助你更好地理解和使用`stable_baselines3`中的`CallbackList`功能，以及如何通过组合不同的回调来丰富和优化你的训练流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586a680",
   "metadata": {},
   "source": [
    "`CallbackList`是`stable_baselines3`库中用于组合多个回调的一个工具类，它允许你将多个`BaseCallback`实例化对象串联起来，这样在训练过程中它们会按顺序被调用。这使得在模型的学习过程中实施多种监控和控制策略变得简单而直接。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "当你希望在训练过程中同时进行多项任务，如保存检查点、记录训练日志、评估模型性能等，就可以使用`CallbackList`来组织你的回调，从而无需修改训练循环的主体代码。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`callbacks`**：`List[BaseCallback]`，一个包含多个`BaseCallback`子类实例的列表。这些回调将按照列表中的顺序被依次调用。\n",
    "\n",
    "### 方法\n",
    "\n",
    "- **`update_child_locals(locals_)`**：\n",
    "  - 用于更新子回调中局部变量的引用。这使得在多个回调之间共享和修改训练过程中的局部变量成为可能。\n",
    "  - **参数**：\n",
    "    - `locals_`：一个字典，包含了在训练循环中收集的局部变量。\n",
    "\n",
    "### 示例\n",
    "\n",
    "假设我们有两个自定义的回调，一个用于在每1000个步骤保存模型，另一个在每2000步评估模型性能，我们可以这样组合使用它们：\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "import gym\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 创建回调\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./logs/\")\n",
    "eval_callback = EvalCallback(eval_env, eval_freq=2000, log_path=\"./logs/eval/\")\n",
    "\n",
    "# 将回调组合成一个列表\n",
    "callback_list = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "# 初始化模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 使用组合的回调开始训练\n",
    "model.learn(total_timesteps=10000, callback=callback_list)\n",
    "```\n",
    "\n",
    "这个示例中，`CallbackList`使得我们能够同时使用检查点保存和模型评估的回调，而无需在模型的`learn`方法中分别调用它们。这种方式使得回调的管理更加集中和方便，尤其是当回调数量增加时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877834ea",
   "metadata": {},
   "source": [
    "## 奖励阈值停止训练 StopTrainingOnRewardThreshold\n",
    "\n",
    "`StopTrainingOnRewardThreshold`是`stable_baselines3`中一个非常实用的回调，它允许你在智能体达到一定的性能标准时自动停止训练。这个功能尤其适用于当你不确定需要训练多久或者想要在达到特定性能水平后即刻停止训练以节省资源时。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "- 当你想要在模型达到一定的奖励阈值后自动停止训练。\n",
    "- 结合`EvalCallback`使用，以便在评估期间检查是否达到了既定的奖励阈值。\n",
    "\n",
    "### 代码示例详解\n",
    "\n",
    "在此例中，我们使用`SAC`算法在`Pendulum-v1`环境上进行训练，并设置一个奖励阈值为`-200`。一旦模型在评估期间的平均奖励达到此阈值，训练将自动停止。\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "# 初始化评估环境\n",
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# 初始化停止训练的回调\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-200, verbose=1)\n",
    "\n",
    "# 初始化评估回调，将停止训练的回调传递给它\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
    "\n",
    "# 初始化模型\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n",
    "\n",
    "# 开始训练，设置一个非常大的时间步长数，但实际上会在达到奖励阈值后提前停止\n",
    "model.learn(int(1e10), callback=eval_callback)\n",
    "```\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- **评估环境**(`eval_env`): 用于评估智能体性能的环境实例，与训练环境相分离，确保评估的公正性。\n",
    "- **奖励阈值**(`reward_threshold`): 设置为`-200`，意味着一旦模型在评估期间达到这个平均奖励值，训练将自动停止。\n",
    "- **`EvalCallback`**: 用于定期评估模型性能，并在发现新的最佳模型时触发事件。\n",
    "- **`StopTrainingOnRewardThreshold`**: 当`EvalCallback`发现新的最佳模型且该模型的性能达到或超过设置的奖励阈值时，触发训练停止。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "这种自动化的训练停止机制不仅可以节省计算资源，避免过度训练，还能帮助你自动寻找到满足特定性能要求的模型。通过这样的方式，你可以更加高效地进行模型训练和实验，尤其是在探索新算法或调整模型参数时。\n",
    "\n",
    "`StopTrainingOnRewardThreshold`和`EvalCallback`的组合使用提供了一种灵活且强大的方法来优化训练过程，使其更加符合实际应用中对模型性能的需求。这种方法的引入，无疑为强化学习模型的训练提供了进一步的便利和效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067278e8",
   "metadata": {},
   "source": [
    "让我们通过另一个实际的例子进一步探索`StopTrainingOnRewardThreshold`回调的使用，这次我们将使用PPO（Proximal Policy Optimization）算法在`CartPole-v1`环境上进行训练。我们的目标是在智能体达到一定的性能水平（即平均奖励阈值）时自动停止训练。\n",
    "\n",
    "### 场景描述\n",
    "\n",
    "- **目标环境**：`CartPole-v1`，一个简单的平衡杆任务，非常适合作为强化学习的入门环境。\n",
    "- **训练算法**：PPO，一种效果良好的策略梯度算法，适用于解决广泛的强化学习问题。\n",
    "- **回调功能**：使用`EvalCallback`定期评估智能体性能，并通过`StopTrainingOnRewardThreshold`在达到特定奖励阈值时停止训练。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "# 创建训练和评估环境\n",
    "env = gym.make('CartPole-v1')\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# 定义停止训练的奖励阈值\n",
    "reward_threshold = 195\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=reward_threshold, verbose=1)\n",
    "\n",
    "# 设置评估回调以及停止训练的条件\n",
    "eval_callback = EvalCallback(eval_env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=1000, \n",
    "                             best_model_save_path='./logs/best_model/',\n",
    "                             verbose=1)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# 开始训练，并将EvalCallback作为回调传入\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "```\n",
    "\n",
    "### 代码解析\n",
    "\n",
    "- **环境设置**：初始化`CartPole-v1`环境用于训练和评估。这个环境任务要求智能体通过移动底座来平衡一根杆，直到杆保持直立一段时间。\n",
    "- **奖励阈值**：设定了一个奖励阈值`195`，意味着当智能体在评估期间的平均奖励达到这个值时，训练过程将自动停止。这个阈值是基于`CartPole-v1`环境的特点，其中智能体需要连续保持杆平衡195步才能被认为解决了问题。\n",
    "- **回调配置**：\n",
    "  - `StopTrainingOnRewardThreshold`：用于设置训练停止的奖励阈值条件。\n",
    "  - `EvalCallback`：配置了评估环境、评估频率、最佳模型保存路径和触发新最佳模型时的回调，确保在智能体表现改善时进行评估并在达到目标阈值时停止训练。\n",
    "- **模型训练**：使用PPO算法进行训练，并设置了总训练步数为10000步。虽然设置了总步数，但由于`StopTrainingOnRewardThreshold`的存在，一旦达到目标奖励阈值，训练将提前结束。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "通过在训练过程中加入`EvalCallback`和`StopTrainingOnRewardThreshold`回调，可以有效地监控智能体的性能，并在满足特定条件时自动停止训练。这种方法不仅节约了计算资源，避免了不必要的训练，还能帮助研究者或开发者快速达到预期的训练目标。此外，自动保存达到阈值的最佳模型为后续的模型评估和部署提供了便利。\n",
    "\n",
    "这个例子展示了如何利用`stable_baselines3`的高级回调功能来构建一个高效且自动化的训练流程，使强化学习项目更加灵活和易于管理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d0e1c",
   "metadata": {},
   "source": [
    "## 每N个时间步 EveryNTimesteps\n",
    "\n",
    "`EveryNTimesteps`是`stable_baselines3`中的一个事件回调，它允许你在每隔一定的时间步（timesteps）时触发指定的子回调。这个功能特别适合于需要定期执行某些操作的场景，例如定期保存模型、记录日志、或者评估智能体的性能。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "- **定期保存模型**：在训练过程中，每隔固定的时间步保存一次模型快照。\n",
    "- **定期评估智能体性能**：定期对智能体进行性能评估，以监控其学习进度。\n",
    "\n",
    "### 代码示例详解\n",
    "\n",
    "在以下示例中，我们将使用`PPO`算法在`Pendulum-v1`环境上训练一个模型，并利用`EveryNTimesteps`回调每隔500步触发一次`CheckpointCallback`，从而实现定期保存模型的功能。\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps\n",
    "\n",
    "# 初始化CheckpointCallback，设置每次触发时保存一次模型\n",
    "checkpoint_on_event = CheckpointCallback(save_freq=1, save_path=\"./logs/\")\n",
    "\n",
    "# 使用EveryNTimesteps回调并设置为每500步触发一次checkpoint_on_event\n",
    "event_callback = EveryNTimesteps(n_steps=500, callback=checkpoint_on_event)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n",
    "\n",
    "# 开始训练，并将event_callback作为回调传入\n",
    "model.learn(int(2e4), callback=event_callback)\n",
    "```\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- **`CheckpointCallback`**：用于保存模型的回调。在这个例子中，虽然`save_freq=1`，但由于它被包装在`EveryNTimesteps`内，实际上它的触发频率是由`EveryNTimesteps`控制的。\n",
    "- **`EveryNTimesteps`**：一个事件回调，用于定期触发子回调。在这里，它被设置为每500个时间步触发一次`checkpoint_on_event`。\n",
    "- **`n_steps`**：`EveryNTimesteps`的参数，定义了两次事件触发之间的最小时间步数。在本例中，每500步触发一次保存操作。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "使用`EveryNTimesteps`可以高度自定义训练过程中的定期任务执行，提高训练的灵活性和控制性。这不仅可以用来保存模型，还可以用来定期调整学习率、记录额外的统计信息、或者评估智能体的即时性能。\n",
    "\n",
    "通过这种方式，你可以确保在长时间的训练过程中，关键状态不会丢失，并且可以在任何特定时间点轻松恢复训练或评估模型性能。这对于运行大规模实验或者在不稳定的环境中进行训练特别有用，为研究者和开发者提供了额外的保障和便利。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf05a5",
   "metadata": {},
   "source": [
    "我们将通过另一个实际的例子来探讨`EveryNTimesteps`回调的使用，这次我们选择使用DQN算法在`LunarLander-v2`环境中训练智能体。此外，我们将结合使用`EvalCallback`来定期评估智能体的性能，并通过`EveryNTimesteps`回调在每1000个时间步骤触发性能评估。\n",
    "\n",
    "### 场景描述\n",
    "\n",
    "- **目标环境**：`LunarLander-v2`，这是一个更为复杂的环境，要求智能体学习如何安全地降落在月球上。\n",
    "- **训练算法**：DQN（Deep Q-Network），一种适用于离散动作空间的强化学习算法。\n",
    "\n",
    "### 回调功能\n",
    "\n",
    "- **定期性能评估**：使用`EvalCallback`在独立的评估环境中定期检查智能体的性能。\n",
    "- **定期触发机制**：通过`EveryNTimesteps`每1000个时间步触发一次性能评估。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, EveryNTimesteps\n",
    "\n",
    "# 创建训练环境\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# 创建独立的评估环境\n",
    "eval_env = gym.make('LunarLander-v2')\n",
    "\n",
    "# 设置定期性能评估的回调\n",
    "eval_callback = EvalCallback(eval_env, \n",
    "                             best_model_save_path='./logs/best_model/', \n",
    "                             log_path='./logs/results/', \n",
    "                             eval_freq=1000,  # 这里的频率将被EveryNTimesteps覆盖\n",
    "                             deterministic=True)\n",
    "\n",
    "# 使用EveryNTimesteps回调并设置为每1000步触发一次性能评估\n",
    "event_callback = EveryNTimesteps(n_steps=1000, callback=eval_callback)\n",
    "\n",
    "# 初始化DQN模型\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# 开始训练，并将event_callback作为回调传入\n",
    "model.learn(total_timesteps=50000, callback=event_callback)\n",
    "```\n",
    "\n",
    "### 代码解析\n",
    "\n",
    "- **环境初始化**：使用`LunarLander-v2`环境进行训练和评估，这个环境相比`CartPole-v1`更具挑战性，需要智能体学习如何控制着陆器在月面上安全着陆。\n",
    "- **回调配置**：\n",
    "  - `EvalCallback`用于定期评估智能体的性能，并将评估结果保存到指定路径。虽然在`EvalCallback`中指定了`eval_freq=1000`，但实际上这个频率会被`EveryNTimesteps`中的设置所覆盖。\n",
    "  - `EveryNTimesteps`回调被配置为每1000个时间步触发一次`eval_callback`，从而确保智能体的性能每隔1000步被评估一次。\n",
    "- **模型训练**：训练过程中使用DQN算法，训练总步数设置为50000步。通过`EveryNTimesteps`回调，我们可以确保在整个训练过程中定期进行性能评估。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "通过这个例子，我们可以看到`EveryNTimesteps`回调如何与其他回调（如`EvalCallback`）结合使用来实现复杂的训练控制逻辑。这种方法允许研究者和开发者在保持训练过程高度自动化的同时，对训练过程进行细粒度的监控和干预。这对于实验调优、资源管理以及实时性能跟踪非常有用，有助于提高强化学习项目的开发效率和模型的最终性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40ffcd",
   "metadata": {},
   "source": [
    "`EveryNTimesteps`是`stable_baselines3`库中用于周期性触发回调的一个工具类。这个类让你能够在每隔特定数量的时间步（timesteps）触发一个指定的回调。这非常适用于需要定期执行某些操作的场景，例如定期保存模型、记录日志、或者评估智能体的性能。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`n_steps`** (`int`)：定义了触发回调之间的时间步数间隔。这意味着每当累积的训练步数达到这个数值时，指定的回调将被执行。\n",
    "- **`callback`** (`BaseCallback`)：当事件被触发时，将调用的回调。这个参数应该是`BaseCallback`的一个实例或其子类的实例。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "当你希望在训练过程中定期执行某项任务时，`EveryNTimesteps`回调非常有用。比如：\n",
    "\n",
    "- **定期保存模型**：可以设置一个`CheckpointCallback`作为`callback`参数，以实现每隔一定时间步自动保存模型的功能。\n",
    "- **定期评估智能体性能**：通过设置`EvalCallback`作为`callback`参数，可以定期评估智能体在一个或多个评估环境上的性能。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "以下是一个示例，展示如何使用`EveryNTimesteps`每隔1000个时间步保存一次模型。\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EveryNTimesteps, CheckpointCallback\n",
    "import gym\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 初始化模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 创建CheckpointCallback，指定保存模型的路径\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1, save_path='./logs/', name_prefix='ppo_cartpole')\n",
    "\n",
    "# 使用EveryNTimesteps回调并设置为每1000步触发一次checkpoint_callback\n",
    "trigger_checkpoint_callback = EveryNTimesteps(n_steps=1000, callback=checkpoint_callback)\n",
    "\n",
    "# 开始训练，并使用trigger_checkpoint_callback\n",
    "model.learn(total_timesteps=50000, callback=trigger_checkpoint_callback)\n",
    "```\n",
    "\n",
    "在这个例子中，每当训练过程中的时间步数达到1000的倍数时，`CheckpointCallback`将被触发，从而保存当前的模型状态。这种方法使得管理训练过程中的重要事件变得既简单又灵活。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1cff8",
   "metadata": {},
   "source": [
    "## 停止训练至最大次数 StopTrainingOnMaxEpisodes\n",
    "\n",
    "`StopTrainingOnMaxEpisodes`是`stable_baselines3`库中的一个回调，用于在训练达到最大集数（episode数）时自动停止训练。这个回调特别适用于那些希望控制训练集数而不是训练步数（timestep数）的情况。这在进行初步实验或者当你知道智能体在经过一定数量的集数后不太可能进一步改善其性能时非常有用。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "- 当你想要基于训练集数而非时间步来控制训练过程。\n",
    "- 在使用多环境训练时，确保总体训练集数符合预期。\n",
    "\n",
    "### 代码示例详解\n",
    "\n",
    "以下代码示例展示了如何使用`StopTrainingOnMaxEpisodes`回调在`Pendulum-v1`环境中使用A2C算法进行训练，并在达到最大集数时停止训练。\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "# 初始化停止训练的回调，设置最大集数为5\n",
    "callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=5, verbose=1)\n",
    "\n",
    "# 初始化模型\n",
    "model = A2C(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n",
    "\n",
    "# 开始训练，设置一个非常大的时间步数，但实际上训练会在达到最大集数后停止\n",
    "model.learn(int(1e10), callback=callback_max_episodes)\n",
    "```\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- **`StopTrainingOnMaxEpisodes`**：这个回调基于集数来控制训练过程的结束。在本例中，它被设置为在训练过程中达到5集时自动停止训练。\n",
    "- **多环境训练注意事项**：如果你使用的是多环境（`n_envs`），那么智能体将总共训练`max_episodes * n_envs`集。但是，不能保证每个环境恰好运行了`max_episodes`集。因此，这里有一个假设，即平均而言，每个环境运行了`max_episodes`集。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "使用`StopTrainingOnMaxEpisodes`可以让你更精确地控制训练过程的长度，特别是在那些集数更为关键的任务中。此外，这种基于集数的训练控制方式也使得实验更加可重复，因为它不依赖于模型在特定时间步的性能。\n",
    "\n",
    "通过`StopTrainingOnMaxEpisodes`回调，`stable_baselines3`为研究者和开发者提供了一种灵活而有效的工具，用以精细调控训练过程，帮助他们根据特定的实验需求和资源限制来优化训练策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af2249",
   "metadata": {},
   "source": [
    "我们将通过一个使用TRPO（Trust Region Policy Optimization）算法在`MountainCarContinuous-v0`环境中进行训练的例子，来进一步探讨`StopTrainingOnMaxEpisodes`回调的使用。在这个例子中，我们的目标是限制训练过程只进行一个固定数量的集数，无论智能体达到了何种性能水平。\n",
    "\n",
    "### 场景描述\n",
    "\n",
    "- **目标环境**：`MountainCarContinuous-v0`，一个连续动作空间的环境，要求智能体学习如何驾驶一辆车达到山顶。\n",
    "- **训练算法**：由于`stable_baselines3`不直接支持TRPO，我们将使用PPO作为一个替代，因为PPO是另一种高效的策略优化算法，适用于连续动作空间的问题。\n",
    "\n",
    "### 回调功能\n",
    "\n",
    "- **限制训练集数**：使用`StopTrainingOnMaxEpisodes`来确保训练过程在达到指定的最大集数后停止。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# 定义最大集数的回调\n",
    "max_episodes = 10\n",
    "stop_callback = StopTrainingOnMaxEpisodes(max_episodes=max_episodes, verbose=1)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 开始训练，即使设置了非常大的时间步数，训练也会在达到最大集数后停止\n",
    "model.learn(int(1e10), callback=stop_callback)\n",
    "```\n",
    "\n",
    "### 代码解析\n",
    "\n",
    "- **环境和模型**：首先，我们通过`gym.make`创建了`MountainCarContinuous-v0`环境，并使用PPO算法初始化了模型。该环境要求智能体学会控制一辆小车，使其能够达到一定的高度，模拟车子爬山的动作。\n",
    "- **`StopTrainingOnMaxEpisodes`回调**：我们创建了一个`StopTrainingOnMaxEpisodes`实例，设置`max_episodes=10`，意味着我们希望训练过程在进行10个集数后停止。这个回调在达到指定集数后自动终止训练，无论智能体的性能如何。\n",
    "- **训练过程**：通过调用`model.learn`方法开始训练，我们传递了一个很大的时间步数（`int(1e10)`）作为参数，但实际上训练会在完成指定的集数后提前结束。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "`StopTrainingOnMaxEpisodes`回调提供了一种便捷的方法来控制训练的持续集数，这在进行早期实验设计或者当训练资源受限时特别有用。它使得研究者可以根据实验需求精确地设定训练长度，而不必担心训练过度消耗计算资源。\n",
    "\n",
    "此外，当结合使用多环境训练时，这个回调还能帮助确保总体训练集数符合预期，即使无法保证每个环境恰好运行相同数量的集数。这种方法强调了在多环境设置中训练的平衡性和灵活性，有助于在不同的训练环境中均衡地推进学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb45af",
   "metadata": {},
   "source": [
    "`StopTrainingOnMaxEpisodes` 是 `stable_baselines3` 中用于控制训练过程，使其在达到最大集数（episodes）时自动停止的回调。这个功能对于在特定条件下精确控制训练持续时间非常有用，尤其是当你希望在每个环境中运行固定数量集数的实验时。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`max_episodes`** (`int`)：定义了停止训练前智能体能够玩的最大集数。当总集数达到这个值时，训练过程将自动停止。\n",
    "- **`verbose`** (`int`)：控制输出信息的详细程度。`0` 表示没有输出，`1` 表示在达到最大集数并停止训练时输出相关信息。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "`StopTrainingOnMaxEpisodes` 特别适用于以下场景：\n",
    "\n",
    "- **有限训练集数**：当你需要在固定集数后评估智能体性能，或者出于计算资源限制需要限制训练长度时。\n",
    "- **多环境训练**：在使用多个环境进行并行训练时，确保总体训练集数符合预期。对于多环境，假设你希望每个环境运行固定集数，总集数将是 `max_episodes * n_envs`。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "下面的示例演示了如何使用 `StopTrainingOnMaxEpisodes` 回调在训练 `PPO` 算法时，一旦达到100个集数就自动停止训练：\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "import gym\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 定义停止训练的回调\n",
    "stop_training_callback = StopTrainingOnMaxEpisodes(max_episodes=100, verbose=1)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 开始训练，并将stop_training_callback作为回调传入\n",
    "model.learn(total_timesteps=int(1e6), callback=stop_training_callback)\n",
    "```\n",
    "\n",
    "在这个例子中，`StopTrainingOnMaxEpisodes` 被设置为在100个集数后停止训练。这意味着无论总时间步数设置为多少（在本例中为 `1e6`），训练将在执行了100个集数后终止。如果你使用的是多环境并行训练，应该考虑到总集数将是每个环境集数乘以环境数。\n",
    "\n",
    "这种方式让训练过程更加灵活和可控，特别是在需要在特定集数后进行评估或其他操作时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88737ab5",
   "metadata": {},
   "source": [
    "## 没有模型改进就停止训练StopTrainingOnNoModelImprovement\n",
    "\n",
    "`StopTrainingOnNoModelImprovement`是`stable_baselines3`库中的一个高级回调，旨在在模型性能不再提升时提前停止训练。这个回调非常适用于那些学习曲线相对稳定、在一定训练阶段后不太可能出现显著提升的情况。通过设定一个阈值来监控连续若干次评估中模型性能的改进，如果超过设定的评估次数仍然没有新的最佳模型出现，就自动停止训练，以节省时间和计算资源。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "- **提升训练效率**：在模型性能达到瓶颈，进一步训练不再带来显著改善时，自动终止训练过程。\n",
    "- **自动化实验流程**：在进行大量实验时自动判断并停止那些不太可能进一步改进的训练，从而优化资源分配。\n",
    "\n",
    "### 代码示例详解\n",
    "\n",
    "在下面的示例中，我们使用SAC算法在`Pendulum-v1`环境上训练一个模型，并设置在连续3次评估中模型性能没有改进时停止训练。\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# 初始化独立的评估环境\n",
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# 设置停止训练的回调：如果在超过3次评估后没有模型改进，则停止训练\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "\n",
    "# 设置评估回调，其中包括停止训练的回调\n",
    "eval_callback = EvalCallback(eval_env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# 初始化模型并设置学习率\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", learning_rate=1e-3, verbose=1)\n",
    "\n",
    "# 开始训练，虽然设置了极大的时间步数，但训练可能会因为没有模型改进而提前停止\n",
    "model.learn(int(1e10), callback=eval_callback)\n",
    "```\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- **`StopTrainingOnNoModelImprovement`**：用于监控模型性能是否在连续几次评估后停止提升。在本例中，`max_no_improvement_evals=3`意味着如果连续3次评估后性能没有提升，训练将停止。\n",
    "- **`EvalCallback`**：用于定期评估模型的性能。通过`eval_freq=1000`设置每1000个时间步进行一次模型评估，`callback_after_eval`参数指定评估完成后执行的回调。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "使用`StopTrainingOnNoModelImprovement`可以显著提高训练过程的效率，特别是在处理那些训练早期快速改进但随后改进速度减慢的模型时。此外，这种方法有助于自动化训练流程，避免在不太可能进一步改善模型的情况下浪费计算资源。通过及时停止训练，研究者可以将注意力和资源集中于更有希望的模型配置或策略上，从而在实验和模型优化过程中实现更高的资源效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416eb8a",
   "metadata": {},
   "source": [
    "让我们通过一个使用DQN算法在`Breakout-v0`环境中训练模型的例子来进一步探索`StopTrainingOnNoModelImprovement`回调的使用。在这个例子中，我们的目标是在连续若干次评估中没有显著模型性能提升时自动停止训练，以节省计算资源。\n",
    "\n",
    "### 场景描述\n",
    "\n",
    "- **目标环境**：`Breakout-v0`，一个雅达利（Atari）游戏环境，要求智能体学习如何打砖块。\n",
    "- **训练算法**：DQN（Deep Q-Network），一种适用于处理离散动作空间问题的深度强化学习算法。\n",
    "\n",
    "### 回调功能\n",
    "\n",
    "- **自动终止训练**：当模型在连续多次评估后没有新的最佳性能时自动停止训练。\n",
    "\n",
    "### 完整代码示例\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "# 创建评估环境\n",
    "eval_env = gym.make('Breakout-v0')\n",
    "\n",
    "# 定义在连续4次评估后无模型改进则停止训练的回调\n",
    "stop_on_no_improvement = StopTrainingOnNoModelImprovement(max_no_improvement_evals=4, verbose=1)\n",
    "\n",
    "# 定义评估回调，每5000步评估一次模型\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=stop_on_no_improvement, eval_freq=5000, verbose=1)\n",
    "\n",
    "# 初始化DQN模型\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1, learning_rate=1e-4)\n",
    "\n",
    "# 开始训练，设置一个非常大的训练步数，但训练可能会因为连续几次评估无改进而提前停止\n",
    "model.learn(int(1e7), callback=eval_callback)\n",
    "```\n",
    "\n",
    "### 代码解析\n",
    "\n",
    "- **环境和模型初始化**：首先，我们通过`gym.make`创建了`Breakout-v0`环境，并使用`DQN`算法和`CnnPolicy`（卷积神经网络策略）初始化了模型。这种设置适用于处理视觉输入的任务，如雅达利游戏。\n",
    "- **`StopTrainingOnNoModelImprovement`回调**：我们创建了一个`StopTrainingOnNoModelImprovement`实例，设置`max_no_improvement_evals=4`，表示如果在连续4次评估中都没有模型性能改进，则自动停止训练。\n",
    "- **`EvalCallback`配置**：通过`EvalCallback`进行模型的定期评估，并在发现新的最佳模型时触发`stop_on_no_improvement`回调。`eval_freq=5000`设置了评估频率。\n",
    "- **模型训练**：通过调用`model.learn`方法开始训练，尽管设置了一个很大的训练步数（`int(1e7)`），但训练会在连续几次评估没有性能提升时提前结束。\n",
    "\n",
    "### 应用价值\n",
    "\n",
    "使用`StopTrainingOnNoModelImprovement`可以有效地防止过度训练和资源浪费，尤其在模型已经达到或接近其性能极限时。这种方法使得训练过程更加智能和高效，有助于在大规模实验和自动化模型调优过程中节省宝贵的计算资源。\n",
    "\n",
    "此外，这种基于性能改进的训练停止机制也为深度强化学习实验提供了一个实用的早停策略（early stopping strategy），有助于快速识别出性能\n",
    "\n",
    "瓶颈，从而指导进一步的算法调整和改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75771c",
   "metadata": {},
   "source": [
    "`StopTrainingOnNoModelImprovement` 是 `stable_baselines3` 中设计用于在模型性能在连续多次评估后没有进一步提升时自动停止训练的回调。这个回调可以帮助避免过度训练和计算资源的浪费，特别是在模型性能已经稳定或达到瓶颈时。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`max_no_improvement_evals`** (`int`)：指定在多少次连续评估中没有出现新的最佳模型（即最高平均奖励）时停止训练。这是判断训练提前结束的关键参数。\n",
    "- **`min_evals`** (`int`)：在开始计算没有改进的评估次数之前需要进行的最小评估次数。这允许模型有足够的时间来稳定其性能，避免因早期波动导致的过早停止。\n",
    "- **`verbose`** (`int`)：控制输出信息的详细程度。`0` 表示没有输出，`1` 表示在训练因没有新的最佳模型而提前结束时输出相关信息。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "当你想要确保训练过程在模型性能不再显著提升时自动停止，从而节省计算资源并避免不必要的训练。特别适用于模型已经接近其性能上限时，进一步训练不太可能带来显著改善的场景。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "假设我们正在使用PPO算法训练`LunarLander-v2`环境的智能体，并希望在连续5次评估中没有新的最佳模型时停止训练：\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# 创建评估环境\n",
    "eval_env = gym.make('LunarLander-v2')\n",
    "\n",
    "# 定义在连续5次评估后无模型改进则停止训练的回调\n",
    "stop_on_no_improvement = StopTrainingOnNoModelImprovement(max_no_improvement_evals=5, verbose=1)\n",
    "\n",
    "# 定义评估回调，每10000步评估一次模型\n",
    "eval_callback = EvalCallback(eval_env, eval_freq=10000, callback_on_new_best=None, verbose=1, callback_after_eval=stop_on_no_improvement)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 开始训练，设置一个较大的训练步数，但训练可能会因为连续几次评估无改进而提前停止\n",
    "model.learn(total_timesteps=int(2e5), callback=eval_callback)\n",
    "```\n",
    "\n",
    "在这个例子中，`StopTrainingOnNoModelImprovement` 被设置为在连续5次评估后，如果没有发现新的最佳模型，则自动停止训练。通过这种方式，训练过程更加智能化，能够自动在模型性能稳定后终止，从而节省资源并避免过度训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973b592",
   "metadata": {},
   "source": [
    "## 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66787b4a",
   "metadata": {},
   "source": [
    "`ConvertCallback`是`stable_baselines3`库中用于将传统风格的函数回调转换为对象回调的一个工具类。在`stable_baselines3`的早期版本中，回调是以函数的形式被定义和使用的。随着库的发展，回调机制被重构为基于对象的系统，以提供更多的灵活性和功能。`ConvertCallback`允许开发者将旧式的函数回调转换为兼容新回调系统的对象，从而无需重写现有的回调逻辑即可继续使用。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`callback`**：这是一个函数类型的参数，它接收两个字典类型的参数（通常是`locals`和`globals`），并返回一个布尔值。这个函数是你希望在训练过程中被调用的旧式回调函数。\n",
    "- **`verbose`** (int)：控制输出信息的详细程度，其中`0`表示没有输出，`1`用于普通信息消息，`2`用于调试信息。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "`ConvertCallback`主要用于以下场景：\n",
    "\n",
    "- 当你从`stable_baselines3`的早期版本迁移代码到较新版本时，而你的项目中已经使用了基于函数的回调。\n",
    "- 当你希望在不重写既有回调逻辑的情况下，利用`stable_baselines3`提供的新回调系统的优点和功能。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "假设我们有一个简单的旧式回调函数，它在每个训练步骤后打印一条消息，并且我们希望在使用`PPO`训练`CartPole-v1`环境时调用它：\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import ConvertCallback\n",
    "\n",
    "# 定义一个旧式的回调函数\n",
    "def simple_callback(locals_, globals_):\n",
    "    print(\"A training step was completed.\")\n",
    "    return True  # 继续训练\n",
    "\n",
    "# 将函数回调转换为对象回调\n",
    "converted_callback = ConvertCallback(callback=simple_callback, verbose=1)\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 初始化模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 使用转换后的回调开始训练\n",
    "model.learn(total_timesteps=10000, callback=converted_callback)\n",
    "```\n",
    "\n",
    "在这个示例中，`ConvertCallback`接收了一个名为`simple_callback`的旧式回调函数，并将其转换为一个对象回调。这使得旧式回调能够在`stable_baselines3`的新回调系统中无缝使用，而无需对旧代码进行大幅修改。\n",
    "\n",
    "通过`ConvertCallback`，`stable_baselines3`提供了一种平滑过渡的方式，帮助开发者从基于函数的回调迁移到更加强大和灵活的基于对象的回调系统，从而提高代码的可维护性和扩展性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498f898",
   "metadata": {},
   "source": [
    "`StopTrainingOnRewardThreshold` 是 `stable_baselines3` 库中用于在智能体达到一定的奖励阈值时自动停止训练的回调。这个功能非常有用，尤其是当你有明确的性能目标，并希望在达到这个目标后立即停止训练以节省计算资源时。\n",
    "\n",
    "### 主要参数\n",
    "\n",
    "- **`reward_threshold`** (`float`)：定义了停止训练前智能体期望每集（episode）能够达到的最小奖励。当智能体在评估环境中的平均奖励达到或超过这个阈值时，训练过程将自动停止。\n",
    "- **`verbose`** (`int`)：控制输出信息的详细程度。`0` 表示没有输出，`1` 表示在训练因达到奖励阈值而提前结束时输出相关信息。\n",
    "\n",
    "### 使用场景\n",
    "\n",
    "`StopTrainingOnRewardThreshold` 回调适用于以下情景：\n",
    "\n",
    "- 当你对智能体的性能有明确的预期，并希望在达到这个预期时自动停止训练。\n",
    "- 在自动化实验和模型调优过程中，用于在满足特定奖励标准后终止训练，从而避免不必要的训练步骤。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "假设我们正在使用PPO算法训练`CartPole-v1`环境的智能体，并希望在平均奖励达到195以上时停止训练：\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 创建评估环境\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# 定义停止训练的回调\n",
    "reward_threshold_callback = StopTrainingOnRewardThreshold(reward_threshold=195, verbose=1)\n",
    "\n",
    "# 定义评估回调\n",
    "eval_callback = EvalCallback(eval_env, eval_freq=1000, callback_on_new_best=None, \n",
    "                             verbose=1, callback_after_eval=reward_threshold_callback)\n",
    "\n",
    "# 初始化PPO模型\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 开始训练\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "```\n",
    "\n",
    "在这个例子中，`StopTrainingOnRewardThreshold` 被设置为在智能体的平均奖励达到195时停止训练。通过与 `EvalCallback` 结合使用，我们可以定期评估智能体的性能，并在满足奖励阈值时自动终止训练。这种方法允许智能体在达到期望的性能水平后立即停止训练，节省了计算资源并减少了训练时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29851228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
