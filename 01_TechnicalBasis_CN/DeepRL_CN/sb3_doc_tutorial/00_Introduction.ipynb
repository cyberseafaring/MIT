{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54050275",
   "metadata": {},
   "source": [
    "# 01. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686d3f3",
   "metadata": {},
   "source": [
    "首先，Stable-Baselines3在内部使用了向量化环境（VecEnv）。这是因为VecEnv可以同时运行多个环境实例，这对于加速训练过程非常有帮助。向量化环境与单一Gym环境的主要区别在于它能够处理并行环境的数据，这对于某些算法来说是必需的。\n",
    "\n",
    "接下来，Stable-Baselines3尽量遵循了一个类似于sklearn的语法来实现强化学习算法，这使得库的使用更加直观和简单。\n",
    "\n",
    "以下是如何在CartPole环境上训练和运行A2C算法的快速示例，以及相应的中文注释：\n",
    "\n",
    "```python\n",
    "# 导入gymnasium库，它是OpenAI Gym的一个分支，用于创建和管理不同的模拟环境\n",
    "import gymnasium as gym\n",
    "\n",
    "# 从stable_baselines3库导入A2C算法\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# 创建CartPole环境实例，并设置渲染模式为rgb_array\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# 初始化A2C模型，使用多层感知器（MLP）策略，并设置环境和verbose（冗长模式）为1\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 训练模型，设置总时间步数为10,000\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "# 获取环境实例\n",
    "vec_env = model.get_env()\n",
    "\n",
    "# 重置环境，获取初始观察值\n",
    "obs = vec_env.reset()\n",
    "\n",
    "# 循环1000次，进行预测和环境交互\n",
    "for i in range(1000):\n",
    "    # 根据当前观察值预测动作，此处设置为确定性预测\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # 执行动作，获取新的观察值、奖励、完成标志和额外信息\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    \n",
    "    # 渲染环境\n",
    "    vec_env.render(\"human\")\n",
    "    \n",
    "    # 如果环境已完成，VecEnv会自动重置，因此不需要手动重置\n",
    "    # if done:\n",
    "    #   obs = vec_env.reset()\n",
    "```\n",
    "\n",
    "此代码段首先创建了一个CartPole环境，并使用A2C算法和多层感知器（MLP）策略对其进行训练。训练过程中，模型会学习如何在环境中执行动作以最大化累积奖励。训练完成后，代码将通过模型预测动作来与环境交互，并渲染结果。\n",
    "\n",
    "此外，如果环境已在Gymnasium中注册且策略也已注册，则可以使用一行代码来训练模型：\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# 使用一行代码创建并训练模型，环境为\"CartPole-v1\"，总时间步数为10000\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\").learn(10000)\n",
    "```\n",
    "\n",
    "这段代码实现了相同的功能，但更加简洁。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947002c5",
   "metadata": {},
   "source": [
    "# 02. Reinforcement Learning Tips and Tricks 强化学习提示和技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252794a",
   "metadata": {},
   "source": [
    "## General advice when using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a55aa6",
   "metadata": {},
   "source": [
    "### 强化学习使用时的一般建议\n",
    "\n",
    "#### 简而言之：\n",
    "\n",
    "- 阅读关于RL和Stable Baselines3的资料\n",
    "- 进行定量实验和必要时的超参数调优\n",
    "- 使用单独的测试环境评估性能（记得检查环境包装器！）\n",
    "- 为了更好的性能，增加训练预算\n",
    "\n",
    "如果你想要从事RL工作，你首先应该阅读相关资料，以理解你正在使用的内容。我们建议你阅读Stable Baselines3（SB3）的文档，并完成教程。这些教程不仅覆盖了基本使用方法，还引导你了解库的更高级概念（例如，回调和包装器）。\n",
    "\n",
    "强化学习与其他机器学习方法不同，它通过智能体自身与环境的交互来收集用于训练的数据（与有固定数据集的监督学习相对）。这种依赖可能导致恶性循环：如果智能体收集到的是质量差的数据（例如，没有奖励的轨迹），那么它就不会改进，继续累积坏的轨迹。\n",
    "\n",
    "这一因素，以及其他因素，解释了为什么RL的结果可能在不同运行中有所不同（即，当伪随机生成器的种子仅仅发生变化时）。因此，你应该总是进行多次运行以获得定量结果。\n",
    "\n",
    "在RL中获得好结果通常依赖于找到合适的超参数。尽管最新的算法（如PPO、SAC、TD3、DroQ）通常需要很少的超参数调整，但不要期望默认的超参数在任何环境上都有效。\n",
    "\n",
    "因此，我们强烈建议你查看RL动物园（或原始论文）以获得调优后的超参数。当你将RL应用到一个新问题时，进行自动超参数优化是一个最佳实践。同样，这也包含在RL动物园中。\n",
    "\n",
    "当将RL应用到自定义问题时，你应该始终对智能体的输入进行标准化（例如，对于PPO/A2C使用VecNormalize）并查看在其他环境上完成的常见预处理（例如，对于Atari，帧堆叠等）。如果你想创建自定义环境，请参考下面有关创建自定义环境时的技巧和建议部分以获得更多建议。\n",
    "\n",
    "### 创建自定义环境时的技巧和建议\n",
    "\n",
    "- **标准化输入**：使用如VecNormalize之类的工具来标准化智能体的输入，这有助于提高学习效率和稳定性。\n",
    "- **预处理**：查看针对其他环境所做的常见预处理步骤，如Atari环境的帧堆叠，可以为你的环境提供一些启示。\n",
    "- **超参数调整**：考虑到不同环境的特性可能有所不同，为你的特定问题进行超参数调整是非常重要的。可以使用RL动物园作为参考或进行自动超参数优化。\n",
    "- **多次运行实验**：由于RL结果的变化性，进行多次运行以获得更稳定和可靠的性能评估是很重要的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dfe17",
   "metadata": {},
   "source": [
    "## Current Limitations of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87c036",
   "metadata": {},
   "source": [
    "强化学习（RL）的当前局限性需要被认识到。\n",
    "\n",
    "### 模型无关的RL算法（Model-free RL）的样本效率低\n",
    "\n",
    "模型无关的RL算法（即SB中实现的所有算法）通常样本效率低下。它们需要大量的样本（有时达到数百万次交互）才能学习到有用的东西。这也是为什么RL的大部分成功案例都是在游戏或模拟环境中实现的。例如，ETH Zurich的研究中，ANYmal机器人仅在模拟环境中训练，然后在现实世界中进行测试。\n",
    "\n",
    "#### 如何应对\n",
    "为了获得更好的性能，一个通用的建议是增加智能体的训练预算（训练时间步的数量）。\n",
    "\n",
    "### 设计合适的奖励函数需要专家知识\n",
    "\n",
    "为了实现期望的行为，经常需要专家知识来设计一个适当的奖励函数。这种奖励工程（或如Freek Stulp所称的RewArt）需要多次迭代。作为奖励塑形的一个好例子，你可以查看Deep Mimic论文，该论文结合了模仿学习和强化学习来完成杂技动作。\n",
    "\n",
    "#### 如何应对\n",
    "设计奖励函数时，考虑利用模仿学习或者奖励塑形技术来简化学习过程，或者通过多次迭代来优化奖励函数。\n",
    "\n",
    "### 训练的不稳定性\n",
    "\n",
    "RL的另一个局限性是训练的不稳定性。也就是说，在训练过程中，你可能会观察到性能的巨大下降。这种行为在DDPG中特别常见，这也是其扩展TD3试图解决该问题的原因。其他方法，如TRPO或PPO，通过使用信任区域来避免过大的更新来最小化该问题。\n",
    "\n",
    "#### 如何应对\n",
    "选择更稳定的算法，如TD3、TRPO或PPO，或者通过细心调整算法的超参数来减少训练过程中的性能波动。\n",
    "\n",
    "这些局限性表明，在将RL应用于实际问题时，需要仔细考虑如何设计实验和算法选择。理解这些局限性并探索解决方案对于成功应用RL至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60baf78",
   "metadata": {},
   "source": [
    "## How to evaluate an RL algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248c97b",
   "metadata": {},
   "source": [
    "在评估强化学习（RL）算法时，考虑以下要点至关重要：\n",
    "\n",
    "### 注意环境包装器\n",
    "\n",
    "当评估你的智能体并与他人的结果进行比较时，要特别注意环境包装器的使用。对剧集奖励或长度的修改也可能影响评估结果，这可能不是你想要的。请检查评估助手部分中的`evaluate_policy`帮助函数。\n",
    "\n",
    "### 使用单独的测试环境\n",
    "\n",
    "由于大多数算法在训练期间使用探索噪声，因此你需要一个单独的测试环境来评估你的智能体在给定时间的性能。建议定期评估你的智能体进行n个测试剧集（n通常在5到20之间），并平均每个剧集的奖励，以便有一个好的估计。\n",
    "\n",
    "### 使用EvalCallback进行评估\n",
    "\n",
    "我们提供了一个`EvalCallback`来进行此类评估。你可以在回调部分阅读更多相关信息。\n",
    "\n",
    "### 考虑确定性预测\n",
    "\n",
    "由于一些策略默认是随机的（例如A2C或PPO），当调用`.predict()`方法时，你也应该尝试设置`deterministic=True`，这通常会带来更好的性能。查看训练曲线（剧集奖励函数随时间步的变化）是一个好的代理，但它低估了智能体的真实性能。\n",
    "\n",
    "### 推荐阅读\n",
    "\n",
    "我们强烈推荐阅读《Empirical Design in Reinforcement Learning》，因为它提供了进行RL实验时最佳实践的宝贵见解。\n",
    "\n",
    "我们还建议阅读《Deep Reinforcement Learning that Matters》以获得关于RL评估的良好讨论，以及《Rliable: Better Evaluation for Reinforcement Learning》以比较结果。\n",
    "\n",
    "你也可以查看Cédric Colas的这篇博客文章和这个问题，以获得更多信息。\n",
    "\n",
    "通过遵循这些建议，你可以更有效地评估RL算法，并确保你的评估结果是准确和可靠的。这不仅有助于你理解智能体的性能，也使得与其他研究或实现的比较更为公正和透明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51656e53",
   "metadata": {},
   "source": [
    "## Which algorithm should I use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7b9a5",
   "metadata": {},
   "source": [
    "RL 中没有灵丹妙药，根据您的需求和问题，您可以选择其中之一。第一个区别来自你的动作空间，即你有离散的（例如向左、向右……）还是连续的动作（例如：达到一定的速度）？\n",
    "\n",
    "有些算法仅针对一个或另一个域进行定制：DQN 仅支持离散操作，而 SAC 仅限于连续操作。\n",
    "\n",
    "帮助您选择的第二个区别是您是否可以并行训练。如果重要的是挂钟训练时间，那么您应该倾向于 A2C 及其衍生物（PPO，...）。查看矢量化环境，了解有关多名工作人员培训的更多信息。\n",
    "\n",
    "为了加速训练，你还可以看看 SBX，它是 SB3 + Jax，它的功能比 SB3 少，但由于梯度更新的 JIT 编译，速度比 SB3 PyTorch 快 20 倍。\n",
    "\n",
    "在稀疏奖励设置中，我们建议使用 HER（见下文）等专用方法或 ARS（可在我们的 contrib 存储库中找到）等基于群体的算法。\n",
    "\n",
    "选择合适的强化学习（RL）算法取决于你的具体需求和问题类型。以下是一些基于不同情况选择算法的建议：\n",
    "\n",
    "### 根据动作空间选择算法\n",
    "\n",
    "- **离散动作空间**（例如，LEFT, RIGHT...）：\n",
    "  - **单进程**：推荐使用DQN及其扩展（双重DQN、优先回放等）。我们在contrib仓库中提供了QR-DQN。DQN通常训练速度较慢（考虑到墙钟时间），但因为有回放缓冲，所以是最有效的样本。\n",
    "  - **多进程**：可以尝试PPO或A2C。\n",
    "\n",
    "- **连续动作空间**（例如，以特定速度行进）：\n",
    "  - **单进程**：当前的最先进算法包括SAC、TD3和TQC（可在我们的contrib仓库中找到）。使用RL动物园中的超参数可获得最佳结果。\n",
    "  - **多进程**：查看PPO、TRPO（可在我们的contrib仓库中找到）或A2C。对于连续动作问题，别忘了从RL动物园中获取超参数（参考Bullet环境）。\n",
    "\n",
    "### 特殊情况\n",
    "\n",
    "- **稀疏奖励设置**：推荐使用专用方法如HER，或基于种群的算法如ARS（在我们的contrib仓库中可用）。\n",
    "- **加速训练**：可以查看SBX（SB3 + Jax），它的功能比SB3少，但由于JIT编译梯度更新，速度可以比SB3 PyTorch快达20倍。\n",
    "- **目标环境**（GoalEnv接口）：应该使用HER +（SAC/TD3/DDPG/DQN/QR-DQN/TQC），具体取决于动作空间。\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "- **标准化**：对于这些算法来说，输入的标准化至关重要。\n",
    "- **批量大小**（batch_size）：在使用HER进行实验时，批量大小是一个重要的超参数。\n",
    "\n",
    "根据你的具体问题和环境类型选择合适的算法，并根据需要进行适当的调整和优化，是实现最佳RL性能的关键。始终记得检查和使用针对特定算法和环境优化的超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d415e25",
   "metadata": {},
   "source": [
    "## Tips and Tricks when creating a custom environment\n",
    "\n",
    "当你打算创建一个自定义环境时，以下是一些建议和技巧，可以帮助你更好地设计和实现环境，以便与强化学习（RL）算法一起使用：\n",
    "\n",
    "### 规范化你的观察空间和动作空间\n",
    "\n",
    "- **观察空间**：尽可能规范化你的观察空间，即当你知道边界时，确保所有的观察值都在一个固定的范围内，比如[-1, 1]。这有助于算法更快地学习。\n",
    "- **动作空间**：对于连续动作空间，规范化并使其对称（通常规模化到[-1, 1]）。这不会限制你，因为你可以在环境内部轻松地重新缩放动作。这是因为大多数RL算法依赖于高斯分布（最初以0为中心，标准差为1）来处理连续动作。\n",
    "\n",
    "### 使用形状奖励和简化版的问题开始\n",
    "\n",
    "- 尝试从提供信息性奖励的简化版本的问题开始，这有助于智能体快速学习。\n",
    "\n",
    "### 使用随机动作调试\n",
    "\n",
    "- 使用随机动作来检查你的环境是否按照Gym接口正常工作。这是一个检查环境是否有错误的简单方法。\n",
    "\n",
    "### 处理自定义环境的特殊情况\n",
    "\n",
    "- **马尔可夫假设**：避免破坏马尔可夫假设，确保每个状态包含了做出决策所需的所有信息。\n",
    "- **超时终止**：正确处理由于超时（一个剧集中的最大步数）而终止。如果使用了gym的`TimeLimit`包装器，这将自动完成。对于超时，你应该返回`truncated=True`。\n",
    "\n",
    "### 检查你的环境是否有错误\n",
    "\n",
    "你可以使用Stable Baselines3提供的环境检查器来检查你的自定义环境是否有错误：\n",
    "\n",
    "```python\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = CustomEnv(arg1, ...)\n",
    "# 它将检查你的自定义环境，并在需要时输出额外的警告\n",
    "check_env(env)\n",
    "```\n",
    "\n",
    "### 使用随机智能体快速尝试你的环境\n",
    "\n",
    "如果你想快速试验一个随机智能体在你的环境中的表现，你也可以这样做：\n",
    "\n",
    "```python\n",
    "env = YourEnv()\n",
    "obs, info = env.reset()\n",
    "n_steps = 10\n",
    "for _ in range(n_steps):\n",
    "    # 随机动作\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "```\n",
    "\n",
    "### 为什么需要规范化动作空间？\n",
    "\n",
    "大多数RL算法依赖于连续动作的高斯分布。如果在使用自定义环境时忘记规范化动作空间，这可能会阻碍学习并难以调试。使用高斯分布的另一个后果是动作范围不受限制，通常使用剪辑作为临时解决方案以保持在有效区间内。一个更好的解决方案是使用压缩函数（如SAC中使用的）或Beta分布。\n",
    "\n",
    "请注意，对于DDPG或TD3，这个建议不适用，因为它们不依赖于任何概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d3d19",
   "metadata": {},
   "source": [
    "## Tips and Tricks when implementing an RL algorithm\n",
    "\n",
    "当你尝试通过实现算法来复现一个强化学习（RL）论文时，以下是一些建议和技巧，可以帮助你更有效地进行研究和开发：\n",
    "\n",
    "### 研究原始论文\n",
    "\n",
    "- **多次阅读原始论文**：确保你完全理解论文中的算法和方法。\n",
    "- **查看现有实现**：如果可用，查看其他人的实现可以帮助你更好地理解算法的细节。\n",
    "\n",
    "### 在简单问题上测试\n",
    "\n",
    "- **在玩具问题上寻找“生命迹象”**：首先在简单的环境上测试你的算法，以确保它能够学习并取得进展。\n",
    "\n",
    "### 逐步验证实现\n",
    "\n",
    "- **在更难的环境上验证实现**：一旦在简单环境上工作正常，尝试在越来越难的环境上运行你的算法。你可以将结果与RL动物园中的结果进行比较。这通常需要进行超参数优化。\n",
    "\n",
    "### 注意对象形状和梯度传播\n",
    "\n",
    "- **注意操作对象的形状**：要特别注意你正在操作的不同对象的形状。广播错误可能会无声地失败（参见issue #75）。\n",
    "- **正确处理梯度传播**：注意何时停止梯度传播，以避免错误。\n",
    "\n",
    "### 处理超时终止\n",
    "\n",
    "- **单独处理超时终止**：如前文自定义环境部分所述，不要忘了单独处理由于超时而终止的情况（参见Issue #284和Issue #633）。\n",
    "\n",
    "### 逐渐提高环境难度\n",
    "\n",
    "对于**连续动作的RL**，以下是一些建议的环境，按难度递增排序：\n",
    "\n",
    "- **Pendulum**（易于解决）\n",
    "- **HalfCheetahBullet**（中等难度，有局部最小值和形状奖励）\n",
    "- **BipedalWalkerHardcore**（如果在这个环境上工作，那么你可以奖励自己一块饼干）\n",
    "\n",
    "对于**离散动作的RL**：\n",
    "\n",
    "- **CartPole-v1**（比随机智能体好是容易的，但实现最大性能较难）\n",
    "- **LunarLander**\n",
    "- **Pong**（最容易的Atari游戏之一）\n",
    "- **其他Atari游戏**（例如Breakout）\n",
    "\n",
    "通过遵循这些建议，你可以更有效地实现和验证RL算法，确保你的研究和开发工作能够顺利进行。始终记得验证你的实现，并在可能的情况下与现有的基准和实现进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff1bba",
   "metadata": {},
   "source": [
    "# 03. RL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3bc79",
   "metadata": {},
   "source": [
    "这段内容提供了Stable Baselines3项目中实现的强化学习（RL）算法的概览，包括它们对离散/连续动作、多进程支持的相关信息。这里将这些信息以便于理解的方式总结如下：\n",
    "\n",
    "| 算法名     | 支持Box | 支持Discrete | 支持MultiDiscrete | 支持MultiBinary | 支持多进程 |\n",
    "|------------|:-------:|:------------:|:-----------------:|:---------------:|:----------:|\n",
    "| ARS        | ✔️       | ✔️           | ❌                | ❌              | ✔️         |\n",
    "| A2C        | ✔️       | ✔️           | ✔️                | ✔️              | ✔️         |\n",
    "| DDPG       | ✔️       | ❌           | ❌                | ❌              | ✔️         |\n",
    "| DQN        | ❌       | ✔️           | ❌                | ❌              | ✔️         |\n",
    "| HER        | ✔️       | ✔️           | ❌                | ❌              | ✔️         |\n",
    "| PPO        | ✔️       | ✔️           | ✔️                | ✔️              | ✔️         |\n",
    "| QR-DQN     | ❌       | ✔️           | ❌                | ❌              | ✔️         |\n",
    "| RecurrentPPO| ✔️      | ✔️           | ✔️                | ✔️              | ✔️         |\n",
    "| SAC        | ✔️       | ❌           | ❌                | ❌              | ✔️         |\n",
    "| TD3        | ✔️       | ❌           | ❌                | ❌              | ✔️         |\n",
    "| TQC        | ✔️       | ❌           | ❌                | ❌              | ✔️         |\n",
    "| TRPO       | ✔️       | ✔️           | ✔️                | ✔️              | ✔️         |\n",
    "| Maskable PPO| ❌      | ✔️           | ✔️                | ✔️              | ✔️         |\n",
    "\n",
    "注释：\n",
    "- 标记为[1]的算法在SB3 Contrib中实现。\n",
    "- Tuple观察空间不被任何算法支持，但是单级Dict空间是支持的。\n",
    "\n",
    "动作空间类型：\n",
    "- **Box**：一个N维盒子，包含动作空间中的每一点。\n",
    "- **Discrete**：一系列可能的动作，每个时间步只能使用其中一个动作。\n",
    "- **MultiDiscrete**：一系列可能的动作，每个时间步可以从每个离散集合中使用一个动作。\n",
    "- **MultiBinary**：一系列可能的动作，每个时间步可以以任何组合使用这些动作。\n",
    "\n",
    "关于可重复性：\n",
    "- 在不同的PyTorch版本或不同平台之间，不能保证完全可重复的结果。即使使用相同的种子，在CPU和GPU执行之间的结果也可能不可重复。\n",
    "- 为了使计算在特定问题上在一个特定平台上确定性，你需要在创建模型时传递一个种子参数。如果你使用`set_env()`将环境传递给模型，那么你也需要首先为环境设置种子。\n",
    "\n",
    "这些信息有助于选择合适的算法来解决你的特定RL问题，并理解在实现和测试RL算法时需要注意的一些关键点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f9a0d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
