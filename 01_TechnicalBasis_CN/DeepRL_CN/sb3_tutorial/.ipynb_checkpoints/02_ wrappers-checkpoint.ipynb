{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01b57e8",
   "metadata": {},
   "source": [
    "了解如何使用 Gym Wrappers，它可以进行监控、标准化、限制步数、功能增强等。\n",
    "\n",
    "查看加载和保存功能，以及如何读取输出的文件以进行可能的导出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362d1f7",
   "metadata": {},
   "source": [
    "# gymnasium.wrappers包装器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd130b48",
   "metadata": {},
   "source": [
    "Gymnasium的包装器可以通过继承gymnasium.Wrapper类来创建。可以重写以下方法来自定义环境的行为：\n",
    "\n",
    "- reset(): 当环境重置时调用。\n",
    "- step(action): 在每个步骤中调用，用于执行动作。\n",
    "- render(mode='human'): 用于环境的渲染。\n",
    "- close(): 当环境关闭时调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建自定义包装器\n",
    "# 假设我们想要创建一个简单的包装器，它会修改环境的奖励信号。以下是如何实现它的例子：\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class RewardModifierWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(RewardModifierWrapper, self).__init__(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # 修改奖励：将所有的奖励乘以一个常数因子\n",
    "        reward = reward * 3\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用自定义包装器\n",
    "# 一旦定义了自定义包装器，我们就可以将其应用到任何Gymnasium环境中：\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env = RewardModifierWrapper(env)\n",
    "\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated:\n",
    "        observation = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d3552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2cf4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a94d6c",
   "metadata": {},
   "source": [
    "保存和加载稳定sb模型非常简单：我们可以直接在模型上调用 .save() 和 .load() 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acba7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 这里定义了一个保存模型的目录/tmp/gym/。os.makedirs用于创建这个目录。参数exist_ok=True表示如果目录已存在，不会抛出错误。\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", \"Pendulum-v1\", verbose=0).learn(8_000)\n",
    "'''一行创建训练代码，创建并训练了一个使用PPO算法的模型。\n",
    "\"MlpPolicy\"表示使用的是多层感知器（MLP，一种简单的前馈神经网络）策略。\n",
    "\"Pendulum-v1\"是Gym提供的一个环境，挑战是控制一个倒立摆让它保持直立。\n",
    "verbose=0意味着在训练过程中不输出额外的日志信息。\n",
    ".learn(8_000)指定了训练步数为8000步。\n",
    "'''\n",
    "# The model will be saved under PPO_tutorial.zip\n",
    "model.save(f\"{save_dir}/PPO_tutorial\")\n",
    "\n",
    "# sample an observation from the environment\n",
    "obs = model.env.observation_space.sample()\n",
    "# Check prediction before saving\n",
    "print(\"pre saved\", model.predict(obs, deterministic=True))\n",
    "'''\n",
    "首先从模型的环境观测空间中随机采样一个观测值。然后，使用model.predict方法对这个观测值进行预测，\n",
    "deterministic=True参数确保预测是确定性的，即在给定相同的观测值下总是产生相同的动作。\n",
    "'''\n",
    "\n",
    "del model  # delete trained model to demonstrate loading\n",
    "loaded_model = PPO.load(f\"{save_dir}/PPO_tutorial\")\n",
    "'''\n",
    "为了演示加载功能，首先删除当前的model对象，然后使用PPO.load方法从保存的文件中加载模型。\n",
    "'''\n",
    "# Check that the prediction is the same after loading (for the same observation)\n",
    "print(\"loaded\", loaded_model.predict(obs, deterministic=True))\n",
    "'''\n",
    "使用同一个观测值obs，对加载后的模型进行预测，并打印结果。这里的目的是验证保存和加载过程是否正确无误，\n",
    "即确保加载后的模型能够产生与之前相同的预测结果。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cd017",
   "metadata": {},
   "source": [
    "sb的save方法非常强大，因为使用当前权重保存训练超参数。这意味着在实践中，可以简单地加载自定义模型，而无需重新定义参数，然后继续学习。\n",
    "\n",
    "加载函数还可以在加载时更新模型的类变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# DummyVecEnv是Stable Baselines 3提供的一个工具，用于创建虚拟环境的容器，可以提高某些类型算法的训练效率。\n",
    "# Create save dir\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"Pendulum-v1\", verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
    "'''\n",
    "使用A2C算法训练模型，选择MlpPolicy，目标环境是Pendulum-v1。gamma=0.9设置了折扣因子，用于计算未来奖励的当前价值；\n",
    "n_steps=20设置了每次更新模型之前的步数。训练过程中不显示额外的日志信息(verbose=0)，并训练8000个时间步。\n",
    "'''\n",
    "# The model will be saved under A2C_tutorial.zip\n",
    "model.save(f\"{save_dir}/A2C_tutorial\")\n",
    "\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# load the model, and when loading set verbose to 1\n",
    "loaded_model = A2C.load(f\"{save_dir}/A2C_tutorial\", verbose=1)\n",
    "#加载时设置verbose=1以显示日志信息。\n",
    "\n",
    "# show the save hyperparameters\n",
    "print(f\"loaded: gamma={loaded_model.gamma}, n_steps={loaded_model.n_steps}\")\n",
    "\n",
    "# as the environment is not serializable, we need to set a new instance of the environment\n",
    "loaded_model.set_env(DummyVecEnv([lambda: gym.make(\"Pendulum-v1\")]))\n",
    "'''\n",
    "由于环境不是可序列化的，因此在加载模型后需要设置一个新的环境实例。这里使用DummyVecEnv来创建环境，\n",
    "它允许模型以向量化的方式处理多个实例，即使在这个例子中只用到了一个环境实例。\n",
    "'''\n",
    "# and continue training\n",
    "loaded_model.learn(8_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa77cd",
   "metadata": {},
   "source": [
    "## 详细说明DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393c5ac",
   "metadata": {},
   "source": [
    "`DummyVecEnv`是Stable Baselines3库中的一个工具，用于创建一个简单的向量化环境。向量化环境允许你同时运行多个实例的环境，这样可以加快训练过程，因为它可以并行地收集多个环境的经验。然而，与更高级的并行环境如`SubprocVecEnv`不同，`DummyVecEnv`并不在真正的并行进程中运行这些环境实例，而是在单一进程中顺序执行它们。尽管如此，它仍然是用于测试和开发时简化环境管理的有用工具。\n",
    "\n",
    "### 为什么使用`DummyVecEnv`\n",
    "\n",
    "- **简化API**：它提供了一个统一的API来处理单个环境或多个环境的情况，让算法的实现可以无缝地在多个环境上运行。\n",
    "- **开发和测试**：在开发和测试阶段，`DummyVecEnv`可以帮助快速迭代和测试，而不需要设置复杂的多进程环境。\n",
    "- **兼容性**：它确保了与Stable Baselines3中的算法兼容，因为这些算法期望环境是向量化的。\n",
    "\n",
    "### 如何使用`DummyVecEnv`\n",
    "\n",
    "创建`DummyVecEnv`通常涉及将一个或多个环境的构造函数传递给它。下面是一个如何使用`DummyVecEnv`来包装单个Gym环境的例子：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff531086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "# 创建环境的函数\n",
    "def make_env():\n",
    "    return gym.make('CartPole-v1')\n",
    "\n",
    "# 使用DummyVecEnv包装环境\n",
    "env = DummyVecEnv([make_env])  # 注意这里传递的是一个函数列表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e70ed4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在这个例子中，`make_env`函数返回一个新的`CartPole-v1`环境实例。`DummyVecEnv`接受一个函数列表，每个函数在调用时应该返回一个新的环境实例。即使你只有一个环境，你也需要将构造函数放在列表中，因为`DummyVecEnv`期望能够处理多个环境。\n",
    "\n",
    "### `DummyVecEnv`和真实并行环境的对比\n",
    "\n",
    "虽然`DummyVecEnv`提供了一个向量化环境的简单实现，但它并不提供真正的并行执行能力。对于需要大规模并行收集数据以加速训练的情况，你可能需要考虑使用`SubprocVecEnv`或其他并行环境实现。这些实现使用Python的多进程功能来在真正的并行进程中运行每个环境实例，可以显著减少数据收集的时间。\n",
    "\n",
    "总之，`DummyVecEnv`是一个在单个进程中管理和执行多个环境实例的有用工具，非常适合于快速开发和测试，但在处理需要高效并行数据收集的复杂场景时，可能需要更高级的向量化环境实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de06fe",
   "metadata": {},
   "source": [
    "## Gym and VecEnv wrappers包装器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ab7a8",
   "metadata": {},
   "source": [
    "gym 包装器遵循gym 接口：它有一个reset() 和step() 方法。\n",
    "\n",
    "因为包装器是围绕环境的，所以我们可以使用 self.env 访问它，这允许轻松地与其交互，而无需修改原始环境。有许多已预定义的包装器，有关完整列表，请参阅gym 文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f925847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, bool, dict) observation, reward, is this a final state (episode finished),\n",
    "        is the max number of steps reached (episode finished artificially), additional informations\n",
    "        \"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1568179",
   "metadata": {},
   "source": [
    "## 第一个例子：限制episode长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb96b0",
   "metadata": {},
   "source": [
    "包装器的一个实际用例是当想要按episode限制步骤数时，使用包装器。因为达到限制时需要覆盖done信号。在信息字典中传递该信息也是一个很好的做法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    :param max_steps: (int) Max number of steps per episode\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, max_steps=100):\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super(TimeLimitWrapper, self).__init__(env)\n",
    "        self.max_steps = max_steps\n",
    "        # Counter of steps per episode\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        \"\"\"\n",
    "        # Reset the counter\n",
    "        self.current_step = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # Overwrite the truncation signal when when the number of steps reaches the maximum\n",
    "        if self.current_step >= self.max_steps:\n",
    "            truncated = True\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the wrapper\n",
    "\n",
    "from gymnasium.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# Here we create the environment directly because gym.make() already wrap the environment in a TimeLimit wrapper otherwise\n",
    "env = PendulumEnv()\n",
    "# Wrap the environment\n",
    "env = TimeLimitWrapper(env, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99022a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "while not done:\n",
    "    # Take random actions\n",
    "    random_action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "    done = terminated or truncated\n",
    "    n_steps += 1\n",
    "\n",
    "print(n_steps, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057d291",
   "metadata": {},
   "source": [
    "实际上，gym 已经有一个名为 TimeLimit (gym.wrappers.TimeLimit) 的包装器，大多数环境都使用该包装器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca7c26",
   "metadata": {},
   "source": [
    "## 第二个示例：规范动作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facc496",
   "metadata": {},
   "source": [
    "在将观察和动作提供给agent之前将其标准化通常是一个好主意，这可以防止出现难以调试的问题。\n",
    "\n",
    "在此示例中，我们将标准化 Pendulum-v1 的动作空间，使其位于 [-1, 1] 而不是 [-2, 2]。\n",
    "\n",
    "注意：这里我们处理的是连续动作，因此是gym.Box空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9501aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Retrieve the action space\n",
    "        action_space = env.action_space\n",
    "        assert isinstance(\n",
    "            action_space, gym.spaces.Box\n",
    "        ), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "        # Retrieve the max/min values\n",
    "        self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "        # We modify the action space, so all actions will lie in [-1, 1]\n",
    "        env.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=action_space.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super(NormalizeActionWrapper, self).__init__(env)\n",
    "\n",
    "    def rescale_action(self, scaled_action):\n",
    "        \"\"\"\n",
    "        Rescale the action from [-1, 1] to [low, high]\n",
    "        (no need for symmetric action space)\n",
    "        :param scaled_action: (np.ndarray)\n",
    "        :return: (np.ndarray)\n",
    "        \"\"\"\n",
    "        return self.low + (0.5 * (scaled_action + 1.0) * (self.high - self.low))\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        \"\"\"\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float,bool, bool, dict) observation, reward, final state? truncated?, additional informations\n",
    "        \"\"\"\n",
    "        # Rescale action from [-1, 1] to original [low, high] interval\n",
    "        rescaled_action = self.rescale_action(action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(rescaled_action)\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ef7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.]\n",
      "[1.8424448]\n",
      "[-0.9342433]\n",
      "[1.5242685]\n",
      "[0.24446234]\n",
      "[1.6761957]\n",
      "[-0.29506642]\n",
      "[0.5200963]\n",
      "[-1.7365894]\n",
      "[1.2014906]\n",
      "[1.3566116]\n"
     ]
    }
   ],
   "source": [
    "# 重新缩放动作之前进行测试\n",
    "\n",
    "original_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(original_env.action_space.low)\n",
    "for _ in range(10):\n",
    "    print(original_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9f9276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.]\n",
      "[-0.07592568]\n",
      "[0.56407446]\n",
      "[0.19095245]\n",
      "[0.37957114]\n",
      "[-0.6485151]\n",
      "[0.64774466]\n",
      "[-0.5325325]\n",
      "[-0.31494254]\n",
      "[-0.8501827]\n",
      "[0.38907593]\n"
     ]
    }
   ],
   "source": [
    "# 测试 NormalizeAction 包装器\n",
    "\n",
    "env = NormalizeActionWrapper(gym.make(\"Pendulum-v1\"))\n",
    "\n",
    "print(env.action_space.low)\n",
    "\n",
    "for _ in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f6236",
   "metadata": {},
   "source": [
    "### 使用 RL 算法进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ebe0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们将使用sb的监控包装器，它允许监控训练统计数据（平均episode奖励、平均episode长度）\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d36448",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(gym.make(\"Pendulum-v1\"))\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadddbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.38e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 2916      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0.0182    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -29.3     |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 939       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.46e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 3059      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | -0.00332  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -24.6     |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 643       |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1675ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用动作包装器\n",
    "\n",
    "normalized_env = Monitor(gym.make(\"Pendulum-v1\"))\n",
    "# Note that we can use multiple wrappers\n",
    "normalized_env = NormalizeActionWrapper(normalized_env)\n",
    "normalized_env = DummyVecEnv([lambda: normalized_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b93f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 200      |\n",
      "|    ep_rew_mean        | -1.3e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 2684     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.0177   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -27.1    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 576      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.31e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 2894      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | -0.164    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -29.6     |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 375       |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0566ce9",
   "metadata": {},
   "source": [
    "## Additional wrappers: VecEnvWrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac552697",
   "metadata": {},
   "source": [
    "与gym包装器一样，Stable Baselines为 VecEnv 提供了包装器。在现有的不同包装器中（您可以创建自己的包装器）：\n",
    "\n",
    "VecNormalize：它计算运行平均值和标准差以标准化观察并返回\n",
    "VecFrameStack：它堆叠多个连续的观察结果（有助于整合观察中的时间，例如 atari 游戏的连续帧）\n",
    "\n",
    "注意：使用 VecNormalize 包装器时，必须将运行平均值和标准差与模型一起保存，否则再次加载代理时将无法获得正确的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6f80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(\"Pendulum-v1\")])\n",
    "normalized_vec_env = VecNormalize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83634b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.233566   0.9155393  0.98969907]] [-10.]\n",
      "[[0.5814982 1.182631  0.9385529]] [-2.0202646]\n",
      "[[0.7960477 1.2037266 1.5585014]] [-1.2303505]\n",
      "[[0.8721311 1.1445833 1.5693926]] [-0.8966906]\n",
      "[[0.66916335 0.86611915 1.7716519 ]] [-0.71118915]\n",
      "[[0.47672254 0.6329985  1.4147632 ]] [-0.59599686]\n",
      "[[0.01398723 0.09806831 1.60136   ]] [-0.5148258]\n",
      "[[-0.33152267 -0.3354909   1.2133656 ]] [-0.4580722]\n",
      "[[-0.7310699 -0.8750509  1.2344685]] [-0.4118374]\n",
      "[[-0.9948084 -1.2466166  1.0051402]] [-0.37627134]\n"
     ]
    }
   ],
   "source": [
    "obs = normalized_vec_env.reset()\n",
    "for _ in range(10):\n",
    "    action = [normalized_vec_env.action_space.sample()]\n",
    "    obs, reward, _, _ = normalized_vec_env.step(action)\n",
    "    print(obs, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55426639",
   "metadata": {},
   "source": [
    "### 练习：编写自己的监视器包装器的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed10a8",
   "metadata": {},
   "source": [
    "现在我们已经知道包装器如何工作以及可以用它做什么，是时候进行实验了。\n",
    "\n",
    "这里的目标是创建一个包装器来监视训练进度，存储episode奖励（一个episode的奖励总和）和episode长度（最后一个episode的步数）。\n",
    "\n",
    "我们将在每集结束后使用信息字典返回这些值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69752520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMonitorWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super().__init__(env)\n",
    "        # === YOUR CODE HERE ===#\n",
    "        # Initialize the variables that will be used\n",
    "        # to store the episode length and episode reward\n",
    "\n",
    "        # ====================== #\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        \"\"\"\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        # === YOUR CODE HERE ===#\n",
    "        # Reset the variables\n",
    "\n",
    "        # ====================== #\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, bool, dict)\n",
    "            observation, reward, is the episode over?, is the episode truncated?, additional information\n",
    "        \"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # === YOUR CODE HERE ===#\n",
    "        # Update the current episode reward and episode length\n",
    "\n",
    "        # ====================== #\n",
    "\n",
    "        if terminated or truncated:\n",
    "            # === YOUR CODE HERE ===#\n",
    "            # Store the episode length and episode reward in the info dict\n",
    "            pass\n",
    "\n",
    "            # ====================== #\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# === YOUR CODE HERE ===#\n",
    "# Wrap the environment\n",
    "\n",
    "# Reset the environment\n",
    "\n",
    "# Take random actions in the environment and check\n",
    "# that it returns the correct values after the end of each episode\n",
    "\n",
    "# ====================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d4e01",
   "metadata": {},
   "source": [
    "### 练习结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c200280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 96 steps with reward -125.72311089361091\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class MyMonitorWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym wrapper that monitors and stores episode reward and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.episode_reward = 0\n",
    "        self.episode_length = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.episode_reward = 0\n",
    "        self.episode_length = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.episode_reward += reward\n",
    "        self.episode_length += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            info['episode'] = {'reward': self.episode_reward, 'length': self.episode_length}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# 创建环境并包装\n",
    "env = MyMonitorWrapper(gym.make(\"LunarLander-v2\"))\n",
    "\n",
    "# 重置环境以开始新的episode\n",
    "obs = env.reset()\n",
    "\n",
    "# 进行随机行动直到episode结束\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # 选择一个随机动作\n",
    "    obs, reward, done, truncated, info = env.step(action)  # 执行动作\n",
    "    if done or truncated:\n",
    "        print(f\"Episode finished after {info['episode']['length']} steps with reward {info['episode']['reward']}\")\n",
    "        obs = env.reset()  # 重置环境\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037b46d",
   "metadata": {},
   "source": [
    "这段代码首先定义了MyMonitorWrapper类，它继承自gym.Wrapper。这个包装器在每个episode结束时（无论是正常结束还是被截断），都会在info字典中返回累积的奖励(episode_reward)和步数(episode_length)。然后，我们创建了一个LunarLander-v2环境实例，并将其包装在MyMonitorWrapper中。通过随机选择动作并执行，我们演示了包装器如何在每个episode结束时通过打印来报告总奖励和步数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9dd1ec",
   "metadata": {},
   "source": [
    "## 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd4a26",
   "metadata": {},
   "source": [
    "如何轻松保存和加载模型\n",
    "\n",
    "什么是包装器以及我们可以用它做什么\n",
    "\n",
    "如何创建自己的包装器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46765e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
