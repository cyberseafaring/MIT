{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d513cbb4",
   "metadata": {},
   "source": [
    "学习如何使用回调来进行训练过程监控、自动保存、模型操作、进度条等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bc35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b642c",
   "metadata": {},
   "source": [
    "## 超参数调整的重要性\n",
    "\n",
    "与监督学习相比，深度强化学习对超参数的选择要敏感得多，例如学习率、神经元数量、层数、优化器……等。超参数选择不当会导致性能不佳/不稳定收敛。这一挑战因随机种子（用于初始化网络权重和环境）的性能变化而变得更加复杂。\n",
    "\n",
    "在这里，我们通过一个玩具示例演示了在 Pendulum 环境中应用的 Soft Actor Critic （SAC）算法。请注意默认参数和“调整”参数之间的性能变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2166e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3746c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a6b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.38e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 454       |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.7      |\n",
      "|    critic_loss     | 1.46      |\n",
      "|    ent_coef        | 0.811     |\n",
      "|    ent_coef_loss   | -0.346    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.45e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 435       |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 43.3      |\n",
      "|    critic_loss     | 0.96      |\n",
      "|    ent_coef        | 0.645     |\n",
      "|    ent_coef_loss   | -0.661    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.44e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 420       |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.7      |\n",
      "|    critic_loss     | 0.743     |\n",
      "|    ent_coef        | 0.52      |\n",
      "|    ent_coef_loss   | -0.805    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 416       |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 77.5      |\n",
      "|    critic_loss     | 2.86      |\n",
      "|    ent_coef        | 0.429     |\n",
      "|    ent_coef_loss   | -0.581    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.34e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 416       |\n",
      "|    time_elapsed    | 9         |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 104       |\n",
      "|    critic_loss     | 4.05      |\n",
      "|    ent_coef        | 0.365     |\n",
      "|    ent_coef_loss   | -0.752    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.34e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 416       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 113       |\n",
      "|    critic_loss     | 3.59      |\n",
      "|    ent_coef        | 0.309     |\n",
      "|    ent_coef_loss   | -0.735    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4699      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 416       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 133       |\n",
      "|    critic_loss     | 2.53      |\n",
      "|    ent_coef        | 0.25      |\n",
      "|    ent_coef_loss   | -1.09     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 416       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 139       |\n",
      "|    critic_loss     | 2.49      |\n",
      "|    ent_coef        | 0.201     |\n",
      "|    ent_coef_loss   | -0.534    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.15e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 415       |\n",
      "|    time_elapsed    | 17        |\n",
      "|    total_timesteps | 7200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 162       |\n",
      "|    critic_loss     | 3.25      |\n",
      "|    ent_coef        | 0.183     |\n",
      "|    ent_coef_loss   | 0.313     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 415       |\n",
      "|    time_elapsed    | 19        |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 165       |\n",
      "|    critic_loss     | 2.45      |\n",
      "|    ent_coef        | 0.19      |\n",
      "|    ent_coef_loss   | -0.179    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "default_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    verbose=1,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    ").learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daff4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zrl_mini/anaconda3/envs/torch2/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-310.80 +/- 112.18\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(default_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f042f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.38e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 179       |\n",
      "|    time_elapsed    | 4         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 22.5      |\n",
      "|    critic_loss     | 0.273     |\n",
      "|    ent_coef        | 0.812     |\n",
      "|    ent_coef_loss   | -0.341    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.39e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 159       |\n",
      "|    time_elapsed    | 10        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 45.8      |\n",
      "|    critic_loss     | 0.164     |\n",
      "|    ent_coef        | 0.649     |\n",
      "|    ent_coef_loss   | -0.575    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.31e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 159       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 62.2      |\n",
      "|    critic_loss     | 0.251     |\n",
      "|    ent_coef        | 0.535     |\n",
      "|    ent_coef_loss   | -0.6      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.19e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 20        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 75.5      |\n",
      "|    critic_loss     | 0.474     |\n",
      "|    ent_coef        | 0.455     |\n",
      "|    ent_coef_loss   | -0.425    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -995     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 75.7     |\n",
      "|    critic_loss     | 0.504    |\n",
      "|    ent_coef        | 0.393    |\n",
      "|    ent_coef_loss   | -0.622   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -845     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 78.3     |\n",
      "|    critic_loss     | 0.846    |\n",
      "|    ent_coef        | 0.334    |\n",
      "|    ent_coef_loss   | -0.686   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -751     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.7     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.274    |\n",
      "|    ent_coef_loss   | -0.794   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -673     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 78.3     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -0.667   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -612     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.3     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -0.894   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -560     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.3     |\n",
      "|    critic_loss     | 0.984    |\n",
      "|    ent_coef        | 0.153    |\n",
      "|    ent_coef_loss   | -0.574   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "tuned_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    seed=0,\n",
    ").learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7d0072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-139.84 +/- 99.76\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(tuned_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1c165",
   "metadata": {},
   "source": [
    "## 回调函数\n",
    "\n",
    "尽管 Stable-Baselines3 为我们提供了回调集合（例如，用于创建检查点或用于评估），但我们将重新实现一些回调集合，以便可以很好地理解它们的工作原理。\n",
    "\n",
    "要构建自定义回调，我们需要创建一个派生自 BaseCallback 的类。这将使我们能够访问事件（_on_training_start、_on_step()）和有用的变量（例如 RL 模型的 self.model）。\n",
    "\n",
    "_on_step 返回一个布尔值，表示训练是否应该继续。\n",
    "\n",
    "由于可以访问模型变量，特别是 self.model，我们甚至可以更改模型的参数，而无需停止训练或更改模型的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19751a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aef77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    a simple callback that can only be called twice\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super(SimpleCallback, self).__init__(verbose)\n",
    "        self._called = False\n",
    "\n",
    "    def _on_step(self):\n",
    "        if not self._called:\n",
    "            print(\"callback - first call\")\n",
    "            self._called = True\n",
    "            return True  # returns True, training continues.\n",
    "        print(\"callback - second call\")\n",
    "        return False  # returns False, training stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26882e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "callback - first call\n",
      "callback - second call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x169b5cb80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n",
    "model.learn(8000, callback=SimpleCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16a95c",
   "metadata": {},
   "source": [
    "## 第一个示例：自动保存最佳模型\n",
    "\n",
    "在强化学习中，在训练时保留模型的干净版本非常有用，因为我们最终可能会老化错误的策略。这是回调的典型用例，因为他们可以调用模型的保存函数，并观察一段时间内的训练情况。\n",
    "\n",
    "使用监控包装器，我们可以保存环境的统计数据，并使用它们来确定平均训练奖励。这使我们能够在训练时保存最佳模型。\n",
    "\n",
    "请注意，这不是评估 RL 代理的正确方法，我们应该创建一个测试环境并在回调中评估代理性能（参见 EvalCallback）。为简单起见，我们将使用训练奖励作为代理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c374d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe34bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2246817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq, log_dir, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\n",
    "                        \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                            self.best_mean_reward, mean_reward\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
    "                        print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30d106e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 40\n",
      "Best mean reward: -inf - Last mean reward per episode: 20.00\n",
      "Saving new best model at 40 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 60\n",
      "Best mean reward: 20.00 - Last mean reward per episode: 20.00\n",
      "Num timesteps: 80\n",
      "Best mean reward: 20.00 - Last mean reward per episode: 20.00\n",
      "Num timesteps: 100\n",
      "Best mean reward: 20.00 - Last mean reward per episode: 20.00\n",
      "Num timesteps: 120\n",
      "Best mean reward: 20.00 - Last mean reward per episode: 20.00\n",
      "Num timesteps: 140\n",
      "Best mean reward: 20.00 - Last mean reward per episode: 44.00\n",
      "Saving new best model at 132 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 160\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 37.75\n",
      "Num timesteps: 180\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.40\n",
      "Num timesteps: 200\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.40\n",
      "Num timesteps: 220\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 35.33\n",
      "Num timesteps: 240\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.29\n",
      "Num timesteps: 260\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.29\n",
      "Num timesteps: 280\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 33.12\n",
      "Num timesteps: 300\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.56\n",
      "Num timesteps: 320\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.70\n",
      "Num timesteps: 340\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.70\n",
      "Num timesteps: 360\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.27\n",
      "Num timesteps: 380\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.27\n",
      "Num timesteps: 400\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.08\n",
      "Num timesteps: 420\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.15\n",
      "Num timesteps: 440\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.15\n",
      "Num timesteps: 460\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.93\n",
      "Num timesteps: 480\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.93\n",
      "Num timesteps: 500\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.93\n",
      "Num timesteps: 520\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.56\n",
      "Num timesteps: 540\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.56\n",
      "Num timesteps: 560\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.76\n",
      "Num timesteps: 580\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.22\n",
      "Num timesteps: 600\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.22\n",
      "Num timesteps: 620\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.22\n",
      "Num timesteps: 640\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 33.16\n",
      "Num timesteps: 660\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.05\n",
      "Num timesteps: 680\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.29\n",
      "Num timesteps: 700\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.29\n",
      "Num timesteps: 720\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 32.23\n",
      "Num timesteps: 740\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 31.35\n",
      "Num timesteps: 760\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.88\n",
      "Num timesteps: 780\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.44\n",
      "Num timesteps: 800\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.42\n",
      "Num timesteps: 820\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.74\n",
      "Num timesteps: 840\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.82\n",
      "Num timesteps: 860\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.82\n",
      "Num timesteps: 880\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.72\n",
      "Num timesteps: 900\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.57\n",
      "Num timesteps: 920\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.29\n",
      "Num timesteps: 940\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.12\n",
      "Num timesteps: 960\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 28.97\n",
      "Num timesteps: 980\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 28.47\n",
      "Num timesteps: 1000\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 28.57\n",
      "Num timesteps: 1020\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 28.22\n",
      "Num timesteps: 1040\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 28.00\n",
      "Num timesteps: 1060\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 27.82\n",
      "Num timesteps: 1080\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 27.69\n",
      "Num timesteps: 1100\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 27.45\n",
      "Num timesteps: 1120\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 27.45\n",
      "Num timesteps: 1140\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 27.41\n",
      "Num timesteps: 1160\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 27.36\n",
      "Num timesteps: 1180\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.80\n",
      "Num timesteps: 1200\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.60\n",
      "Num timesteps: 1220\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.46\n",
      "Num timesteps: 1240\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.21\n",
      "Num timesteps: 1260\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.92\n",
      "Num timesteps: 1280\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.56\n",
      "Num timesteps: 1300\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.56\n",
      "Num timesteps: 1320\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.80\n",
      "Num timesteps: 1340\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.58\n",
      "Num timesteps: 1360\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.58\n",
      "Num timesteps: 1380\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.68\n",
      "Num timesteps: 1400\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.45\n",
      "Num timesteps: 1420\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.18\n",
      "Num timesteps: 1440\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.02\n",
      "Num timesteps: 1460\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 25.07\n",
      "Num timesteps: 1480\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.88\n",
      "Num timesteps: 1500\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.54\n",
      "Num timesteps: 1520\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.35\n",
      "Num timesteps: 1540\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.06\n",
      "Num timesteps: 1560\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.89\n",
      "Num timesteps: 1580\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.85\n",
      "Num timesteps: 1600\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.85\n",
      "Num timesteps: 1620\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.12\n",
      "Num timesteps: 1640\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.04\n",
      "Num timesteps: 1660\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.97\n",
      "Num timesteps: 1680\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.97\n",
      "Num timesteps: 1700\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.89\n",
      "Num timesteps: 1720\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.79\n",
      "Num timesteps: 1740\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.43\n",
      "Num timesteps: 1760\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.37\n",
      "Num timesteps: 1780\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.24\n",
      "Num timesteps: 1800\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.08\n",
      "Num timesteps: 1820\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.92\n",
      "Num timesteps: 1840\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.67\n",
      "Num timesteps: 1860\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.60\n",
      "Num timesteps: 1880\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.37\n",
      "Num timesteps: 1900\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.24\n",
      "Num timesteps: 1920\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.16\n",
      "Num timesteps: 1940\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 1960\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.92\n",
      "Num timesteps: 1980\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.73\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.48\n",
      "Num timesteps: 2020\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.39\n",
      "Num timesteps: 2040\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.40\n",
      "Num timesteps: 2060\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.42\n",
      "Num timesteps: 2080\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.39\n",
      "Num timesteps: 2100\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.35\n",
      "Num timesteps: 2120\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.31\n",
      "Num timesteps: 2140\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.23\n",
      "Num timesteps: 2160\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.15\n",
      "Num timesteps: 2180\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 21.30\n",
      "Num timesteps: 2200\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.52\n",
      "Num timesteps: 2220\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.66\n",
      "Num timesteps: 2240\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.54\n",
      "Num timesteps: 2260\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.33\n",
      "Num timesteps: 2280\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.09\n",
      "Num timesteps: 2300\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.09\n",
      "Num timesteps: 2320\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.76\n",
      "Num timesteps: 2340\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.76\n",
      "Num timesteps: 2360\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.37\n",
      "Num timesteps: 2380\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.25\n",
      "Num timesteps: 2400\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.90\n",
      "Num timesteps: 2420\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2440\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.74\n",
      "Num timesteps: 2460\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.28\n",
      "Num timesteps: 2480\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.30\n",
      "Num timesteps: 2500\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.17\n",
      "Num timesteps: 2520\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.97\n",
      "Num timesteps: 2540\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.04\n",
      "Num timesteps: 2560\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.94\n",
      "Num timesteps: 2580\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.80\n",
      "Num timesteps: 2600\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.86\n",
      "Num timesteps: 2620\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.81\n",
      "Num timesteps: 2640\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.73\n",
      "Num timesteps: 2660\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.63\n",
      "Num timesteps: 2680\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.71\n",
      "Num timesteps: 2700\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.71\n",
      "Num timesteps: 2720\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.73\n",
      "Num timesteps: 2740\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.66\n",
      "Num timesteps: 2760\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.85\n",
      "Num timesteps: 2780\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.70\n",
      "Num timesteps: 2800\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.79\n",
      "Num timesteps: 2820\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.81\n",
      "Num timesteps: 2840\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.81\n",
      "Num timesteps: 2860\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.78\n",
      "Num timesteps: 2880\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.78\n",
      "Num timesteps: 2900\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.91\n",
      "Num timesteps: 2920\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 17.91\n",
      "Num timesteps: 2940\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.03\n",
      "Num timesteps: 2960\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.03\n",
      "Num timesteps: 2980\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.18\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.30\n",
      "Num timesteps: 3020\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.36\n",
      "Num timesteps: 3040\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.36\n",
      "Num timesteps: 3060\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.46\n",
      "Num timesteps: 3080\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.53\n",
      "Num timesteps: 3100\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.53\n",
      "Num timesteps: 3120\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.76\n",
      "Num timesteps: 3140\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.91\n",
      "Num timesteps: 3160\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 18.91\n",
      "Num timesteps: 3180\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.06\n",
      "Num timesteps: 3200\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.06\n",
      "Num timesteps: 3220\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.06\n",
      "Num timesteps: 3240\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.54\n",
      "Num timesteps: 3260\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.54\n",
      "Num timesteps: 3280\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.54\n",
      "Num timesteps: 3300\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.54\n",
      "Num timesteps: 3320\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.86\n",
      "Num timesteps: 3340\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.86\n",
      "Num timesteps: 3360\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 19.86\n",
      "Num timesteps: 3380\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3400\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3420\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3440\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3460\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3480\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3500\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3520\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3540\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3560\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 20.38\n",
      "Num timesteps: 3580\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3600\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3620\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3640\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3660\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3680\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3700\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3720\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3740\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 22.03\n",
      "Num timesteps: 3760\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.70\n",
      "Num timesteps: 3780\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.70\n",
      "Num timesteps: 3800\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.70\n",
      "Num timesteps: 3820\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.70\n",
      "Num timesteps: 3840\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.70\n",
      "Num timesteps: 3860\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 23.70\n",
      "Num timesteps: 3880\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 3900\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 3920\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 3940\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 3960\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 3980\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 4020\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 4040\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 4060\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 4080\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 24.69\n",
      "Num timesteps: 4100\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4120\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4140\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4160\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4180\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4200\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4220\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4240\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4260\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4280\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4300\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4320\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4340\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4360\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 26.86\n",
      "Num timesteps: 4380\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4400\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4420\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4440\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4460\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4480\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4500\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4520\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 29.36\n",
      "Num timesteps: 4540\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4560\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4580\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4600\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4620\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4640\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4660\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4680\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4700\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4720\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4740\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4760\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4780\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4800\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4820\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4840\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4860\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4880\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4900\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4920\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 30.77\n",
      "Num timesteps: 4940\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.66\n",
      "Num timesteps: 4960\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.66\n",
      "Num timesteps: 4980\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.66\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 44.00 - Last mean reward per episode: 34.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x169b5f250>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1, monitor_dir=log_dir)\n",
    "# it is equivalent to:\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = Monitor(env, log_dir)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=20, log_dir=log_dir, verbose=1)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=5000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d0106",
   "metadata": {},
   "source": [
    "## 第二个示例：实时绘制性能图\n",
    "在训练时，有时相对于episodic奖励，训练如何随着时间的推移而进展是有用的。为此，Stable-Baselines 有 Tensorboard 支持，但这可能非常麻烦，尤其是在磁盘空间使用方面。\n",
    "\n",
    "注意：不幸的是，实时绘图在 google colab 上无法开箱即用\n",
    "\n",
    "在这里，我们可以再次使用回调，使用监控包装器来实时绘制情景奖励："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6141aede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_device_pixel_ratio', {\n",
       "                device_pixel_ratio: fig.ratio,\n",
       "            });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute('tabindex', '0');\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;' +\n",
       "            'z-index: 2;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box;' +\n",
       "            'pointer-events: none;' +\n",
       "            'position: relative;' +\n",
       "            'z-index: 0;'\n",
       "    );\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box;' +\n",
       "            'left: 0;' +\n",
       "            'pointer-events: none;' +\n",
       "            'position: absolute;' +\n",
       "            'top: 0;' +\n",
       "            'z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            /* This rescales the canvas back to display pixels, so that it\n",
       "             * appears correct on HiDPI screens. */\n",
       "            canvas.style.width = width + 'px';\n",
       "            canvas.style.height = height + 'px';\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        /* User Agent sniffing is bad, but WebKit is busted:\n",
       "         * https://bugs.webkit.org/show_bug.cgi?id=144526\n",
       "         * https://bugs.webkit.org/show_bug.cgi?id=181818\n",
       "         * The worst that happens here is that they get an extra browser\n",
       "         * selection when dragging, if this check fails to catch them.\n",
       "         */\n",
       "        var UA = navigator.userAgent;\n",
       "        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n",
       "        if(isWebKit) {\n",
       "            return function (event) {\n",
       "                /* This prevents the web browser from automatically changing to\n",
       "                 * the text insertion cursor when the button is pressed. We\n",
       "                 * want to control all of the cursor setting manually through\n",
       "                 * the 'cursor' event from matplotlib */\n",
       "                event.preventDefault()\n",
       "                return fig.mouse_event(event, name);\n",
       "            };\n",
       "        } else {\n",
       "            return function (event) {\n",
       "                return fig.mouse_event(event, name);\n",
       "            };\n",
       "        }\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'dblclick',\n",
       "        on_mouse_event_closure('dblclick')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    canvas_div.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    canvas_div.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    fig.canvas_div.style.cursor = msg['cursor'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            var img = evt.data;\n",
       "            if (img.type !== 'image/png') {\n",
       "                /* FIXME: We get \"Resource interpreted as Image but\n",
       "                 * transferred with MIME type text/plain:\" errors on\n",
       "                 * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "                 * to be part of the websocket stream */\n",
       "                img.type = 'image/png';\n",
       "            }\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                img\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "function getModifiers(event) {\n",
       "    var mods = [];\n",
       "    if (event.ctrlKey) {\n",
       "        mods.push('ctrl');\n",
       "    }\n",
       "    if (event.altKey) {\n",
       "        mods.push('alt');\n",
       "    }\n",
       "    if (event.shiftKey) {\n",
       "        mods.push('shift');\n",
       "    }\n",
       "    if (event.metaKey) {\n",
       "        mods.push('meta');\n",
       "    }\n",
       "    return mods;\n",
       "}\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * https://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    // from https://stackoverflow.com/q/1114465\n",
       "    var boundingRect = this.canvas.getBoundingClientRect();\n",
       "    var x = (event.clientX - boundingRect.left) * this.ratio;\n",
       "    var y = (event.clientY - boundingRect.top) * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        modifiers: getModifiers(event),\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.key === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.key;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.key !== 'Control') {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    else if (event.altKey && event.key !== 'Alt') {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    else if (event.shiftKey && event.key !== 'Shift') {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k' + event.key;\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.binaryType = comm.kernel.ws.binaryType;\n",
       "    ws.readyState = comm.kernel.ws.readyState;\n",
       "    function updateReadyState(_event) {\n",
       "        if (comm.kernel.ws) {\n",
       "            ws.readyState = comm.kernel.ws.readyState;\n",
       "        } else {\n",
       "            ws.readyState = 3; // Closed state.\n",
       "        }\n",
       "    }\n",
       "    comm.kernel.ws.addEventListener('open', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('close', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('error', updateReadyState);\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        var data = msg['content']['data'];\n",
       "        if (data['blob'] !== undefined) {\n",
       "            data = {\n",
       "                data: new Blob(msg['buffers'], { type: data['blob'] }),\n",
       "            };\n",
       "        }\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(data);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x169b5e860>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "class PlottingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for plotting the performance in realtime.\n",
    "\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self._plot = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # get the monitor's data\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if self._plot is None: # make the plot\n",
    "            plt.ion()\n",
    "            fig = plt.figure(figsize=(6,3))\n",
    "            ax = fig.add_subplot(111)\n",
    "            line, = ax.plot(x, y)\n",
    "            self._plot = (line, ax, fig)\n",
    "            plt.show()\n",
    "        else: # update and rescale the plot\n",
    "            self._plot[0].set_data(x, y)\n",
    "            self._plot[-2].relim()\n",
    "            self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
    "                                    self.locals[\"total_timesteps\"] * 1.02])\n",
    "            self._plot[-2].autoscale_view(True,True,True)\n",
    "            self._plot[-1].canvas.draw()\n",
    "        \n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('MountainCarContinuous-v0', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "plotting_callback = PlottingCallback()\n",
    "        \n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(10000, callback=plotting_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fe223",
   "metadata": {},
   "source": [
    "## 第三个例子：进度条\n",
    "\n",
    "开发和使用强化学习时，生活质量的改善总是受到欢迎。在这里，我们使用 tqdm 显示训练的进度条，以及每秒的时间步数和训练结束的估计剩余时间：\n",
    "\n",
    "请注意，此回调已包含在 SB3 中，可以通过将 Progress_bar=True 传递给 learn() 方法来使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c09bcbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d13043da214b7f9d073f50c3d3388c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pbar):\n",
    "        super().__init__()\n",
    "        self._pbar = pbar\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        self._pbar.n = self.num_timesteps\n",
    "        self._pbar.update(0)\n",
    "\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_timesteps):  # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "\n",
    "    def __enter__(self):  # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "\n",
    "        return ProgressBarCallback(self.pbar)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "model = TD3(\"MlpPolicy\", \"Pendulum-v1\", verbose=0)\n",
    "# Using a context manager garanties that the tqdm progress bar closes correctly\n",
    "with ProgressBarManager(2000) as callback:\n",
    "    model.learn(2000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a559b",
   "metadata": {},
   "source": [
    "## 第四个例子：组合\n",
    "\n",
    "由于回调的功能性质，可以将回调组合成单个回调。这意味着我们可以自动保存最好的模型，显示训练的进度条和情景奖励。\n",
    "\n",
    "当将列表传递给 learn() 方法时，回调会自动组成。在底层，创建了一个 CallbackList。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0205ef49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c31ea12bd64736bb9c00cf632305e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('CartPole-v1', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "# Create callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "with ProgressBarManager(1000) as progress_callback:\n",
    "  # This is equivalent to callback=CallbackList([progress_callback, auto_save_callback])\n",
    "  model.learn(1000, callback=[progress_callback, auto_save_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73896e85",
   "metadata": {},
   "source": [
    "## 练习：编写自己的回调代码\n",
    "\n",
    "前面的示例展示了什么是回调以及如何使用它的基础知识。\n",
    "\n",
    "本练习的目标是创建一个回调，该回调将使用测试环境评估模型，如果这是最知名的模型，则保存它。\n",
    "\n",
    "为了让事情变得更简单，我们将使用类而不是带有魔术方法 __call__ 的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eda84093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Best mean reward: 8.60 - Last mean reward per episode: 8.60\n",
      "Best mean reward: 8.60 - Last mean reward per episode: 8.60\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 7947     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Best mean reward: 104.80 - Last mean reward per episode: 104.80\n",
      "Best mean reward: 104.80 - Last mean reward per episode: 74.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.3        |\n",
      "|    ep_rew_mean          | 26.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4551        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008413288 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.000405   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.08        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 53.3        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 108.40 - Last mean reward per episode: 108.40\n",
      "Best mean reward: 130.60 - Last mean reward per episode: 130.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.8        |\n",
      "|    ep_rew_mean          | 36.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3940        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009142556 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 35.4        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 237.00 - Last mean reward per episode: 237.00\n",
      "Best mean reward: 313.20 - Last mean reward per episode: 313.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 45.7        |\n",
      "|    ep_rew_mean          | 45.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3511        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009695422 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 59.9        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 314.80 - Last mean reward per episode: 314.80\n",
      "Best mean reward: 314.80 - Last mean reward per episode: 291.40\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 61.6        |\n",
      "|    ep_rew_mean          | 61.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3241        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007985068 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 67.7        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 427.20 - Last mean reward per episode: 427.20\n",
      "Best mean reward: 427.20 - Last mean reward per episode: 349.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 79.2        |\n",
      "|    ep_rew_mean          | 79.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3058        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009139341 |\n",
      "|    clip_fraction        | 0.0829      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 54          |\n",
      "-----------------------------------------\n",
      "Best mean reward: 488.40 - Last mean reward per episode: 488.40\n",
      "Best mean reward: 488.40 - Last mean reward per episode: 484.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95           |\n",
      "|    ep_rew_mean          | 95           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2878         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065671876 |\n",
      "|    clip_fraction        | 0.0703       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.6         |\n",
      "|    explained_variance   | 0.68         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.64         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.011       |\n",
      "|    value_loss           | 44.3         |\n",
      "------------------------------------------\n",
      "Best mean reward: 488.40 - Last mean reward per episode: 408.00\n",
      "Best mean reward: 488.40 - Last mean reward per episode: 440.40\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | 108         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2799        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006914785 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.4        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 82.4        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 488.40 - Last mean reward per episode: 301.20\n",
      "Best mean reward: 488.40 - Last mean reward per episode: 273.80\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 125          |\n",
      "|    ep_rew_mean          | 125          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2779         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032831184 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.557       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.6         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00623     |\n",
      "|    value_loss           | 69.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 143         |\n",
      "|    ep_rew_mean          | 143         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2677        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012323601 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.51        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 24.2        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 161         |\n",
      "|    ep_rew_mean          | 161         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2621        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007678435 |\n",
      "|    clip_fraction        | 0.078       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.11        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0093     |\n",
      "|    value_loss           | 65.3        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | 179         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2579        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006311932 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.407       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 9.73        |\n",
      "-----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 197          |\n",
      "|    ep_rew_mean          | 197          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2546         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024749073 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.501       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.191        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 7.37e-05     |\n",
      "|    value_loss           | 5.43         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 216          |\n",
      "|    ep_rew_mean          | 216          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2512         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036638211 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.501       |\n",
      "|    explained_variance   | -0.0314      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.142        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000253    |\n",
      "|    value_loss           | 3.64         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 234          |\n",
      "|    ep_rew_mean          | 234          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2473         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021192995 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.187        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.000542     |\n",
      "|    value_loss           | 2.44         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 250          |\n",
      "|    ep_rew_mean          | 250          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2447         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040897196 |\n",
      "|    clip_fraction        | 0.0194       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.487       |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.127        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    value_loss           | 1.44         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 478.60\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 269        |\n",
      "|    ep_rew_mean          | 269        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2430       |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00618798 |\n",
      "|    clip_fraction        | 0.0422     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.477     |\n",
      "|    explained_variance   | 0.426      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0604     |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.00374   |\n",
      "|    value_loss           | 0.951      |\n",
      "----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 457.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 285          |\n",
      "|    ep_rew_mean          | 285          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2418         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011644698 |\n",
      "|    clip_fraction        | 0.00498      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.258        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.28         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | 0.000535     |\n",
      "|    value_loss           | 50.7         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 462.60\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 497.60\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 299          |\n",
      "|    ep_rew_mean          | 299          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2403         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021312698 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 70.1         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    value_loss           | 71.7         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 405.80\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 418.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 315           |\n",
      "|    ep_rew_mean          | 315           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2401          |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 17            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040293537 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.484        |\n",
      "|    explained_variance   | 0.753         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.65          |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000901     |\n",
      "|    value_loss           | 52.7          |\n",
      "-------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 393.20\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 481.80\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 392.40\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 330          |\n",
      "|    ep_rew_mean          | 330          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2377         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025941113 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.475       |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.206        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00511     |\n",
      "|    value_loss           | 4.63         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 457.20\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 441.80\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 347          |\n",
      "|    ep_rew_mean          | 347          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2373         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022107135 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.162        |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0037      |\n",
      "|    value_loss           | 2.46         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 357          |\n",
      "|    ep_rew_mean          | 357          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2365         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037008692 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.79         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00343     |\n",
      "|    value_loss           | 13.4         |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 369        |\n",
      "|    ep_rew_mean          | 369        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2358       |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00652156 |\n",
      "|    clip_fraction        | 0.0549     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.449     |\n",
      "|    explained_variance   | 0.793      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0967     |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00569   |\n",
      "|    value_loss           | 0.314      |\n",
      "----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 381         |\n",
      "|    ep_rew_mean          | 381         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2352        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004480499 |\n",
      "|    clip_fraction        | 0.0615      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0155      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    value_loss           | 0.0369      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 396         |\n",
      "|    ep_rew_mean          | 396         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2344        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005444841 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00812     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    value_loss           | 0.0236      |\n",
      "-----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 410          |\n",
      "|    ep_rew_mean          | 410          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2332         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030377714 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.154        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00222      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000994    |\n",
      "|    value_loss           | 0.0147       |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 422          |\n",
      "|    ep_rew_mean          | 422          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2328         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034797834 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.406       |\n",
      "|    explained_variance   | 0.0555       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000762    |\n",
      "|    value_loss           | 0.00991      |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 433          |\n",
      "|    ep_rew_mean          | 433          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2321         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042335857 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 0.0175       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000651    |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 0.00651      |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 444           |\n",
      "|    ep_rew_mean          | 444           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2313          |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096023217 |\n",
      "|    clip_fraction        | 0.0112        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.415        |\n",
      "|    explained_variance   | -0.0376       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000526      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | 0.000183      |\n",
      "|    value_loss           | 0.00435       |\n",
      "-------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 454         |\n",
      "|    ep_rew_mean          | 454         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2309        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003802226 |\n",
      "|    clip_fraction        | 0.031       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.0549      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00163    |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 464          |\n",
      "|    ep_rew_mean          | 464          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2303         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048003094 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.384       |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00118     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000403    |\n",
      "|    value_loss           | 0.00203      |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 472         |\n",
      "|    ep_rew_mean          | 472         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2300        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002650015 |\n",
      "|    clip_fraction        | 0.0167      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.0726      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00172    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.000661   |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 472          |\n",
      "|    ep_rew_mean          | 472          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2297         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071001803 |\n",
      "|    clip_fraction        | 0.0689       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.383       |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0169      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00322     |\n",
      "|    value_loss           | 0.00107      |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 472         |\n",
      "|    ep_rew_mean          | 472         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2291        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004685711 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.369      |\n",
      "|    explained_variance   | -0.00884    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000139   |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    value_loss           | 0.000696    |\n",
      "-----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 472          |\n",
      "|    ep_rew_mean          | 472          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2282         |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026236433 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | -0.0576      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00516      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 0.000499     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 472          |\n",
      "|    ep_rew_mean          | 472          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2277         |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016048349 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | 0.0888       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0143       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000787    |\n",
      "|    value_loss           | 0.000359     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 472           |\n",
      "|    ep_rew_mean          | 472           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2274          |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071270304 |\n",
      "|    clip_fraction        | 0.0268        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.316        |\n",
      "|    explained_variance   | 0.248         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.0111       |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.000125     |\n",
      "|    value_loss           | 0.000232      |\n",
      "-------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 472          |\n",
      "|    ep_rew_mean          | 472          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2268         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021675252 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | 0.353        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000561    |\n",
      "|    value_loss           | 0.000161     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 477          |\n",
      "|    ep_rew_mean          | 477          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2266         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032603648 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | 0.264        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00651      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 0.000118     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 483         |\n",
      "|    ep_rew_mean          | 483         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2265        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001634245 |\n",
      "|    clip_fraction        | 0.0172      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00715     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.000358   |\n",
      "|    value_loss           | 7.46e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 490          |\n",
      "|    ep_rew_mean          | 490          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2250         |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007618759 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.287       |\n",
      "|    explained_variance   | 0.281        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00543     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    value_loss           | 5.55e-05     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 497          |\n",
      "|    ep_rew_mean          | 497          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2249         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014660372 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.315        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00864     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 4.72e-05     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 498          |\n",
      "|    ep_rew_mean          | 498          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2245         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038855926 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.293       |\n",
      "|    explained_variance   | 0.36         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00212      |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 3.39e-05     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2243         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038900063 |\n",
      "|    clip_fraction        | 0.0595       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.271       |\n",
      "|    explained_variance   | 0.237        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00301     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 2.44e-05     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 500        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2243       |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00109106 |\n",
      "|    clip_fraction        | 0.0156     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.281     |\n",
      "|    explained_variance   | 0.533      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00196   |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.000973  |\n",
      "|    value_loss           | 1.73e-05   |\n",
      "----------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2240         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012338621 |\n",
      "|    clip_fraction        | 0.00942      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | 0.21         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00103      |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000968    |\n",
      "|    value_loss           | 1.68e-05     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2239         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071967565 |\n",
      "|    clip_fraction        | 0.0927       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.285       |\n",
      "|    explained_variance   | 0.00267      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0307      |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00418     |\n",
      "|    value_loss           | 9.29e-06     |\n",
      "------------------------------------------\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "Best mean reward: 500.00 - Last mean reward per episode: 500.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2237         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019081244 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.279       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00103     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000273    |\n",
      "|    value_loss           | 6.44e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2a4ac5270>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class EvalCallback(BaseCallback):\n",
    "    def __init__(self, eval_env, n_eval_episodes=5, eval_freq=20, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            total_rewards = []\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                obs = self.eval_env.reset()\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    action, _states = self.model.predict(obs, deterministic=True)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    total_reward += reward\n",
    "                total_rewards.append(total_reward)\n",
    "            mean_reward = np.mean(total_rewards)\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.model.save(\"best_model\")\n",
    "            if self.verbose > 0:\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "        return True\n",
    "\n",
    "# 创建训练和评估环境\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "'''\n",
    "使用DummyVecEnv将评估环境包装起来，这样就可以确保观测值的格式与模型期望的一致。这通常是处理单个环境时推荐的方法。\n",
    "'''\n",
    "\n",
    "# 包装评估环境\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "eval_env = DummyVecEnv([lambda: eval_env])  # 注意这里的包装方式\n",
    "\n",
    "# 创建回调对象\n",
    "callback = EvalCallback(eval_env, n_eval_episodes=5, eval_freq=1000)\n",
    "\n",
    "# 创建RL模型，这里使用PPO算法\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 训练RL模型\n",
    "model.learn(int(10000), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bf058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f449eeb",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "好的超参数是 RL 成功的关键，你不应该只使用默认的超参数来解决所有问题\n",
    "\n",
    "什么是回调以及你可以用它做什么\n",
    "\n",
    "如何创建自己的回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23435c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
