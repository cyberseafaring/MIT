{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89389646",
   "metadata": {},
   "source": [
    "PyTorch是一个广泛使用的开源机器学习库，尤其在深度学习和人工智能领域中非常流行。它提供了灵活性和高速的计算能力，主要用于应用程序如计算机视觉、自然语言处理等。在PyTorch中，有几个核心的类，这些类构成了库的基础。下面是一些常用的PyTorch类及其简要用途说明：\n",
    "\n",
    "1. **`torch.Tensor`**：这是PyTorch的核心类，用于表示数据的多维矩阵。它是库中所有计算的基础，可以用来存储和操作数据，例如模型的输入数据、模型的参数等。`Tensor`类似于NumPy的ndarray，但它还可以在GPU上进行计算以加速运算。\n",
    "\n",
    "2. **`torch.nn.Module`**：这是所有神经网络模块的基类。您可以通过继承`torch.nn.Module`来定义自己的网络层或模型。这个类提供了模型训练、参数管理等功能。它允许您组合多个层（如卷积层、池化层等）来构建复杂的网络结构。\n",
    "\n",
    "3. **`torch.nn`**：这个命名空间下包含了构建神经网络所需的所有构建块，比如网络层（`Linear`、`Conv2d`、`LSTM`等）、激活函数（`ReLU`、`Sigmoid`等）、损失函数（`MSELoss`、`CrossEntropyLoss`等）。这些都是预定义的类，可以直接在模型中使用。\n",
    "\n",
    "4. **`torch.optim`**：这个模块提供了各种优化算法来更新网络的权重和参数，例如SGD、Adam、RMSprop等。优化器是用于模型训练过程中的参数优化。\n",
    "\n",
    "5. **`torch.utils.data.Dataset`**：这是一个表示数据集的抽象类。您可以通过继承这个类来创建自己的数据集。它为数据加载提供了一个统一的接口，使得数据预处理和加载更加方便。\n",
    "\n",
    "6. **`torch.utils.data.DataLoader`**：这个类配合`Dataset`使用，用于包装任何类型的`Dataset`对象，它提供了对数据的批处理、排序、打乱等操作，是模型训练时数据加载的重要工具。\n",
    "\n",
    "7. **`torch.autograd`**：这是PyTorch中实现自动微分功能的模块。它提供了`Tensor`上的自动求导操作，非常关键于神经网络的训练过程中。通过`autograd`，模型的反向传播计算可以自动完成。\n",
    "\n",
    "这些类构成了PyTorch框架的核心，通过它们你可以定义模型、加载数据、训练模型以及执行模型的推断。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a80b4",
   "metadata": {},
   "source": [
    "以下表格详细说明了PyTorch框架中一些常用的类及其用途：\n",
    "\n",
    "| 类别                          | 类名                        | 用途说明                                                                 |\n",
    "|:-----------------------------|:---------------------------|:-----------------------------------------------------------------------|\n",
    "| 基础数据结构                      | `torch.Tensor`            | 多维数组，支持自动微分功能，是构建和训练网络的基础。可以在CPU或GPU上进行计算以加速。                       |\n",
    "| 网络构建                        | `torch.nn.Module`         | 所有神经网络模块的基类。用于定义一个网络，可以包含其他模块（层）或Tensor作为成员变量。                  |\n",
    "| 层（网络构件）                    | `torch.nn` 下的各种层如 `Linear`、`Conv2d` | 包含构建神经网络所需的各种层，例如全连接层、卷积层、激活层等。用于定义网络的结构。                     |\n",
    "| 激活函数                        | `torch.nn` 下的激活函数如 `ReLU`、`Sigmoid` | 提供神经网络中常用的激活函数。这些函数通常作为网络层的一部分来使用，增加模型的非线性。                 |\n",
    "| 损失函数                        | `torch.nn` 下的损失如 `MSELoss`、`CrossEntropyLoss` | 定义了用于训练网络的损失函数。这些损失函数用于计算模型输出与目标之间的差距，指导网络的优化。                |\n",
    "| 优化算法                        | `torch.optim` 下的如 `SGD`、`Adam`     | 提供了多种优化算法来调整网络参数，用于网络的训练过程中。                                          |\n",
    "| 数据处理                        | `torch.utils.data.Dataset` | 一个抽象类，用于定义加载数据的方式。用户可以根据需要继承并实现数据加载的逻辑，以便于后续的数据处理和训练。       |\n",
    "| 数据加载                        | `torch.utils.data.DataLoader` | 提供了对`Dataset`的封装，通过多线程来加载数据，支持自动批处理、打乱数据和并行数据加载等功能。             |\n",
    "| 自动微分                        | `torch.autograd`           | 提供自动微分机制，使模型可以自动进行梯度计算。这是训练神经网络时反向传播算法的基础。                      |\n",
    "\n",
    "这个表格提供了PyTorch中一些核心类的概览，以及它们在构建和训练神经网络中的作用。通过组合使用这些类，可以灵活地定义出各种复杂的模型结构，进行高效的数据处理和模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24579096",
   "metadata": {},
   "source": [
    "## 00. torch.autograd\n",
    "\n",
    "`torch.autograd` 是 PyTorch 中实现自动微分的核心库，它支持所有张量操作的自动求导。在深度学习中，我们通过训练数据来调整模型参数以最小化某种损失函数，这个过程需要计算损失函数相对于模型参数的梯度。`torch.autograd` 使这一过程自动化，极大简化了模型的开发和训练过程。\n",
    "\n",
    "### 关键特性\n",
    "- **自动求导**：你只需要通过正常的前向计算来定义模型的计算图，`torch.autograd` 就能够利用链式法则自动计算梯度。\n",
    "- **动态计算图**：PyTorch 使用动态计算图（也称为命令式编程范式），这意味着图的构建是即时的，随着代码的执行而发生。这种方式使得模型更加灵活，可以根据输入数据的不同而改变操作流程。\n",
    "\n",
    "### 核心类和函数\n",
    "- **`torch.Tensor`**：是 `torch.autograd` 的主要类，如果设置 `.requires_grad` 为 `True`，PyTorch 会自动跟踪所有对于该张量的操作。完成计算后，你可以调用 `.backward()` 来自动计算所有的梯度，这些梯度会累积到对应张量的 `.grad` 属性中。\n",
    "- **`grad_fn`**：每个 Tensor 都有一个 `grad_fn` 属性，该属性引用了创建 Tensor 自身的 Function，是自动求导的关键。\n",
    "- **`backward()`**：是计算梯度的关键函数。当从某个张量调用该函数时，PyTorch 会计算其梯度，并将其存储在张量的 `.grad` 属性中。\n",
    "- **`torch.autograd.grad()`**：这个函数提供了另一种计算梯度的方式，它不会像 `.backward()` 那样将梯度累积到 `.grad` 属性中，而是直接返回梯度值。\n",
    "\n",
    "### 示例代码\n",
    "以下是一个使用 `torch.autograd` 的简单示例，演示了如何进行自动求导：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 创建两个张量，并设置requires_grad=True来跟踪它们的计算\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = torch.tensor([[2.0, 1.0], [0.0, 2.0]], requires_grad=True)\n",
    "\n",
    "# 进行一些操作\n",
    "z = x + 2\n",
    "f = z * y\n",
    "\n",
    "# 假设我们对f的每个元素求和得到一个标量输出\n",
    "out = f.sum()\n",
    "\n",
    "# 使用自动微分计算out关于x和y的梯度\n",
    "out.backward()\n",
    "\n",
    "# 梯度现在保存在x.grad和y.grad中\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "```\n",
    "\n",
    "在这个例子中，`x` 和 `y` 是需要计算梯度的张量。我们对它们执行了一系列操作，并通过调用 `out.backward()` 来计算这些操作相对于 `x` 和 `y` 的梯度。这些梯度值被存储在了 `x.grad` 和 `y.grad` 属性中。\n",
    "\n",
    "`torch.autograd` 是实现神经网络反向传播的基础，使得网络训练过程自动化和简化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03feff58",
   "metadata": {},
   "source": [
    "## 01. torch.Tensor\n",
    "\n",
    "`torch.Tensor` 是 PyTorch 中最核心的类，它代表了一个多维矩阵，也是神经网络中数据的基本单位。`Tensor` 提供了丰富的方法来进行创建、操作和转换。下面是一些常用的 `torch.Tensor` 方法及其示例代码：\n",
    "\n",
    "1. **创建 Tensor**\n",
    "   - `torch.tensor()`：直接从数据创建一个 Tensor。\n",
    "   - `torch.zeros()`：创建一个填充了零的 Tensor。\n",
    "   - `torch.ones()`：创建一个填充了一的 Tensor。\n",
    "   - `torch.rand()`：创建一个具有随机初始化值的 Tensor。\n",
    "\n",
    "2. **Tensor 操作**\n",
    "   - `.reshape()`：改变 Tensor 的形状。\n",
    "   - `.to()`：转移 Tensor 到不同的设备（例如，从 CPU 到 GPU）。\n",
    "   - `.item()`：将一个标量 Tensor 转换为一个 Python 数字。\n",
    "   - `.numpy()`：将 Tensor 转换为一个 NumPy 数组。\n",
    "\n",
    "3. **数学运算**\n",
    "   - `torch.add()`：对两个 Tensor 进行加法运算。\n",
    "   - `.sum()`：计算 Tensor 所有元素的和。\n",
    "   - `.mean()`：计算 Tensor 所有元素的平均值。\n",
    "   - `.max()`、`.min()`：计算 Tensor 中的最大值和最小值。\n",
    "\n",
    "下面是具体的示例代码：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 创建 Tensor\n",
    "x = torch.tensor([1, 2, 3])\n",
    "zeros = torch.zeros(3, 3)\n",
    "ones = torch.ones(3, 3)\n",
    "random_tensor = torch.rand(3, 3)\n",
    "\n",
    "# Tensor 操作\n",
    "reshaped = random_tensor.reshape(1, 9)\n",
    "to_device = random_tensor.to('cuda')  # 假设系统支持 CUDA\n",
    "item = x[0].item()\n",
    "\n",
    "# 数学运算\n",
    "added = torch.add(x, x)\n",
    "summed = x.sum()\n",
    "mean_val = random_tensor.mean()\n",
    "max_val, min_val = random_tensor.max(), random_tensor.min()\n",
    "\n",
    "# 示例输出\n",
    "print(\"Original:\", x)\n",
    "print(\"Reshaped:\", reshaped)\n",
    "print(\"To Device:\", to_device)\n",
    "print(\"Item:\", item)\n",
    "print(\"Added:\", added)\n",
    "print(\"Summed:\", summed)\n",
    "print(\"Mean Value:\", mean_val)\n",
    "print(\"Max and Min Value:\", max_val, min_val)\n",
    "```\n",
    "\n",
    "请注意，`.to('cuda')` 部分假设你的系统支持 CUDA，并且你已经安装了 CUDA。如果你的系统不支持 CUDA，可以忽略这一行或者将其替换为 `.to('cpu')`。这些示例展示了 `torch.Tensor` 类的一些基本方法和数学运算，是在 PyTorch 中进行数据处理和模型开发时经常会用到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a760310",
   "metadata": {},
   "source": [
    "## 02. torch.nn.Module\n",
    "\n",
    "`torch.nn.Module` 是 PyTorch 中所有神经网络模块的基类。几乎所有使用 PyTorch 构建的神经网络组件都继承自 `torch.nn.Module`。这个类提供了一套易于使用的接口来定义自己的网络层、组合它们成一个模型，以及管理模型内部的参数。以下是 `torch.nn.Module` 的一些关键特性和常用方法：\n",
    "\n",
    "### 关键特性：\n",
    "- **参数管理**：`Module` 自动跟踪它内部的 `Tensor`，特别是那些通过 `torch.nn.Parameter` 定义的参数，这使得参数的初始化、共享和更新变得简单直接。\n",
    "- **模块化**：可以通过组合不同的 `Module` 子类来构建复杂的网络结构，每个子模块也都是 `Module` 的实例。这种模块化设计使得网络的各个部分可以被重用、修改和扩展。\n",
    "- **钩子函数**：提供了前向和后向钩子，允许在模型的前向传递和反向传播过程中插入自定义的操作。\n",
    "\n",
    "### 常用方法：\n",
    "- `__init__(self)`: 构造函数，用于初始化模块的子组件和参数。\n",
    "- `forward(self, input)`: 定义了模型的前向传播逻辑。所有子类必须覆盖这个方法，它接受输入数据并返回模型的输出。\n",
    "- `parameters(self)`: 返回模型参数的迭代器，通常用于模型参数的优化。\n",
    "- `to(device)`: 将模型的所有参数和缓冲区移动到给定的设备（CPU或GPU）上。\n",
    "- `train(self, mode=True)`: 设置模块为训练模式。影响某些层的行为，比如 `Dropout` 和 `BatchNorm`。\n",
    "- `eval(self)`: 设置模型为评估模式。与 `train(False)` 等效。\n",
    "\n",
    "### 示例代码：\n",
    "\n",
    "下面的示例代码演示了如何使用 `torch.nn.Module` 来定义一个简单的多层感知机（MLP）：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # 定义第一层（线性层）\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # 定义第二层（线性层）\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 前向传播：通过第一层，激活函数，然后通过第二层\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleMLP(input_size=784, hidden_size=100, num_classes=10)\n",
    "print(model)\n",
    "\n",
    "# 假设我们有一些随机输入数据\n",
    "input_data = torch.randn(1, 784)\n",
    "# 前向传播\n",
    "output = model(input_data)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "这个例子展示了如何定义一个模型类，它继承自 `torch.nn.Module`，并实现了自己的构造函数和 `forward` 方法。通过这种方式，你可以使用 PyTorch 提供的丰富的预定义层和操作来灵活构建和管理复杂的神经网络模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9773a",
   "metadata": {},
   "source": [
    "## 03. torch.nn 下的各种层如 Linear、Conv2d\n",
    "\n",
    "`torch.nn` 模块下提供了多种构建神经网络所需的层，这些层是构建深度学习模型的基石。下面介绍几种常用的层以及它们的用途：\n",
    "\n",
    "1. **Linear (全连接层)**：\n",
    "   - `torch.nn.Linear(in_features, out_features, bias=True)`：\n",
    "     全连接层或称为线性层，对输入数据应用线性变换（`y = Ax + b`）。\n",
    "   - **参数**：\n",
    "     - `in_features`：输入的特征维数。\n",
    "     - `out_features`：输出的特征维数。\n",
    "     - `bias`：布尔值，是否添加偏置项（默认为True）。\n",
    "\n",
    "2. **Conv2d (二维卷积层)**：\n",
    "   - `torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`：\n",
    "     二维卷积层，常用于处理图像数据。\n",
    "   - **参数**：\n",
    "     - `in_channels`：输入信号的通道数。\n",
    "     - `out_channels`：卷积产生的通道数。\n",
    "     - `kernel_size`：卷积核的大小。\n",
    "     - `stride`：卷积步长。\n",
    "     - `padding`：输入的每一条边补充0的层数。\n",
    "     - `dilation`：卷积核元素之间的间距。\n",
    "     - `groups`：控制输入和输出之间的连接（分组卷积）。\n",
    "     - `bias`：布尔值，是否添加偏置项（默认为True）。\n",
    "     - `padding_mode`：填充模式。\n",
    "\n",
    "3. **ReLU (激活函数)**：\n",
    "   - `torch.nn.ReLU(inplace=False)`：\n",
    "     应用了修正线性单元函数（`max(0,x)`）的激活层，增加了模型的非线性，没有参数。\n",
    "   - **参数**：\n",
    "     - `inplace`：布尔值，如果设置为True，会在原地执行操作，节省内存。\n",
    "\n",
    "4. **BatchNorm2d (二维批量归一化层)**：\n",
    "   - `torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)`：\n",
    "     在每个批次数据上应用二维批量归一化操作，可以加速训练过程并提高模型的稳定性。\n",
    "   - **参数**：\n",
    "     - `num_features`：来自期望输入的特征数。\n",
    "     - `eps`：为数值稳定性添加到分母的值。\n",
    "     - `momentum`：用于运行均值和方差的移动平均的值。\n",
    "     - `affine`：布尔值，当设为True时，给定的数据将会乘以一些可学习的参数。\n",
    "     - `track_running_stats`：布尔值，是否跟踪整个数据集的运行均值和方差。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "以下是如何在PyTorch中使用这些层的简单示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的CNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(in_features=50 * 4 * 4, out_features=500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = x.view(-1, 50 * 4 * 4)  # Flatten the output\n",
    "        x = self\n",
    "\n",
    ".relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "model = SimpleCNN()\n",
    "print(model)\n",
    "```\n",
    "\n",
    "这个例子中，`SimpleCNN` 模型由两个卷积层（`Conv2d`）、三个ReLU激活函数层以及两个全连接层（`Linear`）组成。这个简单的CNN模型可以用于处理图像数据，如分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231f7d3",
   "metadata": {},
   "source": [
    "## 04. torch.nn 下的激活函数如 ReLU、Sigmoid\n",
    "\n",
    "在`torch.nn`模块下，PyTorch 提供了多种激活函数，它们是神经网络中非常关键的组成部分，用于引入非线性，使得网络能够学习和模拟复杂的函数映射。激活函数通常在网络的层之间使用。下面介绍几种常见的激活函数及其用途：\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**\n",
    "   - **类**: `torch.nn.ReLU()`\n",
    "   - **用途**: ReLU函数提供了一个简单的非线性变换。对于正输入，直接输出该值；对于负输入，输出0。它是最常用的激活函数之一，特别是在卷积神经网络（CNNs）中，因为它有助于缓解梯度消失问题，同时计算效率较高。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     import torch.nn as nn\n",
    "\n",
    "     relu = nn.ReLU()\n",
    "     ```\n",
    "\n",
    "2. **Sigmoid**\n",
    "   - **类**: `torch.nn.Sigmoid()`\n",
    "   - **用途**: Sigmoid函数将输入值压缩到0和1之间。它常用于二分类问题的输出层，因为其输出可以解释为概率。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     sigmoid = nn.Sigmoid()\n",
    "     ```\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)**\n",
    "   - **类**: `torch.nn.Tanh()`\n",
    "   - **用途**: Tanh函数将输入值压缩到-1和1之间。它和Sigmoid函数相似，但是由于其输出范围是中心化的，所以在某些情况下（特别是在网络较深时）比Sigmoid函数表现得更好。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     tanh = nn.Tanh()\n",
    "     ```\n",
    "\n",
    "4. **LeakyReLU**\n",
    "   - **类**: `torch.nn.LeakyReLU(negative_slope=0.01)`\n",
    "   - **用途**: LeakyReLU是ReLU的一个变种，它允许小的梯度当输入为负数时流过，通过引入一个小的斜率来避免“死亡ReLU”问题。这个小的斜率是可调的。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     leaky_relu = nn.LeakyReLU(0.01)\n",
    "     ```\n",
    "\n",
    "5. **Softmax**\n",
    "   - **类**: `torch.nn.Softmax(dim=None)`\n",
    "   - **用途**: Softmax函数将输入值“压缩”到(0, 1)区间内，使它们可以解释为概率。它通常用于多分类问题的输出层。\n",
    "   - **注意**: 在使用交叉熵损失(`nn.CrossEntropyLoss`)时，不需要在输出层使用Softmax，因为`CrossEntropyLoss`内部已经包含了Softmax。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     softmax = nn.Softmax(dim=1)  # 假设输入是[N, C]，其中N是批量大小，C是类别数\n",
    "     ```\n",
    "\n",
    "这些激活函数可以通过直接实例化相应的类来使用，或者作为`nn.Sequential`模型的一部分来链式调用。选择哪种激活函数取决于具体的应用场景和网络架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe165b4",
   "metadata": {},
   "source": [
    "## 05. torch.nn 下的损失如 MSELoss、CrossEntropyLoss\n",
    "\n",
    "在深度学习中，损失函数（或代价函数）是衡量模型输出与目标值之间差异的指标，它是训练过程中优化的关键。`torch.nn` 模块提供了多种损失函数，以适应不同的应用和需求。下面是两种常用的损失函数：\n",
    "\n",
    "1. **MSELoss (均方误差损失)**\n",
    "   - **类**: `torch.nn.MSELoss()`\n",
    "   - **用途**: 常用于回归任务。它计算模型输出和目标值之间差异的平方的平均值。适用于输出和目标直接进行数值比较的情况。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     import torch\n",
    "     import torch.nn as nn\n",
    "\n",
    "     mse_loss = nn.MSELoss()\n",
    "     outputs = torch.randn(3, requires_grad=True)\n",
    "     targets = torch.randn(3)\n",
    "     loss = mse_loss(outputs, targets)\n",
    "     ```\n",
    "\n",
    "2. **CrossEntropyLoss (交叉熵损失)**\n",
    "   - **类**: `torch.nn.CrossEntropyLoss()`\n",
    "   - **用途**: 常用于多分类问题。它结合了LogSoftmax和NLLLoss（负对数似然损失）于一体，因此输入到这个损失函数的raw scores（模型的原始输出）不需要经过Softmax处理。适用于具有两个或更多类别的分类任务，其中任务的目标是单个类别。\n",
    "   - **注意**: 使用这个损失函数时，模型最后的输出层不应该包含Softmax激活函数，因为`CrossEntropyLoss`已经内置了这个计算。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     ce_loss = nn.CrossEntropyLoss()\n",
    "     outputs = torch.randn(3, 5, requires_grad=True)  # 假设有5个类别\n",
    "     targets = torch.tensor([1, 0, 4], dtype=torch.long)  # 目标类别\n",
    "     loss = ce_loss(outputs, targets)\n",
    "     ```\n",
    "\n",
    "除了上述两种，`torch.nn` 还提供了其他多种损失函数，比如：\n",
    "- **`nn.BCELoss`**：二进制交叉熵损失，用于二分类任务。\n",
    "- **`nn.BCEWithLogitsLoss`**：结合了`Sigmoid`层和`BCELoss`于一体，用于二分类任务，输入是raw scores。\n",
    "- **`nn.NLLLoss`**：负对数似然损失，通常与`LogSoftmax`一起使用，用于多分类问题。\n",
    "\n",
    "选择合适的损失函数对于训练有效的模型非常关键，它应该反映出模型的优化目标和任务的特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fd606",
   "metadata": {},
   "source": [
    "## 06. torch.optim 下的如 SGD、Adam\n",
    "\n",
    "在 PyTorch 中，`torch.optim` 是一个实现各种优化算法的模块。优化算法用于更新网络的权重和参数，以最小化或最大化损失函数或目标函数。下面介绍两种常用的优化器：\n",
    "\n",
    "1. **SGD (随机梯度下降)**\n",
    "   - **类**: `torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\n",
    "   - **用途**: SGD 是最基本的优化算法，它在每一步更新参数时只考虑一个随机选择的样本的梯度，这使得训练过程更快，但可能会增加变异性。它适用于大规模数据集和多种任务。\n",
    "   - **参数**:\n",
    "     - `params` (iterable)：要优化或包含要优化参数的字典的可迭代对象。\n",
    "     - `lr` (float)：学习率。\n",
    "     - `momentum` (float, 可选)：动量因子。\n",
    "     - `weight_decay` (float, 可选)：权重衰减（L2惩罚）。\n",
    "     - `nesterov` (bool, 可选)：是否使用Nesterov动量。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "     ```\n",
    "\n",
    "2. **Adam (自适应矩估计)**\n",
    "   - **类**: `torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)`\n",
    "   - **用途**: Adam 是一种基于一阶梯度的优化算法，它计算每个参数的自适应学习率。相比于 SGD，Adam 通常在实践中更快地收敛，并且对初学习率的选择不那么敏感。\n",
    "   - **参数**:\n",
    "     - `params` (iterable)：要优化或包含要优化参数的字典的可迭代对象。\n",
    "     - `lr` (float, 可选)：学习率（默认：1e-3）。\n",
    "     - `betas` (Tuple[float, float], 可选)：用于计算梯度及其平方的运行平均值的系数（默认：(0.9, 0.999)）。\n",
    "     - `eps` (float, 可选)：增加数值稳定性的项（默认：1e-8）。\n",
    "     - `weight_decay` (float, 可选)：权重衰减（L2惩罚）。\n",
    "     - `amsgrad` (boolean, 可选)：是否使用AMSGrad变种来改善Adam的收敛性（默认：False）。\n",
    "   - **示例**:\n",
    "     ```python\n",
    "     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "     ```\n",
    "\n",
    "这两种优化器在深度学习领域非常流行，适用于不同的应用和网络结构。选择哪种优化器往往取决于具体的任务、数据集的特性以及个人偏好。实际应用中可能需要尝试几种不同的设置，以找到最适合当前任务的配置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba485362",
   "metadata": {},
   "source": [
    "## 07. torch.utils.data.Dataset\n",
    "\n",
    "`torch.utils.data.Dataset` 是 PyTorch 用于处理数据集的一个抽象类。它为数据集提供了一个统一的接口，让用户可以轻松地实现和使用各种复杂的数据集。要使用 `Dataset` 类，通常需要继承它并实现两个核心方法：`__len__()` 和 `__getitem__()`。\n",
    "\n",
    "### `__len__(self)`\n",
    "这个方法应返回数据集中的样本数。当你调用 `len(dataset)` 时，它会返回 `__len__` 方法的结果。\n",
    "\n",
    "### `__getitem__(self, index)`\n",
    "这个方法应返回一个索引为 `index` 的样本。这使得可以使用 `dataset[index]` 来获取数据集中的特定样本。\n",
    "\n",
    "### 示例：自定义 Dataset\n",
    "下面是一个简单的例子，展示了如何创建一个自定义的 `Dataset` 类来处理一个简单的数值列表。\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"一个简单的数据集示例\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# 使用示例\n",
    "data = list(range(0, 100))  # 一个简单的数据列表\n",
    "simple_dataset = SimpleDataset(data)\n",
    "\n",
    "print(f\"数据集大小: {len(simple_dataset)}\")\n",
    "print(f\"索引为10的数据样本: {simple_dataset[10]}\")\n",
    "```\n",
    "\n",
    "### 实际应用\n",
    "在实际应用中，`Dataset` 类经常用于处理更复杂的数据，如图像、文本或音频。在这些情况下，`__getitem__` 方法会涉及到加载数据文件、预处理等操作。\n",
    "\n",
    "例如，如果你正在处理图像数据，`__getitem__` 方法可能会从磁盘加载图像文件，将其转换为张量，然后返回这个图像张量和对应的标签。这样的实现确保了数据的加载是高效和灵活的，尤其是结合 `torch.utils.data.DataLoader` 使用时，可以轻松地实现多进程数据加载和自动批量处理。\n",
    "\n",
    "`Dataset` 和 `DataLoader` 的组合为 PyTorch 用户提供了一个强大而灵活的工具，用于处理各种类型的数据集，并支持复杂的数据预处理和加载策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02cb63",
   "metadata": {},
   "source": [
    "## 08. torch.utils.data.DataLoader\n",
    "\n",
    "`torch.utils.data.DataLoader` 是 PyTorch 中的一个重要类，用于封装数据集，提供批处理、打乱数据、多线程数据加载等功能。这使得加载数据变得既简单又高效，是训练神经网络时不可或缺的工具。\n",
    "\n",
    "### 核心功能\n",
    "- **批处理**：自动将数据集的样本组合成批次，这对于利用现代计算库（如CUDA）进行批量运算非常重要。\n",
    "- **打乱数据**：在每个epoch开始时，可以选择是否打乱数据，这有助于模型避免记住批次中样本的顺序，从而提高模型的泛化能力。\n",
    "- **多线程加载**：`DataLoader` 可以使用多个工作线程来并行加载数据，极大地加快了数据准备的速度，使得训练过程中的数据供给不再成为瓶颈。\n",
    "\n",
    "### 使用方法\n",
    "要使用 `DataLoader`，你需要先定义一个 `Dataset` 对象，然后将其传递给 `DataLoader` 的构造函数。`DataLoader` 的构造函数还接受多个参数来控制加载的过程，如批大小、是否打乱数据等。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "假设我们已经有一个名为 `simple_dataset` 的 `Dataset` 实例，下面是如何使用 `DataLoader` 来加载数据的示例：\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 假设 simple_dataset 是一个已经定义好的 Dataset 实例\n",
    "data_loader = DataLoader(simple_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# 使用 DataLoader 在训练循环中加载数据\n",
    "for i, data in enumerate(data_loader):\n",
    "    # 在这里处理训练数据\n",
    "    # data 包含了一个批次的数据\n",
    "    pass\n",
    "```\n",
    "\n",
    "在这个示例中，`DataLoader` 会在每个 epoch 时自动打乱数据（如果 `shuffle=True`），并且以批次大小为 4 来加载数据。`num_workers=2` 指定了使用两个工作线程来加载数据，这有助于加快数据加载速度，特别是当数据预处理比较耗时时。\n",
    "\n",
    "### 参数详解\n",
    "- **dataset**：要加载的数据集。\n",
    "- **batch_size**：每个批次的大小。\n",
    "- **shuffle**：是否在每个epoch开始时打乱数据。\n",
    "- **num_workers**：用于数据加载的子进程数。设置为0意味着数据将在主进程中加载（默认值）。\n",
    "- **pin_memory**：如果为True，DataLoader会在返回前将Tensors复制到CUDA固定内存中。\n",
    "- **drop_last**：设置为True时，会丢弃最后一个不完整的批次。如果数据集大小不能被批大小整除，这个选项可以避免某些情况下的问题。\n",
    "\n",
    "`DataLoader` 的这些参数提供了强大的灵活性，可以根据你的具体需求来调整，以优化你的训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d84d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
